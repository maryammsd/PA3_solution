@article{Author2022,
   abstract = {The execution of smart contracts on the Ethereum blockchain consumes gas paid for by users submitting contracts' invocation requests. A contract execution proceeds as long as the users dedicate enough gas, within the limit set by Ethereum. If insufficient gas is provided, the contract execution halts and changes made during execution get reverted. Unfortunately, contracts may contain code patterns that increase execution cost, causing the contracts to run out of gas. These patterns can be manipulated by malicious attackers to induce unwanted behavior in the targeted victim contracts, e.g., Denial-of-Service (DoS) attacks. We call these gas-related vul-nerabilities. We propose eTainter, a static analyzer for detecting gas-related vulnerabilities based on taint tracking in the bytecode of smart contracts. We evaluate eTainter by comparing it with the prior work, MadMax, on a dataset of annotated contracts. The results show that eTainter outperforms MadMax in both precision and recall, and that eTainter has a precision of 90% based on manual inspection. We also use eTainter to perform large-scale analysis of 60,612 real-world contracts on the Ethereum blockchain. We find that gas-related vulnerabilities exist in 2,763 of these contracts, and that eTainter analyzes a contract in eight seconds, on average. CCS CONCEPTS • Security and privacy → Software and application security;},
   author = {Anonymous Author},
   doi = {10.1145/3533767.3534378},
   isbn = {9781450393799},
   keywords = {Ethereum,Ethereum security,Solidity,taint analysis},
   title = {eTainter: Detecting Gas-Related Vulnerabilities in Smart Contracts},
   url = {https://doi.org/10.1145/3533767.3534378},
   year = {2022},
}
@article{Choi2022,
   abstract = {While reactive synthesis and syntax-guided synthesis (Sy-GuS) have seen enormous progress in recent years, combining the two approaches has remained a challenge. In this work, we present the synthesis of reactive programs from Temporal Stream Logic modulo theories (TSL-MT), a framework that unites the two approaches to synthesize a single program. In our approach, reactive synthesis and SyGuS collaborate in the synthesis process, and generate executable code that implements both reactive and data-level properties. We present a tool, temos, that combines state-of-the-art methods in reactive synthesis and SyGuS to synthesize programs from TSL-MT specifications. We demonstrate the applicability of our approach over a set of benchmarks, and present a deep case study on synthesizing a music keyboard synthesizer. CCS Concepts: • Theory of computation → Modal and temporal logics.},
   author = {Wonhyuk Choi and Bernd Finkbeiner and Ruzica Piskac and Mark Santolucito},
   doi = {10.1145/3519939.3523429},
   isbn = {9781450392655},
   keywords = {Pro-gram Synthesis,Reactive Synthesis,Syntax-Guided Synthesis},
   title = {Can Reactive Synthesis and Syntax-Guided Synthesis Be Friends?},
   url = {https://doi.org/10.1145/3519939.3523429},
   year = {2022},
}
@article{Liu2022,
   abstract = {Exploiting the relationships among data, such as primary and foreign keys, is a classical query optimization technique. As persistent data is increasingly being created and maintained programmatically (e.g., web applications), prior work that focuses on inferring data relationships by tabulating statistics from the stored data misses an important opportunity. We present ConstrOpt, the first tool that identifies data relationships by analyzing the programs that generate and maintain the persistent data. Once identified, ConstrOpt leverages the found constraints to optimize the application's physical design and query execution by rewriting queries. Instead of developing a fixed set of predefined rewriting rules, ConstrOpt employs an enumerate-test-verify technique to automatically exploit the discovered data constraints to improve query execution. Each resulting rewrite is provably semantically equivalent to the original query. Using 14 real-world web applications, our experiments show that ConstrOpt can discover over 4306 data constraints by analyzing application source code. On 3 of the evaluated applications, among queries with at least one constrained column, 42% can benefit from data layout optimization, and 35% are optimized by changing the application code. Finally, ConstrOpt's constraint-driven optimizer improves the performance of 826 queries, 9.8% of which has over 2x speedup.},
   author = {Xiaoxuan Liu and Shuxian Wang and Mengzhu Sun and Sharon Lee and Sicheng Pan and Joshua Wu and Cong Yan and Junwen Yang and Shan Lu and Alvin Cheung},
   month = {5},
   title = {Leveraging Application Data Constraints to OptimizeDatabase-Backed Web Applications},
   url = {http://arxiv.org/abs/2205.02954},
   year = {2022},
}
@inproceedings{Guo2022,
   abstract = {With the rise of software-as-a-service and microservice architectures, RESTful APIs are now ubiquitous in mobile and web applications. A service can have tens or hundreds of API methods, making it a challenge for programmers to find the right combination of methods to solve their task. We present APIphany, a component-based synthesizer for programs that compose calls to RESTful APIs. The main innovation behind APIphany is the use of precise semantic types, both to specify user intent and to direct the search. APIphany contributes three novel mechanisms to overcome challenges in adapting component-based synthesis to the REST domain: (1) a type inference algorithm for augmenting REST specifications with semantic types; (2) an efficient synthesis technique for "wrangling" semi-structured data, which is commonly required in working with RESTful APIs; and (3) a new form of simulated execution to avoid executing APIs calls during synthesis. We evaluate APIphany on three real-world APIs and 32 tasks extracted from GitHub repositories and StackOverflow. In our experiments, APIphany found correct solutions to 29 tasks, with 23 of them reported among top ten synthesis results.},
   author = {Zheng Guo and David Cao and Davin Tjong and Jean Yang and Cole Schlesinger and Nadia Polikarpova},
   doi = {10.1145/3519939.3523450},
   month = {6},
   pages = {122-136},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Type-directed program synthesis for RESTful APIs},
   year = {2022},
}
@inproceedings{Zhou2022,
   abstract = {Analytical SQL is widely used in modern database applications and data analysis. However, its partitioning and grouping operators are challenging for novice users. Unfortunately, programming by example, shown effective on standard SQL, are less attractive because examples for analytical queries are more laborious to solve by hand. To make demonstrations easier to create, we designed a new end-user specification, programming by computation demonstration, that allows the user to demonstrate the task using a (possibly incomplete) cell-level computation trace. This specification is exploited in a new abstraction-based synthesis algorithm to prove that a partially formed query cannot be completed to satisfy the specification, allowing us to prune the search space. We implemented our approach in a tool named Sickle and tested it on 80 real-world analytical SQL tasks. Results show that even from small demonstrations, Sickle can solve 76 tasks, in 12.8 seconds on average, while the prior approaches can solve only 60 tasks and are on average 22.5x slower. Our user study with 13 participants reveals that our specification increases user efficiency and confidence on challenging tasks.},
   author = {Xiangyu Zhou and Rastislav Bodik and Alvin Cheung and Chenglong Wang},
   doi = {10.1145/3519939.3523712},
   month = {6},
   pages = {168-182},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Synthesizing analytical SQL queries from computation demonstration},
   year = {2022},
}
@inproceedings{Kellogg2022,
   abstract = {A typestate specification indicates which behaviors of an object are permitted in each of the object's states. In the general case, soundly checking a typestate specification requires precise information about aliasing (i.e., an alias or pointer analysis), which is computationally expensive. This requirement has hindered the adoption of sound typestate analyses in practice. This paper identifies accumulation typestate specifications, which are the subset of typestate specifications that can be soundly checked without any information about aliasing. An accumulation typestate specification can be checked instead by an accumulation analysis: a simple, fast dataflow analysis that conservatively approximates the operations that have been performed on an object. This paper formalizes the notions of accumulation analysis and accumulation typestate specification. It proves that accumulation typestate specifications are exactly those typestate specifications that can be checked soundly without aliasing information. Further, 41% of the typestate specifications that appear in the research literature are accumulation typestate specifications.},
   author = {Martin Kellogg and Narges Shadab and Manu Sridharan and Michael D. Ernst},
   doi = {10.4230/LIPIcs.ECOOP.2022.10},
   isbn = {9783959772259},
   issn = {18688969},
   journal = {Leibniz International Proceedings in Informatics, LIPIcs},
   keywords = {Typestate,finite-state property},
   month = {6},
   publisher = {Schloss Dagstuhl- Leibniz-Zentrum fur Informatik GmbH, Dagstuhl Publishing},
   title = {Accumulation Analysis},
   volume = {222},
   year = {2022},
}
@article{Laddad2022,
   abstract = {<p>Conflict-free replicated data types (CRDTs) are a promising tool for designing scalable, coordination-free distributed systems. However, constructing correct CRDTs is difficult, posing a challenge for even seasoned developers. As a result, CRDT development is still largely the domain of academics, with new designs often awaiting peer review and a manual proof of correctness. In this paper, we present Katara, a program synthesis-based system that takes sequential data type implementations and automatically synthesizes verified CRDT designs from them. Key to this process is a new formal definition of CRDT correctness that combines a reference sequential type with a lightweight ordering constraint that resolves conflicts between non-commutative operations. Our process follows the tradition of work in verified lifting, including an encoding of correctness into SMT logic using synthesized inductive invariants and hand-crafted grammars for the CRDT state and runtime. Katara is able to automatically synthesize CRDTs for a wide variety of scenarios, from reproducing classic CRDTs to synthesizing novel designs based on specifications in existing literature. Crucially, our synthesized CRDTs are fully, automatically verified, eliminating entire classes of common errors and reducing the process of producing a new CRDT from a painstaking paper proof of correctness to a lightweight specification.</p>},
   author = {Shadaj Laddad and Conor Power and Mae Milano and Alvin Cheung and Joseph M. Hellerstein},
   doi = {10.1145/3563336},
   issn = {2475-1421},
   issue = {OOPSLA2},
   journal = {Proceedings of the ACM on Programming Languages},
   month = {10},
   pages = {1349-1377},
   title = {Katara: synthesizing CRDTs with verified lifting},
   volume = {6},
   url = {https://dl.acm.org/doi/10.1145/3563336},
   year = {2022},
}
@inproceedings{Shi2021,
   abstract = {Sparse program analysis is fast as it propagates data flow facts via data dependence, skipping unnecessary control flows. However, when path-sensitively checking millions of lines of code, it is still prohibitively expensive because a huge number of path conditions have to be computed and solved via an SMT solver. This paper presents Fusion, a fused approach to inter-procedurally path-sensitive sparse analysis. In Fusion, the SMT solver does not work as a standalone tool on path conditions but directly on the program together with the sparse analysis. Such a fused design allows us to determine the path feasibility without explicitly computing path conditions, not only saving the cost of computing path conditions but also providing an opportunity to enhance the SMT solving algorithm. To the best of our knowledge, Fusion, for the first time, enables whole program bug detection on millions of lines of code in a common personal computer, with the precision of inter-procedural path-sensitivity. Compared to two state-of-the-art tools, Fusion is 10× faster but consumes only 10% of memory on average. Fusion has detected over a hundred bugs in mature open-source software, some of which have even been assigned CVE identifiers due to their security impact.},
   author = {Qingkai Shi and Peisen Yao and Rongxin Wu and Charles Zhang},
   doi = {10.1145/3453483.3454086},
   isbn = {9781450383912},
   journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
   keywords = {SMT solving,Sparse analysis,path sensitivity,program dependence graph},
   month = {6},
   pages = {930-943},
   publisher = {Association for Computing Machinery},
   title = {Path-sensitive sparse analysis without path conditions},
   year = {2021},
}
@article{Spall2020,
   abstract = {Build scripts for most build systems describe the actions to run, and the dependencies between those actions - but often build scripts get those dependencies wrong. Most build scripts have both too few dependencies (leading to incorrect build outputs) and too many dependencies (leading to excessive rebuilds and reduced parallelism). Any programmer who has wondered why a small change led to excess compilation, or who resorted to a clean step, has suffered the ill effects of incorrect dependency specification. We outline a build system where dependencies are not specified, but instead captured by tracing execution. The consequence is that dependencies are always correct by construction and build scripts are easier to write. The simplest implementation of our approach would lose parallelism, but we are able to recover parallelism using speculation.},
   author = {Sarah Spall and Neil Mitchell and Sam Tobin-Hochstadt},
   doi = {10.1145/3428237},
   issn = {24751421},
   issue = {OOPSLA},
   journal = {Proceedings of the ACM on Programming Languages},
   keywords = {build systems,functional programming},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Build scripts with perfect dependencies},
   volume = {4},
   year = {2020},
}
@article{Sotiropoulos2020,
   abstract = {Incremental and parallel builds are crucial features of modern build systems. Parallelism enables fast builds by running independent tasks simultaneously, while incrementality saves time and computing resources by processing the build operations that were affected by a particular code change. Writing build definitions that lead to error-free incremental and parallel builds is a challenging task. This is mainly because developers are often unable to predict the effects of build operations on the file system and how different build operations interact with each other. Faulty build scripts may seriously degrade the reliability of automated builds, as they cause build failures, and non-deterministic and incorrect outputs. To reason about arbitrary build executions, we present BuildFS, a generally-applicable model that takes into account the specification (as declared in build scripts) and the actual behavior (low-level file system operation) of build operations. We then formally define different types of faults related to incremental and parallel builds in terms of the conditions under which a file system operation violates the specification of a build operation. Our testing approach, which relies on the proposed model, analyzes the execution of single full build, translates it into BuildFS, and uncovers faults by checking for corresponding violations. We evaluate the effectiveness, efficiency, and applicability of our approach by examining 612 Make and Gradle projects. Notably, thanks to our treatment of build executions, our method is the first to handle JVM-oriented build systems. The results indicate that our approach is (1) able to uncover several important issues (247 issues found in 47 open-source projects have been confirmed and fixed by the upstream developers), and (2) much faster than a state-of-the-art tool for Make builds (the median and average speedup is 39X and 74X respectively).},
   author = {Thodoris Sotiropoulos and Stefanos Chaliasos and Dimitris Mitropoulos and Diomidis Spinellis},
   doi = {10.1145/3428212},
   issn = {24751421},
   issue = {OOPSLA},
   journal = {Proceedings of the ACM on Programming Languages},
   keywords = {Gradle,JVM-based builds,Make,incremental builds,parallel builds},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {A model for detecting faults in build specifications},
   volume = {4},
   year = {2020},
}
@article{James2020,
   abstract = {We present Hoogle+, a web-based API discovery tool for Haskell. A Hoogle+ user can specify a programming task using either a type, a set of input-output tests, or both. Given a specification, the tool returns a list of matching programs composed from functions in popular Haskell libraries, and annotated with automatically-generated examples of their behavior. These features of Hoogle+ are powered by three novel techniques. First, to enable efficient type-directed synthesis from tests only, we develop an algorithm that infers likely type specifications from tests. Second, to return high-quality programs even with ambiguous specifications, we develop a technique that automatically eliminates meaningless and repetitive synthesis results. Finally, we show how to extend this elimination technique to automatically generate informative inputs that can be used to demonstrate program behavior to the user. To evaluate the effectiveness of Hoogle+ compared with traditional API search techniques, we perform a user study with 30 participants of varying Haskell proficiency. The study shows that programmers equipped with Hoogle+ generally solve tasks faster and were able to solve 50% more tasks overall.},
   author = {Michael B. James and Zheng Guo and Ziteng Wang and Shivani Doshi and Hila Peleg and Ranjit Jhala and Nadia Polikarpova},
   doi = {10.1145/3428273},
   issn = {24751421},
   issue = {OOPSLA},
   journal = {Proceedings of the ACM on Programming Languages},
   keywords = {Human-Computer Interaction,Program Synthesis,Type Inference},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Digging for fold: Synthesis-aided API discovery for Haskell},
   volume = {4},
   year = {2020},
}
@article{Guo2020,
   abstract = {We consider the problem of type-directed component-based synthesis where, given a set of (typed) components and a query type, the goal is to synthesize a term that inhabits the query. Classical approaches based on proof search in intuitionistic logics do not scale up to the standard libraries of modern languages, which span hundreds or thousands of components. Recent graph reachability based methods proposed for Java do scale, but only apply to monomorphic data and components: polymorphic data and components infinitely explode the size of the graph that must be searched, rendering synthesis intractable. We introduce type-guided abstraction refinement (TYGAR), a new approach for scalable type-directed synthesis over polymorphic datatypes and components. Our key insight is that we can overcome the explosion by building a graph over abstract types which represent a potentially unbounded set of concrete types. We show how to use graph reachability to search for candidate terms over abstract types, and introduce a new algorithm that uses proofs of untypeability of ill-typed candidates to iteratively refine the abstraction until a well-typed result is found. We have implemented TYGAR in H+, a tool that takes as input a set of Haskell libraries and a query type, and returns a Haskell term that uses functions from the provided libraries to implement the query type. Our support for polymorphism allows H+ to work with higher-order functions and type classes, and enables more precise queries due to parametricity. We have evaluated H+ on 44 queries using a set of popular Haskell libraries with a total of 291 components. H+ returns an interesting solution within the first five results for 32 out of 44 queries. Our results show that TYGAR allows H+ to rapidly return well-typed terms, with the median time to first solution of just 1.4 seconds. Moreover, we observe that gains from iterative refinement over exhaustive enumeration are more pronounced on harder queries.},
   author = {Zheng Guo and Michael James and David Justo and Jiaxiao Zhou and Ziteng Wang and Ranjit Jhala and Nadia Polikarpova},
   doi = {10.1145/3371080},
   issn = {24751421},
   issue = {POPL},
   journal = {Proceedings of the ACM on Programming Languages},
   keywords = {Abstract Interpretation,Program Synthesis,Type Systems},
   month = {1},
   publisher = {Association for Computing Machinery},
   title = {Program synthesis by type-guided abstraction refinement},
   volume = {4},
   year = {2020},
}
@article{Barke2020,
   abstract = {A key challenge in program synthesis is the astronomical size of the search space the synthesizer has to explore. In response to this challenge, recent work proposed to guide synthesis using learned probabilistic models. Obtaining such a model, however, might be infeasible for a problem domain where no high-quality training data is available. In this work we introduce an alternative approach to guided program synthesis: instead of training a model ahead of time we show how to bootstrap one just in time, during synthesis, by learning from partial solutions encountered along the way. To make the best use of the model, we also propose a new program enumeration algorithm we dub guided bottom-up search, which extends the efficient bottom-up search with guidance from probabilistic models. We implement this approach in a tool called Probe, which targets problems in the popular syntax-guided synthesis (SyGuS) format. We evaluate Probe on benchmarks from the literature and show that it achieves significant performance gains both over unguided bottom-up search and over a state-of-the-art probability-guided synthesizer, which had been trained on a corpus of existing solutions. Moreover, we show that these performance gains do not come at the cost of solution quality: programs generated by Probe are only slightly more verbose than the shortest solutions and perform no unnecessary case-splitting.},
   author = {Shraddha Barke and Hila Peleg and Nadia Polikarpova},
   doi = {10.1145/3428295},
   issn = {24751421},
   issue = {OOPSLA},
   journal = {Proceedings of the ACM on Programming Languages},
   keywords = {Domain-specific languages,Probabilistic models,Program Synthesis},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Just-in-time learning for bottom-up enumerative synthesis},
   volume = {4},
   year = {2020},
}
@inproceedings{Kellogg2020,
   abstract = {In object-oriented languages, constructors often have a combination of required and optional formal parameters. It is tedious and inconvenient for programmers to write a constructor by hand for each combination. The multitude of constructors is error-prone for clients, and client code is difficult to read due to the large number of constructor arguments. Therefore, programmers often use design patterns that enable more flexible object construction-the builder pattern, dependency injection, or factory methods. However, these design patterns can be too flexible: not all combinations of logical parameters lead to the construction of wellformed objects. When a client uses the builder pattern to construct an object, the compiler does not check that a valid set of values was provided. Incorrect use of builders can lead to security vulnerabilities, run-time crashes, and other problems. This work shows how to statically verify uses of object construction, such as the builder pattern. Using a simple specification language, programmers specify which combinations of logical arguments are permitted. Our compile-time analysis detects client code that may construct objects unsafely. Our analysis is based on a novel special case of typestate checking, accumulation analysis, that modularly reasons about accumulations of method calls. Because accumulation analysis does not require precise aliasing information for soundness, our analysis scales to industrial programs. We evaluated it on over 9 million lines of code, discovering defects which included previously-unknown security vulnerabilities and potential null-pointer violations in heavily-used open-source codebases. Our analysis has a low false positive rate and low annotation burden. Our implementation and experimental data are publicly available.},
   author = {Martin Kellogg and Manli Ran and Manu Sridharan and Martin Schaf and Michael D. Ernst},
   doi = {10.1145/3377811.3380341},
   isbn = {9781450371216},
   issn = {02705257},
   journal = {Proceedings - International Conference on Software Engineering},
   keywords = {Ami sniping,Autovalue,Builder pattern,Lightweight verification,Lombok,Pluggable type systems},
   month = {6},
   pages = {1447-1458},
   publisher = {IEEE Computer Society},
   title = {Verifying object construction},
   year = {2020},
}
@article{Cousot2019,
   abstract = {The fundamental idea of Abstract 2 Interpretation (A 2 I), also called meta-abstract interpretation, is to apply abstract interpretation to abstract interpretation-based static program analyses. A 2 I is generally meant to use abstract interpretation to analyse properties of program analysers. A 2 I can be either offline or online. Offline A 2 I is performed either before the program analysis, such as variable packing used by the Astrée program analyser, or after the program analysis, such as in alarm diagnosis. Online A 2 I is performed during the program analysis, such as Venet's cofibred domains or Halbwachs et al. 's and Singh et al. 's variable partitioning techniques for fast polyhedra/numerical abstract domains. We formalize offline and online meta-abstract interpretation and illustrate this notion with the design of widenings and the decomposition of relational abstract domains to speed-up program analyses. This shows how novel static analyses can be extracted as meta-abstract interpretations to design efficient and precise program analysis algorithms.},
   author = {Patrick Cousot and Roberto Giacobazzi and Francesco Ranzato},
   doi = {10.1145/3290355},
   issue = {POPL},
   journal = {Proceedings of the ACM on Programming Languages},
   pages = {1-31},
   title = {A²I: abstract² interpretation},
   volume = {3},
   year = {2019},
}
@article{Pham2019,
   abstract = {Analyzing and verifying heap-manipulating programs automatically is challenging. A key for fighting the complexity is to develop compositional methods. For instance, many existing verifiers for heap-manipulating programs require user-provided specification for each function in the program in order to decompose the verification problem. The requirement, however, often hinders the users from applying such tools. To overcome the issue, we propose to automatically learn heap-related program invariants in a property-guided way for each function call. The invariants are learned based on the memory graphs observed during test execution and improved through memory graph mutation. We implemented a prototype of our approach and integrated it with two existing program verifiers. The experimental results show that our approach enhances existing verifiers effectively in automatically verifying complex heap-manipulating programs with multiple function calls.},
   author = {Long H. Pham and Jun Sun and Quang Loc Le},
   pages = {1-20},
   title = {Compositional Verification of Heap-Manipulating Programs through Property-Guided Learning},
   url = {http://arxiv.org/abs/1908.10051},
   year = {2019},
}
@article{Gorogiannis2019,
   abstract = {RacerD is a static race detector that has been proven to be effective in engineering practice: it has seen thousands of data races fixed by developers before reaching production, and has supported the migration of Facebook's Android app rendering infrastructure from a single-threaded to a multi-threaded architecture. We prove a True Positives Theorem stating that, under certain assumptions, an idealized theoretical version of the analysis never reports a false positive. We also provide an empirical evaluation of an implementation of this analysis, versus the original RacerD. The theorem was motivated in the first case by the desire to understand the observation from production that RacerD was providing remarkably accurate signal to developers, and then the theorem guided further analyzer design decisions. Technically, our result can be seen as saying that the analysis computes an under-approximation of an over-approximation, which is the reverse of the more usual (over of under) situation in static analysis. Until now, static analyzers that are effective in practice but unsound have often been regarded as ad hoc; in contrast, we suggest that, in the future, theorems of this variety might be generally useful in understanding, justifying and designing effective static analyses for bug catching. 1 CONTEXT FOR THE TRUE POSITIVES THEOREM The purpose of this paper is to state and prove a theorem that has come about by reacting to surprising properties we observed of a static program analysis that has been in production at Facebook for over a year. The RacerD program analyzer searches for data races in Java programs, and it has had significantly more reported industrial impact than any other concurrency analysis that we are aware of. It was released as open source in October of 2017, and the OOPSLA'18 paper by Blackshear et al. (2018) describes its design, and gives more details about its deployment. They report, for example, that over 2,500 concurrent data races found by RacerD have been fixed by Facebook developers, and that it has been used to support the conversion of Facebook's Android app rendering infrastructure from a single-threaded to a multi-threaded architecture.},
   author = {Nikos Gorogiannis and Peter W. O'Hearn and Ilya Sergey},
   doi = {10.1145/3290370},
   issue = {POPL},
   journal = {Proceedings of the ACM on Programming Languages},
   pages = {1-29},
   title = {A true positives theorem for a static race detector},
   volume = {3},
   year = {2019},
}
@article{Le2019,
   abstract = {We introduce a new dynamic analysis technique to discover invariants in separation logic for heap-manipulating programs. First, we use a debugger to obtain rich program execution traces at locations of interest on sample inputs. These traces consist of heap and stack information of variables that point to dynamically allocated data structures. Next, we iteratively analyze separate memory regions related to each pointer variable and search for a formula over predefined heap predicates in separation logic to model these regions. Finally, we combine the computed formulae into an invariant that describes the shape of explored memory regions. We present SLING, a tool that implements these ideas to automatically generate invariants in separation logic at arbitrary locations in C programs, e.g., program pre and postconditions and loop invariants. Preliminary results on existing benchmarks show that SLING can efficiently generate correct and useful invariants for programs that manipulate a wide variety of complex data structures.},
   author = {Ton Chanh Le and Guolong Zheng and Thanh Vu Nguyen},
   doi = {10.1145/3314221.3314634},
   isbn = {9781450367127},
   journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
   keywords = {Dynamic invariant analysis,Separation logic},
   pages = {788-801},
   title = {SLinG: Using dynamic analysis to infer program invariants in separation logic},
   year = {2019},
}
@article{Sianturi2019,
   abstract = {ΕΙΣ ΤΟΝ ΑΙΩΝΑ},
   author = {E.T. Sianturi},
   issue = {5},
   journal = {Αγαη},
   keywords = {ΕΡΨΣ-ΑΓΑΠΗ},
   pages = {55},
   title = {No TitleΕΛΕΝΗ},
   volume = {8},
   year = {2019},
}
@article{Xie2019,
   abstract = {Analyzing loops is very important for various software engineering tasks such as bug detection, test case generation and program optimization. However, loops are very challenging structures for program analysis, especially when (nested) loops contain multiple paths that have complex interleaving relationships. In this paper, we propose the path dependency automaton (PDA) to capture the dependencies among the multiple paths in a loop. Based on the PDA, we first propose a loop classification to understand the complexity of loop summarization. Then, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects (i.e., disjunctive loop summary) on the variables of interest. An algorithm is proposed to traverse the PDA to summarize the effect for all possible executions in the loop. We have evaluated Proteus using loops from five open-source projects and two well-known benchmarks and applying the disjunctive loop summary to three applications: loop bound analysis, program verification and test case generation. The evaluation results have demonstrated that Proteus can compute a more precise bound than the existing loop bound analysis techniques; Proteus can significantly outperform the state-of-the-art tools for loop program verification; and Proteus can help generate test cases for deep loops within one second, while symbolic execution tools KLEE and Pex either need much more time or fail.},
   author = {Xiaofei Xie and Bihuan Chen and Liang Zou and Yang Liu and Wei Le and Xiaohong Li},
   doi = {10.1109/TSE.2017.2788018},
   issn = {19393520},
   issue = {6},
   journal = {IEEE Transactions on Software Engineering},
   keywords = {Disjunctive loop summary,path dependency automaton,path interleaving},
   pages = {537-557},
   title = {Automatic Loop Summarization via Path Dependency Analysis},
   volume = {45},
   year = {2019},
}
@article{Kapus2019,
   abstract = {Analysing and comprehending C programs that use strings is hard: Using standard library functions for manipulating strings is not enforced and programs often use complex loops for the same purpose. We introduce the notion of memoryless loops that capture some of these string loops and present a counterexample-guided inductive synthesis approach to summarise memoryless string loops using C standard library functions, which has applications to testing, optimization and refactoring. We prove our summarization is correct for arbitrary input strings and evaluate it on a database of loops we gathered from a set of 13 open-source programs. Our approach can summarize over two thirds of memoryless loops in less than 5 minutes of computation time per loop. We then show that these summaries can be used to (1) enhance symbolic execution testing, where we observed median speedups of 79x when employing a string constraint solver, (2) optimize native code, where certain summarizations led to significant performance gains, and (3) refactor code, where we had several patches accepted in the codebases of popular applications such as patch and wget.},
   author = {Timotej Kapus and Oren Ish-Shalom and Shachar Itzhaky and Noam Rinetzky and Cristian Cadar},
   doi = {10.1145/3314221.3314610},
   isbn = {9781450367127},
   journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
   keywords = {Loop Summarisation,Optimisation,Refactoring,Strings,Symbolic Execution,Synthesis},
   pages = {874-888},
   title = {Computing summaries of string loops in C for better testing and refactoring},
   year = {2019},
}
@article{Dash2018,
   abstract = {Source code is bimodal: it combines a formal, algorithmic channel and a natural language channel of identiiers and comments. In this work, we model the bimodality of code with name lows, an assignment low graph augmented to track identiier names. Conceptual types are logically distinct types that do not always coincide with program types. Passwords and URLs are example conceptual types that can share the program type string. Our tool, RefiNym, is an unsupervised method that mines a lattice of conceptual types from name lows and reiies them into distinct nominal types. For string, RefiNym inds and splits conceptual types originally merged into a single type, reducing the number of same-type variables per scope from 8.7 to 2.2 while eliminating 21.9% of scopes that have more than one same-type variable in scope. This makes the code more self-documenting and frees the type system to prevent a developer from inadvertently assigning data across conceptual types. CCS CONCEPTS · Software and its engineering → Data types and structures;},
   author = {Santanu Kumar Dash and Miltiadis Allamanis and Earl T. Barr},
   doi = {10.1145/3236024.3236042},
   isbn = {9781450355735},
   journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Information-theoretic Clustering,Type Reinement},
   pages = {107-117},
   title = {RefiNym: Using names to refine types},
   year = {2018},
}
@article{Baldoni2018,
   abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program's authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the last four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience. The present survey has been accepted for publication at ACM Computing Surveys. If you are considering citing this survey, we would appreciate if you could use the following BibTeX entry: http://goo.gl/Hf5Fvc},
   author = {Roberto Baldoni and Emilio Coppa and Daniele Cono D'elia and Camil Demetrescu and Irene Finocchi},
   doi = {10.1145/3182657},
   issn = {15577341},
   issue = {3},
   journal = {ACM Computing Surveys},
   keywords = {Concolic execution,Software testing,Static analysis,Symbolic execution},
   title = {A survey of symbolic execution techniques},
   volume = {51},
   year = {2018},
}
@article{Karkare2018,
   abstract = {© 2018 ACM. A major drawback of shape analysis techniques is the trade-off between speed and precision. We present TwAS: a novel method to combine two shape analysis techniques-a fastbutless precise technique with a slow but more precise one-to get the best of both (speed as well as precision). The novelty of our approach is the use of fast analysis to filter heap variables for which the shape information is already precise and can not be refined further This allows us to run the slow analysis on only a small portion of the program, thus improving its performance. We implemented TwAS in GCC as a dynamic plugin as an inter-procedural data-flow analysis and evaluated it on standard benchmarks against the component analyses. TwAS is able to achieve the same precision as the slow analysis at a marginal slowdown compared to the fast analysis. TwAS is able to improve precision for 5 out of 8 Olden benchmarks compared to the fast-but-imprecise analysis, while retaining the precision for the others. At the same time, TwAS is 4X - 2500X faster than the precise-but-slow analysis for these benchmarks, without any loss in precision.},
   author = {Amey Karkare},
   doi = {10.1145/3167132.3167330},
   isbn = {9781450351911},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Dataflow analysis,Heap analysis,Shape analysis,Static analysis},
   pages = {1857-1864},
   title = {TwAS: Two-stage shape analysis for speed and precision},
   year = {2018},
}
@article{Grech2018,
   abstract = {Traditional whole-program static analysis (e.g., a points-to analysis that models the heap) encounters scalability problems for realistic applications. We propose a łfeatherweightž analysis that combines a dynamic snapshot of the heap with otherwise full static analysis of program behavior. The analysis is extremely scalable, offering speedups of well over 3x, with complexity empirically evaluated to grow linearly relative to the number of reachable methods. The analysis is also an excellent tradeoff of precision and recall (relative to different dynamic executions): while it can never fully capture all program behaviors (i.e., it cannot match the near-perfect recall of a full static analysis) it often approaches it closely while achieving much higher (3.5x) precision.},
   author = {Neville Grech and George Fourtounis and Adrian Francalanza and Yannis Smaragdakis},
   doi = {10.1145/3213846.3213860},
   isbn = {9781450356992},
   journal = {ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
   keywords = {Heap Snapshots,Program Analysis,Scalability},
   pages = {198-208},
   title = {Shooting from the heap: Ultra-scalable static analysis with heap snapshots},
   year = {2018},
}
@article{Arndt2018,
   author = {Hannah Arndt and Christina Jansen and Joost-Pieter Katoen and Christoph Matheja and Thomas Noll},
   doi = {10.1007/978-3-319-96142-2_1},
   isbn = {9783319961422},
   issue = {401},
   pages = {3-11},
   title = {Let this Graph Be Your Witness!},
   year = {2018},
}
@article{Lee2018,
   abstract = {A key challenge in program synthesis concerns how to efficiently search for the desired program in the space of possible programs. We propose a general approach to accelerate search-based program synthesis by biasing the search towards likely programs. Our approach targets a standard formulation, syntax-guided synthesis (SyGuS), by extending the grammar of possible programs with a probabilistic model dictating the likelihood of each program. We develop a weighted search algorithm to efficiently enumerate programs in order of their likelihood. We also propose a method based on transfer learning that enables to effectively learn a powerful model, called probabilistic higher-order grammar, from known solutions in a domain. We have implemented our approach in a tool called Euphony and evaluate it on SyGuS benchmark problems from a variety of domains. We show that Euphony can learn good models using easily obtainable solutions, and achieves significant performance gains over existing general-purpose as well as domain-specific synthesizers.},
   author = {Woosuk Lee and Kihong Heo and Rajeev Alur and Mayur Naik},
   doi = {10.1145/3192366.3192410},
   issn = {15232867},
   issue = {4},
   journal = {ACM SIGPLAN Notices},
   keywords = {Domain-specific languages,Statistical methods,Synthesis,Transfer learning},
   month = {6},
   pages = {436-449},
   publisher = {Association for Computing Machinery},
   title = {Accelerating search-based program synthesis using learned probabilistic models},
   volume = {53},
   year = {2018},
}
@report{Author2018,
   abstract = {In the past decades, many software manufacturers have failed to adopt security patches timely because more and more vulnerabili-ties and bugs were discovered. To detect the unpatched binaries as soon as possible, both signature-based patch presence tests (SPPT) and software similarity based patch presence tests (SSPPT) have been proposed to check whether a certain patch has been applied to a released software binary. However, SPPTs only analyze function changes. Therefore, they cannot be applied to detect a large number of bug-fixing patches that are small in size and only modify function-irrelevant program elements. Meanwhile, SSPPT tools cannot precisely determine the presence of patches either due to the small size of patches.In this work, we propose PPTFI, a patch presence test for function-irrelevant patches. PPTFI first understands the function-irrelevant patches and then carefully extracts the code information and data information as patch signatures, which are later used to scan the target binaries. We have evaluated PPTFI over 62 different versions of 31 real-world function-irrelevant patches and 512 binaries in 16 different compilation environments, and the results show that PPFTI achieved the accuracy of 77.54%, whereas the state-of-the-art SPPT tools such as Fiber and BinXray cannot understand any of the function-irrelevant patches. Experimented on x86_64 platforms, PPTFI increases the accuracy of the state-of-the-art SSPPT tool, B2SFinder, by 13.79%.},
   author = {Anonymous Author},
   keywords = {patch presence test,patch semantics},
   title = {PPTFI: Patch Presence Test for Function-Irrelevant Patches},
   url = {https://doi.org/XXXXXXX.XXXXXXX},
   year = {2018},
}
@inproceedings{Ahmad2018,
   abstract = {MapReduce is a popular programming paradigm for developing large-scale, data-intensive computation. Many frameworks that implement this paradigm have recently been developed. To leverage these frameworks, however, developers must become familiar with their APIs and rewrite existing code. We present Casper, a new tool that automatically translates sequential Java programs into the MapReduce paradigm. Casper identifies potential code fragments to rewrite and translates them in two steps: (1) Casper uses program synthesis to search for a program summary (i.e., a functional specification) of each code fragment. The summary is expressed using a high-level intermediate language resembling the MapReduce paradigm and verified to be semantically equivalent to the original using a theorem prover. (2) Casper generates executable code from the summary, using either the Hadoop, Spark, or Flink API. We evaluated Casper by automatically converting realworld, sequential Java benchmarks to MapReduce. The resulting benchmarks perform up to 48.2× faster compared to the original.},
   author = {Maaz Bin Safeer Ahmad and Alvin Cheung},
   doi = {10.1145/3183713.3196891},
   isbn = {9781450317436},
   issn = {07308078},
   journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
   month = {5},
   pages = {1205-1220},
   publisher = {Association for Computing Machinery},
   title = {Automatically leveraging MapReduce frameworks for data-intensive applications},
   year = {2018},
}
@article{Brockschmidt2017,
   author = {Marc Brockschmidt and Yuxin Chen and Pushmeet Kohli and Siddharth Krishna and Daniel Tarlow},
   doi = {10.1007/978-3-319-66706-5_4},
   isbn = {9783319667058},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {66-87},
   title = {Learning shape analysis},
   volume = {10422 LNCS},
   year = {2017},
}
@article{Kanvar2017,
   abstract = {A points-To analysis computes a sound abstraction of heap memory conventionally using a name-based abstraction that summarizes runtime memory by grouping locations using the names of allocation sites: All concrete heap locations allocated by the same statement are grouped together. The locations in the same group are treated alike i.e., a pointer to any one location of the group is assumed to point to every location in the group leading to an over-Approximation of points-To relations. We propose an access-based abstraction that partitions each name-based group of locations into equivalence classes at every program point using an additional criterion of the sets of access paths (chains of pointer indirections) reaching the locations in the memory. The intuition is that the locations that are both allocated and accessed alike should be grouped into the same equivalence class. Since the access paths in the memory could reach different locations at different program points, our groupings change flow sensitively unlike the name-based groupings. This creates a more precise view of the memory. Theoretically, it is strictly more precise than the name-based abstraction except in some trivial cases; practically it is far more precise. Our empirical measurements show the benefits of our tool Access-Based Heap Analyzer (ABHA) on SPEC CPU 2006 and heap manipulating SV-COMP benchmarks. ABHA, which is field-, flow-, and context-sensitive, scales to 20 kLoC and can improve the precision even up to 99% (in terms of the number of aliases). Additionally, ABHA allows any user-defined summarization of an access path to be plugged in; we have implemented and evaluated four summarization techniques. ABHA can also act as a front-end to TVLA, a parametrized shape analyzer, in order to automate its parametrization by generating predicates that capture the program behaviour more accurately.},
   author = {Vini Kanvar and Uday P. Khedker},
   doi = {10.1145/3092255.3092267},
   isbn = {9781450350440},
   journal = {International Symposium on Memory Management, ISMM},
   keywords = {Access path,Alias,Allocation site,Heap abstraction,Static points-To analysis,Summarization},
   pages = {92-103},
   title = {What's in a name? Going beyond allocation site names in heap analysis},
   volume = {Part F1286},
   year = {2017},
}
@article{Li2017,
   abstract = {© 2017 ACM. To infer complex structural invariants, shape analyses rely on expressive families of logical properties. Many such analyses manipulate abstract memory states that consist of separating conjunctions of basic predicates describing atomic blocks or summaries. Moreover, they use finite disjunctions of abstract memory states in order to account for dissimilar shapes. Disjunctions should be kept small for scalability, though precision often requires keeping additional case splits. In this context, deciding when and how to merge case splits and to replace them with summaries is critical both for precision and efficiency. Existing techniques use sets of syntactic rules, which are tedious to design and prone to failure. In this paper, we design a semantic criterion to clump abstract states based on their silhouette, which applies not only to the conservative union of disjuncts but also to the weakening of separating conjunctions of memory predicates into inductive summaries. Our approach allows us to define union and widening operators that aim at preserving the case splits that are required for the analysis to succeed. We implement this approach in the MemCAD analyzer and evaluate it on real-world C codes from existing libraries dealing with doubly-linked lists, red-black trees, AVL-trees and splay-trees.},
   author = {Huisong Li and Francois Berenger and Bor Yuh Evan Chang and Xavier Rival},
   doi = {10.1145/3009837.3009881},
   isbn = {9781450346603},
   issn = {07308566},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   keywords = {Abstract interpretation,Clumping of disjuncts,Disjunctions,Heap abstraction,Separation logics,Silhouette,Static analysis},
   pages = {32-45},
   title = {Semantic-directed clumping of disjunctive abstract states},
   year = {2017},
}
@article{Doctoral2017,
   author = {Programme Doctoral and E N Informatique and E T Communications},
   title = {Algorithmic Resource Verification},
   volume = {7885},
   year = {2017},
}
@article{Xie2017,
   abstract = {Loop termination is an important problem for proving the correctness of a system and ensuring that the system always reacts. Existing loop termination analysis techniques mainly depend on the synthesis of ranking functions, which is often expensive. In this paper, we present a novel approach, named Loopster, which performs an efficient static analysis to decide the termination for loops based on path termination analysis and path dependency reasoning. Loopster adopts a divide-and-conquer approach: (1) we extract individual paths from a target multi-path loop and analyze the termination of each path, (2) analyze the dependencies between each two paths, and then (3) determine the overall termination of the target loop based on the relations among paths. We evaluate Loopster by applying it on the loop termination competition benchmark and three real-world projects. The results show that Loopster is effective in a majority of loops with better accuracy and 20 ×+ performance improvement compared to the state-of-the-art tools.},
   author = {Xiaofei Xie and Bihuan Chen and Liang Zou and Shang Wei Lin and Yang Liu and Xiaohong Li},
   doi = {10.1145/3106237.3106260},
   isbn = {9781450351058},
   journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
   keywords = {Loop termination,Path dependency automaton,Reachability},
   pages = {84-94},
   title = {Loopster: Static loop termination analysis},
   volume = {Part F1301},
   year = {2017},
}
@article{Dietrich2017,
   abstract = {Software projects that use a compiled language are built hundreds of thousands of times during their lifespan. Hence, the compiler is invoked over and over again on an incrementally changing source base. As previous work has shown, up to 97 percent of these invocations are re-dundant and do not lead to an altered compilation result. In order to avoid such redundant builds, many developers use caching tools that are based on textual hashing of the source files. However, these tools fail in the presence of modifications that leave the compilation result unchanged. Especially for C projects, where module-interface defi-nitions are imported textually with the C preprocessor, modifications to header files lead to many redundant com-pilations. In this paper, we present the cHash approach and com-piler extension to quickly detect modifications on the language level that will not lead to a changed compilation result. By calculating a hash over the abstract syntax tree, we achieve a high precision at comparatively low costs. While cHash is light-weight and build system agnostic, it can cancel 80 percent of all compiler invocations early and reduce the build-time of incremental builds by up to 51 percent. In comparison to the state-of-the-art CCache tool, cHash is at least 30 percent more precise in detecting redundant compilations.},
   author = {Christian Dietrich and Valentin Rothberg and Ludwig Füracker and Andreas Ziegler and Daniel Lohmann},
   isbn = {978-1-931971-38-6},
   journal = {Atc'17},
   pages = {527-538},
   title = {cHash: Detection of Redundant Compilations via AST Hashing},
   url = {https://www.usenix.org/conference/atc17/technical-sessions/presentation/dietrich},
   year = {2017},
}
@article{Babati2017,
   author = {Bence Babati and Norbert Pataki},
   doi = {10.15439/2017f358},
   journal = {Communiation Papers of the 2017 Federated Conference on Computer Science and Information Systems},
   pages = {149-156},
   title = {Analysis of Include Dependencies in C++ Source Code},
   volume = {13},
   year = {2017},
}
@article{Feng2017,
   abstract = {This paper presents a novel component-based synthesis algorithm that marries the power of type-directed search with lightweight SMT-based deduction and partial evaluation. Given a set of components together with their over-approximate first-order specifications, our method first generates a program sketch over a subset of the components and checks its feasibility using an SMT solver. Since a program sketch typically represents many concrete programs, the use of SMT-based deduction greatly increases the scalability of the algorithm. Once a feasible program sketch is found, our algorithm completes the sketch in a bottom-up fashion, using partial evaluation to further increase the power of deduction for rejecting partially-filled program sketches. We apply the proposed synthesis methodology for automating a large class of data preparation tasks that commonly arise in data science. We have evaluated our synthesis algorithm on dozens of data wrangling and consolidation tasks obtained from on-line forums, and we show that our approach can automatically solve a large class of problems encountered by R users.},
   author = {Yu Feng and Ruben Martins and Jacob Van Geffen and Isil Dillig and Swarat Chaudhuri},
   doi = {10.1145/3062341.3062351},
   isbn = {9781450349888},
   issn = {15232867},
   issue = {6},
   journal = {ACM SIGPLAN Notices},
   keywords = {Component-based synthesis,Data preparation,Program synthesis,Programming by example,SMT-based deduction},
   month = {6},
   pages = {422-436},
   publisher = {Association for Computing Machinery},
   title = {Component-based synthesis of table consolidation and transformation tasks from examples},
   volume = {52},
   year = {2017},
}
@article{Wang2017,
   abstract = {SQL is the de facto language for manipulating relational data. Though powerful, many users find it difficult to write SQL queries due to highly expressive constructs. While using the programming-by-example paradigm to help users write SQL queries is an attractive proposition, as evidenced by online help forums such as Stack Overflow, developing techniques for synthesizing SQL queries from given input-output (I/O) examples has been difficult, due to the large space of SQL queries as a result of its rich set of operators. In this paper, we present a new scalable and efficient algorithm for synthesizing SQL queries based on I/O examples. The key innovation of our algorithm is development of a language for abstract queries, i.e., queries with uninstantiated operators, that can be used to express a large space of SQL queries efficiently. Using abstract queries to represent the search space nicely decomposes the synthesis problem into two tasks: 1) searching for abstract queries that can potentially satisfy the given I/O examples, and 2) instantiating the found abstract queries and ranking the results. We have implemented this algorithm in a new tool called Scythe and evaluated it using 193 benchmarks collected from Stack Overflow. Our evaluation shows that Scythe can efficiently solve 74% of the benchmarks, most in just a few seconds, and the queries range from simple ones involving a single selection to complex queries with 6 nested subqueires.},
   author = {Chenglong Wang and Alvin Cheung and Rastislav Bodik},
   doi = {10.1145/3062341.3062365},
   isbn = {9781450349888},
   issn = {15232867},
   issue = {6},
   journal = {ACM SIGPLAN Notices},
   keywords = {Program Synthesis,Query by Example,SQL},
   month = {6},
   pages = {452-466},
   publisher = {Association for Computing Machinery},
   title = {Synthesizing highly expressive SQL queries from input-output examples},
   volume = {52},
   year = {2017},
}
@article{Abdulla2016,
   author = {Parosh Aziz Abdulla and Lukáš Holík and Bengt Jonsson and Ondřej Lengál and Cong Quy Trinh and Tomáš Vojnar},
   doi = {10.1007/s00236-015-0235-0},
   issn = {14320525},
   issue = {4},
   journal = {Acta Informatica},
   pages = {357-385},
   title = {Verification of heap manipulating programs with ordered data by extended forest automata},
   volume = {53},
   year = {2016},
}
@article{Haller2016,
   abstract = {Many existing techniques for reversing data structures in C/C++ binaries are limited to low-level programming constructs, such as individual variables or structs. Unfor- tunately, without detailed information about a program’s pointer structures, forensics and reverse engineering are exceedingly hard. To fill this gap, we propose MemPick, a tool that detects and classifies high-level data structures used in stripped binaries. By analyzing how links between memory objects evolve throughout the program execution, it distinguishes between many commonly used data structures, such as singly- or doubly-linked lists, many types of trees (e.g., AVL, red-black trees, B-trees), and graphs. We evaluate the technique on 10 real world applications, 4 file system implementations and 16 popular libraries. The results show that MemPick can identify the data structures with high accuracy.},
   author = {Istvan Haller and Asia Slowinska and Herbert Bos},
   doi = {10.1007/s10664-015-9363-y},
   issn = {15737616},
   issue = {3},
   journal = {Empirical Software Engineering},
   keywords = {Data structures,Dynamic binary analysis},
   pages = {778-810},
   publisher = {Empirical Software Engineering},
   title = {Scalable data structure detection and classification for C/C++ binaries},
   volume = {21},
   url = {http://dx.doi.org/10.1007/s10664-015-9363-y},
   year = {2016},
}
@article{Xie2016,
   abstract = {Loops are challenging structures for program analysis, especial-ly when loops contain multiple paths with complex interleaving executions among these paths. In this paper, we first propose a classification of multi-path loops to understand the complexity of the loop execution, which is based on the variable updates on the loop conditions and the execution order of the loop paths. Second-ly, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects on the variables. The key contribution is to use a path dependency automaton (PDA) to capture the execution dependency between the paths. A DFS-based algorithm is proposed to traverse the PDA to summarize the effect for all feasible executions in the loop. The experimental results show that Proteus is effective in three applications: Proteus can 1) compute a more precise bound than the existing loop bound analysis techniques; 2) significantly outperform state-of-the-art tools for loop verification; and 3) generate test cases for deep loops within one second, while KLEE and Pex either need much more time or fail.},
   author = {Xiaofei Xie and Bihuan Chen and Yang Liu and Wei Le and Xiaohong Li},
   doi = {10.1145/2950290.2950340},
   isbn = {9781450342186},
   journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
   keywords = {Disjunctive Summary,Loop Summarization},
   pages = {61-72},
   title = {Proteus: Computing disjunctive loop summary via path dependency analysis},
   volume = {13-18-Nove},
   year = {2016},
}
@article{Li2016,
   abstract = {© 2017 EAI. In this paper, from the perspective of human ergonomics, we analyze the movement of the joints in the process of human body movements, and we establish a dynamic model according to the human skeleton structure. On this basis, from the rigid body dynamics point of view, combined with the principle of inertial navigation, a body sensor network based on MEMS inertial sensors is built to capture human body motion in real time. On the basis of space trajectory of human body movement and traditional human motion solution strategy, a human motion solution strategy based on particle filter fusion solution is proposed to realize the prediction of human motion analysis. Therefore, we evaluate the performance of the designed system by comparing with the real motion. Finally, in order to verify the human motion data, the motion capture data verification platforms are established. Experimental results show that the proposed joint attitude solution algorithm can achieve a relatively smooth tracking effect and provides a certain reference value.},
   author = {Jie Li and Zhe Long Wang and Hongyu Zhao and Raffaele Gravina and Giancarlo Fortino and Yongmei Jiang and Kai Tang},
   doi = {10.1145/0000000.0000000},
   issn = {23103582},
   issue = {212},
   journal = {BodyNets International Conference on Body Area Networks},
   keywords = {Body sensor network,Inertial navigation,Motion capture,Particle filter},
   title = {Networked human motion capture system based on quaternion navigation},
   volume = {V},
   year = {2016},
}
@article{Zhang2016,
   abstract = {Software building is recurring and time-consuming. Based on the finding that a significant portion of compilations in incremental build is unnecessary, we propose bypath compilation, an efficient build technique that avoids unnecessary recompila- tion with automated detection of redundant dependencies and unessential changes in source files. The technique is lightweight and transparent to software developers, and can be easily applied to existing build systems. We evaluated our approach on a set of real-world open source projects. The results show that 83% ~ 97% of the recompilations are unnecessary and our approach can accelerate the incremental build up to 44.20%.},
   author = {Ying Zhang and Yanyan Jiang and Chang Xu and Xiaoxing Ma and Ping Yu},
   doi = {10.1109/APSEC.2015.27},
   isbn = {9781467396448},
   issn = {15301362},
   journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
   keywords = {Build system,Bypath compilation,Incremental build},
   pages = {182-189},
   publisher = {IEEE},
   title = {ABC: Accelerated building of C/C++ projects},
   volume = {2016-May},
   year = {2016},
}
@article{Techopedia2016,
   author = {Techopedia},
   issue = {May},
   journal = {Career: Analysis and Design},
   keywords = {@Career},
   pages = {1-28},
   title = {What is a Use Case?},
   url = {https://www.techopedia.com/definition/25813/use-case},
   year = {2016},
}
@article{Babati2016,
   author = {Bence Babati and Norbert Pataki and Zoltán Porkoláb},
   doi = {10.1109/Informatics.2015.7377804},
   isbn = {9781467398688},
   journal = {2015 IEEE 13th International Scientific Conference on Informatics, INFORMATICS 2015 - Proceedings},
   pages = {36-40},
   title = {C/C++ Preprocessing with modern data storage devices},
   year = {2016},
}
@article{Heinen2015,
   abstract = {This paper argues that graph grammars naturally model dynamic data structures such as lists, trees and combinations thereof. These grammars can be exploited to obtain finite abstractions of pointer-manipulating programs, thus enabling model checking. Experimental results for verifying Lindstrom's variant of the Deutsch-Schorr-Waite tree traversal algorithm illustrate this.},
   author = {Jonathan Heinen and Christina Jansen and Joost Pieter Katoen and Thomas Noll},
   doi = {10.1016/j.scico.2013.11.012},
   issn = {01676423},
   issue = {P1},
   journal = {Science of Computer Programming},
   keywords = {Dynamic data structures,Hyperedge replacement grammars,Java bytecode,Verification},
   pages = {157-162},
   publisher = {Elsevier B.V.},
   title = {Verifying pointer programs using graph grammars},
   volume = {97},
   url = {http://dx.doi.org/10.1016/j.scico.2013.11.012},
   year = {2015},
}
@article{Friggens2015,
   abstract = {Canonical abstraction is a static analysis technique that represents states as 3-valued logical structures, and produces finite abstract systems. Despite providing a finite bound, these abstractions may still suffer from the state explosion problem. Notably, for concurrent programs with arbitrary interleaving, if threads in a state are abstracted based on their location, then the number of locations will be a combinatorial factor in the size of the statespace. We present an approach using canonical abstraction that avoids this state explosion by "collapsing" all of the threads in a state into a single abstract representative. Properties of threads that would be lost by the abstraction, but are needed for verification, are retained by defining conditional "soft invariant" instrumentation predicates. This technique is used to adapt previous models for verifying linearizability of nonblocking concurrent data structure algorithms, resulting in exponentially smaller statespaces.},
   author = {David Friggens and Lindsay Groves},
   title = {Collapsing Threads Safely with Soft Invariants},
   url = {http://arxiv.org/abs/1512.09186},
   year = {2015},
}
@article{Smaragdakis2015,
   abstract = {During the past twenty-one years, over seventy-five papers and nine Ph.D. theses have been published on pointer analysis. Given the tomes of work on this topic one may wonder, “Haven'trdquo; we solved this problem yet?''  With input from many researchers in the field, this paper describes issues related to pointer analysis and remaining open problems.},
   author = {Yannis Smaragdakis and George Balatsouras},
   doi = {10.1561/2500000014},
   issn = {2325-1107},
   issue = {1},
   journal = {Foundations and Trends® in Programming Languages},
   pages = {1-69},
   title = {Pointer Analysis},
   volume = {2},
   year = {2015},
}
@article{Xie2015,
   abstract = {? 2015 ACM.Loops are important yet most challenging program constructs to analyze for various program analysis tasks. Existing loop analysis techniques mainly handle well loops that contain only integer variables with a single path in the loop body. The key challenge in summarizing a multiple-path loop is that a loop traversal can yield a large number of possibilities due to the different execution orders of these paths located in the loop; when a loop contains a conditional branch related to string content, we potentially need to track every character in the string for loop summarization, which is expensive. In this paper, we propose an approach, named S-Looper, to automatically summarize a type of loops related to a string traversal. This type of loops can contain multiple paths, and the branch conditions in the loop can be related to string content. Our approach is to identify patterns of the string based on the branch conditions along each path in the loop. Based on such patterns, we then generate a loop summary that describes the path conditions of a loop traversal as well as the symbolic values of each variable at the exit of a loop. Combined with vulnerability conditions, we are thus able to generate test inputs that traverse a loop in a specific way and lead to exploitation. Our experiments show that handling such string loops can largely improve the buffer overflow detection capabilities of the existing symbolic analysis tool. We also compared our techniques with KLEE and PEX, and show that we can generate test inputs more effectively and efficiently.},
   author = {Xiaofei Xie and Yang Liu and Wei Le and Xiaohong Li and Hongxu Chen},
   doi = {10.1145/2771783.2771815},
   isbn = {9781450336208},
   journal = {2015 International Symposium on Software Testing and Analysis, ISSTA 2015 - Proceedings},
   keywords = {Loop summarization,String constraints,Symbolic execution},
   pages = {188-198},
   title = {S-Looper: Automatic summarization for multipath string loops},
   year = {2015},
}
@article{Lvov2015,
   author = {M. S. Lvov},
   doi = {10.1007/s10559-015-9736-7},
   issn = {15738337},
   issue = {3},
   journal = {Cybernetics and Systems Analysis},
   keywords = {Invariant polynomial,Linear loop,Loop invariant,Static analysis of programs},
   pages = {448-460},
   title = {Software–hardware systems: The structure of polynomial invariants of linear loops},
   volume = {51},
   year = {2015},
}
@article{Blazy2015,
   abstract = {Summary: This book constitutes the refereed proceedings of the 22nd International Static Analysis Symposium, SAS 2015, held in Saint-Malo, France, in September 2015. The 18 papers presented in this volume were carefully reviewed and selected from 44 submissions. All fields of static analysis as a fundamental tool for program verification, bug detection, compiler optimization, program understanding, and software maintenance are addressed, featuring theoretical, practical, and application advances in the area.},
   author = {Sandrine Blazy and Thomas Jensen},
   doi = {10.1007/978-3-662-48288-9},
   isbn = {9783662482872},
   issn = {16113349},
   issue = {295311},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {145-161},
   title = {Static Analysis: 22nd International Symposium, SAS 2015 Saint-Malo, France, September 9-11, 2015 Proceedings},
   volume = {9291},
   year = {2015},
}
@article{Toubhans2014,
   abstract = {© Springer International Publishing Switzerland 2014. The breadth and depth of heap properties that can be inferred by the union of today’s shape analyses is quite astounding. Yet, achieving scalability while supporting a wide range of complex data structures in a generic way remains a long-standing challenge. In this paper, we propose a way to side-step this issue by defining a generic abstract domain combinator for combining memory abstractions on disjoint regions. In essence, our abstract domain construction is to the separating conjunction in separation logic as the reduced product construction is to classical, non-separating conjunction. This approach eases the design of the analysis as memory abstract domains can be re-used by applying our separating conjunction domain combinator. And more importantly, this combinator enables an analysis designer to easily create a combined domain that applies computationally-expensive abstract domains only where it is required.},
   author = {Antoine Toubhans and Bor Yuh Evan Chang and Xavier Rival},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {285-301},
   title = {An abstract domain combinator for separately conjoining memory abstractions},
   volume = {8723},
   year = {2014},
}
@inproceedings{Piskac2014,
   author = {Ruzica Piskac and Thomas Wies and Damien Zufferey},
   doi = {10.1007/978-3-319-08867-9_47},
   isbn = {9783319088662},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {711-728},
   title = {Automating separation logic with trees and data},
   volume = {8559 LNCS},
   year = {2014},
}
@article{Cousot2014,
   abstract = {Abstract interpretation is a theory of abstraction and constructive approximation of the mathematical structures used in the formal description of complex or infinite systems and the inference or verification of their combinatorial or undecidable properties. Developed in the late seventies, it has been since then used, implicitly or explicitly, to many aspects of computer science (such as static analysis and verification, contract inference, type inference, termination inference, model-checking, abstraction/refinement, program transformation (including watermarking, obfuscation, etc), combination of decision procedures, security, malware detection, database queries, etc) and more recently, to system biology and SAT/SMT solvers. Production-quality verification tools based on abstract interpretation are available and used in the advanced software, hardware, transportation, communication, and medical industries. The talk will consist in an introduction to the basic notions of abstract interpretation and the induced methodology for the systematic development of sound abstract interpretation-based tools. Examples of abstractions will be provided, from semantics to typing, grammars to safety, reachability to potential/definite termination, numerical to protein-protein abstractions, as well as applications (including those in industrial use) to software, hardware and system biology. This paper is a general discussion of abstract interpretation, with selected publications, which unfortunately are far from exhaustive both in the considered themes and the corresponding references.},
   author = {Patrick Cousot and Radhia Cousot},
   doi = {10.1145/2603088.2603165},
   isbn = {9781450328869},
   journal = {Proceedings of the Joint Meeting of the 23rd EACSL Annual Conference on Computer Science Logic, CSL 2014 and the 29th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS 2014},
   keywords = {Abstract interpretation,Proof,Semantics,Static Analysis,Verification},
   title = {Abstract interpretation: Past, present and future},
   year = {2014},
}
@article{Maisonneuve2014,
   abstract = {Using abstract interpretation, invariants are usually obtained by solving iteratively a system of equations linking preconditions according to program statements. However, it is also possible to abstract first the statements as transformers, and then propagate the preconditions using the transformers. The second approach is modular because procedures and loops can be abstracted once and for all, avoiding an iterative resolution over the call graph and all the control flow graphs. However, the transformer approach based on polyhedral abstract domains encurs two penalties: some invariant accuracy may be lost when computing transformers, and the execution time may increase exponentially because the dimension of a transformer is twice the dimension of a precondition. The purposes of this article are 1) to measure the benefits of the modular approach and its drawbacks in terms of execution time and accuracy using significant examples and a newly developed benchmark for loop invariant analysis, ALICe, 2) to present a new technique designed to reduce the accuracy loss when computing transformers, 3) to evaluate experimentally the accuracy gains this new technique and other previously discussed ones provide with ALICe test cases and 4) to compare the executions times and accuracies of different tools, ASPIC, ISL, PAGAI and PIPS. Our results suggest that the transformer-based approach used in PIPS, once improved with transformer lists, is as accurate as the other tools when dealing with the ALICe benchmark. Its modularity nevertheless leads to shorter execution times when dealing with nested loops and procedure calls found in real applications. © 2014 Elsevier B.V. All rights reserved.},
   author = {Vivien Maisonneuve and Olivier Hermant and François Irigoin},
   doi = {10.1016/j.entcs.2014.08.003},
   issn = {15710661},
   issue = {Nsad},
   journal = {Electronic Notes in Theoretical Computer Science},
   keywords = {abstract interpretation,automatic invariant detection,benchmark,linear relation analysis,loop invariant,model checking,static program analysis,transformer},
   pages = {17-31},
   title = {Computing invariants with transformers: Experimental scalability and accuracy},
   volume = {307},
   year = {2014},
}
@article{Jeannet2014,
   abstract = {We present abstract acceleration techniques for computing loop invariants for numerical programs with linear assignments and conditionals. Whereas abstract interpretation techniques typically over-approximate the set of reachable states iteratively, abstract acceleration captures the effect of the loop with a single, non-iterative transfer function applied to the initial states at the loop head. In contrast to previous acceleration techniques, our approach applies to any linear loop without restrictions. Its novelty lies in the use of the Jordan normal form decomposition of the loop body to derive symbolic expressions for the entries of the matrix modeling the effect of n>=0 iterations of the loop. The entries of such a matrix depend on $n$ through complex polynomial, exponential and trigonometric functions. Therefore, we introduces an abstract domain for matrices that captures the linear inequality relations between these complex expressions. This results in an abstract matrix for describing the fixpoint semantics of the loop. Our approach integrates smoothly into standard abstract interpreters and can handle programs with nested loops and loops containing conditional branches. We evaluate it over small but complex loops that are commonly found in control software, comparing it with other tools for computing linear loop invariants. The loops in our benchmarks typically exhibit polynomial, exponential and oscillatory behaviors that present challenges to existing approaches. Our approach finds non-trivial invariants to prove useful bounds on the values of variables for such loops, clearly outperforming the existing approaches in terms of precision while exhibiting good performance.},
   author = {Bertrand Jeannet and Peter Schrammel and Sriram Sankaranarayanan},
   isbn = {9781450325448},
   issn = {15232867},
   issue = {1},
   journal = {ACM SIGPLAN Notices},
   pages = {529-540},
   title = {Abstract acceleration of general linear loops},
   volume = {49},
   year = {2014},
}
@article{Head2014,
   author = {Informatics Head},
   title = {Analysis and Methods for Supporting Generative Metaprogramming in Large Scale C ++ Projects},
   year = {2014},
}
@article{Piskac2013,
   abstract = {Separation logic (SL) has gained widespread popularity because of its ability to succinctly express complex invariants of a program's heap con-figurations. Several specialized provers have been developed for decidable SL fragments. However, these provers cannot be easily extended or combined with solvers for other theories that are important in program verification, e.g., linear arithmetic. In this paper, we present a reduction of decidable SL fragments to a decidable first-order theory that fits well into the satisfiability modulo theories (SMT) framework. We show how to use this reduction to automate satisfiability, entailment, frame inference, and abduction problems for separation logic using SMT solvers. Our approach provides a simple method of integrating separation logic into existing verification tools that provide SMT backends, and an elegant way of combining SL fragments with other decidable first-order theories. We im-plemented this approach in a verification tool and applied it to heap-manipulating programs whose verification involves reasoning in theory combinations.},
   author = {Ruzica Piskac and Thomas Wies and Damien Zufferey},
   doi = {10.1007/978-3-642-39799-8_54},
   isbn = {9783642397981},
   issn = {03029743},
   journal = {Cav},
   pages = {773-789},
   title = {Automating Separation Logic Using SMT (Technical Report)},
   volume = {8044},
   year = {2013},
}
@article{Kreiker2013,
   abstract = {We present a framework for interprocedural shape analysis, which is context- and flow-sensitive with the ability to perform destructive pointer updates. We limit our attention to cutpoint-free programs—programs in which reasoning on a procedure call only requires consideration of context reachable from the actual parameters. For such programs, we show that our framework is able to perform an efficient modular analysis. Technically, our analysis computes procedure summaries as transformers from inputs to outputs while ignoring parts of the heap not relevant to the procedure. This makes the analysis modular in the heap and thus allows reusing the effect of a procedure at different call-sites and even between different contexts occurring at the same call-site. We have implemented a prototype of our framework and used it to verify interesting properties of cutpoint-free programs, including partial correctness of a recursive quicksort implementation.},
   author = {J. Kreiker and T. Reps and N. Rinetzky and M. Sagiv and Reinhard Wilhelm and E. Yahav},
   doi = {10.1007/978-3-642-37651-1_17},
   isbn = {9783642376504},
   issn = {03029743},
   issue = {304},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {414-445},
   title = {Interprocedural shape analysis for effectively cutpoint-free programs},
   volume = {7797 LNCS},
   year = {2013},
}
@article{Chang2013,
   abstract = {The aim of static analysis is to infer invariants about programs that are precise enough to establish semantic properties, such as the absence of run-time errors. Broadly speaking, there are two major branches of static analysis for imperative programs. Pointer and shape analyses focus on inferring properties of pointers, dynamically-allocated memory, and recursive data structures, while numeric analyses seek to derive invariants on numeric values. Although simultaneous inference of shapenumeric invariants is often needed, this case is especially challenging and is not particularly well explored. Notably, simultaneous shape-numeric inference raises complex issues in the design of the static analyzer itself. In this paper, we study the construction of such shape-numeric, static analyzers. We set up an abstract interpretation framework that allows us to reason about simultaneous shape-numeric properties by combining shape and numeric abstractions into a modular, expressive abstract domain. Such a modular structure is highly desirable to make its formalization and implementation easier to do and get correct. To achieve this, we choose a concrete semantics that can be abstracted step-by-step, while preserving a high level of expressiveness. The structure of abstract operations (i.e., transfer, join, and comparison) follows the structure of this semantics. The advantage of this construction is to divide the analyzer in modules and functors that implement abstractions of distinct features. © B.-Y. E. Chang and X. Rival.},
   author = {Bor Yuh Evan Chang and Xavier Rival},
   doi = {10.4204/EPTCS.129.11},
   issn = {20752180},
   issue = {Section 2},
   journal = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
   pages = {161-185},
   title = {Modular construction of shape-numeric analyzers},
   volume = {129},
   year = {2013},
}
@article{Qin2013,
   abstract = {Automated verification of memory safety and functional correctness for heap-manipulating programs has been a challenging task, especially when dealing with complex data structures with strong invariants involving both shape and numerical properties. Existing verification systems usually rely on users to supply annotations to guide the verification, which can be cumbersome and error-prone by hand and can significantly restrict the usability of the verification system. In this paper, we reduce the need for some user annotations by automatically inferring loop invariants over an abstract domain with both shape and numerical information. Our loop invariant synthesis is conducted automatically by a fixed-point iteration process, equipped with newly designed abstraction mechanism, together with join and widening operators over the combined domain. We have also proven the soundness and termination of our approach. Initial experiments confirm that we can synthesise loop invariants with non-trivial constraints. © 2012 Elsevier B.V.},
   author = {Shengchao Qin and Guanhua He and Chenguang Luo and Wei Ngan Chin and Xin Chen},
   doi = {10.1016/j.jsc.2012.08.007},
   issn = {07477171},
   journal = {Journal of Symbolic Computation},
   keywords = {Abstraction,Combining analysis,Fixpoint analysis,Loop invariant,Numerical analysis,Separation logic,Shape analysis},
   pages = {386-408},
   title = {Loop invariant synthesis in a combined abstract domain},
   volume = {50},
   year = {2013},
}
@article{Ullio2012,
   abstract = {These are the notes to accompany a course at the Marktoberdorf PhD summer school in 2011. The course consists of an introduction to separation logic, with a slant towards its use in automatic program verification and analysis.},
   author = {R. Ullio and N. Riva and P. Pellegrino and P. Deloo},
   isbn = {9789290922551},
   issn = {03796566},
   journal = {European Space Agency, (Special Publication) ESA SP},
   keywords = {abstract interpretation,automatic program verification,program logic},
   pages = {286-318},
   title = {LSD (Landing system development) impact simulation},
   volume = {691 SP},
   year = {2012},
}
@article{Li2012,
   author = {Yi Li},
   doi = {10.3724/SP.J.1001.2012.03982},
   issn = {10009825},
   issue = {5},
   journal = {Ruan Jian Xue Bao/Journal of Software},
   keywords = {DISCOVERER,Nonlinear loop,Termination analysis,Trustworthy computing},
   pages = {1045-1052},
   title = {Termination analysis of nonlinear loops},
   volume = {23},
   year = {2012},
}
@article{Lvov2012,
   author = {M. S. Lvov and V. A. Kreknin},
   doi = {10.1007/s10559-012-9406-y},
   issn = {15738337},
   issue = {2},
   journal = {Cybernetics and Systems Analysis},
   keywords = {Automatic generation problem,Eigenpolynomial of a linear operator,Polynomial loop invariant,Static program analysis},
   pages = {268-281},
   title = {Nonlinear invariants for linear loops and eigenpolynomials of linear operators},
   volume = {48},
   year = {2012},
}
@article{Yu2012,
   author = {Yijun Yu},
   title = {Faster Compilation through Lighter Precompilation},
   year = {2012},
}
@article{Zhao2012,
   abstract = {Scripting languages are widely used to quickly accomplish a variety of tasks because of the high productivity they enable. Among other reasons, this increased productivity results from a combination of extensive libraries, fast development cycle, dynamic typing, and polymorphism. The dynamic features of scripting languages are traditionally associated with interpreters, which is the approach used to implement most scripting languages. Although easy to implement, interpreters are generally slow, which makes scripting languages prohibitive for implementing large, CPU-intensive applications. This efficiency problem is particularly important for PHP given that it is the most commonly used language for server-side web development. This paper presents the design, implementation, and an evaluation of the HipHop compiler for PHP. HipHop goes against the standard practice and implements a very dynamic language through static compilation. After describing the most challenging PHP features to support through static compilation, this paper presents HipHop's design and techniques that support almost all PHP features. We then present a thorough evaluation of HipHop running both standard benchmarks and the Facebook web site. Overall, our experiments demonstrate that HipHop is about 5.5x faster than standard, interpreted PHP engines. As a result, HipHop has reduced the number of servers needed to run Facebook and other web sites by a factor between 4 and 6, thus drastically cutting operating costs. },
   author = {Haiping Zhao and Iain Proctor and Minghui Yang and Xin Qi and Mark Williams and Qi Gao and Guilherme Ottoni and Andrew Paroski and Scott Mac Vicar and Jason Evans and Stephen Tu},
   doi = {10.1145/2398857.2384658},
   isbn = {9781450315616},
   issn = {15232867},
   issue = {10},
   journal = {ACM SIGPLAN Notices},
   keywords = {C++,Compilation,Dynamic languages,PHP},
   pages = {575-585},
   title = {The HipHop compiler for PHP},
   volume = {47},
   year = {2012},
}
@article{Analysis2011,
   author = {Collection Analysis},
   pages = {2019-2021},
   title = {Reading Lists （ Updating ） Container / Collection Analysis Redundant Header File Analysis ( Unclassified )},
   year = {2011},
}
@article{Krishnamurti2011,
   abstract = {Using the artifice of the i-device as an analogue, this paper examines issues in making design software more accessible through projects on computer-aided sus-tainable design, panelization for design and fabrication, and parametric shape grammar interpretation. In each case accessibility is improved by transitioning design from a representational concern to one that is more process oriented through the use of localized semantics.},
   author = {Ramesh Krishnamurti},
   journal = {SDC’10: NSF International Workshop on Studying Visual and Spatial Reasoning for Design Creativity},
   pages = {1-20},
   title = {Bridging parametric shape and parametric design”},
   year = {2011},
}
@article{Sotin2011,
   abstract = {Policy Iteration is an algorithm for the exact solving of optimization and game theory problems, formulated as equations on min max affine expressions. It has been shown that the problem of finding the least fixpoint of semantic equations on some abstract domains can be reduced to such optimization problems. This enables the use of Policy Iteration to solve such equations, instead of the traditional Kleene iteration that performs approximations to ensure convergence. We first show in this paper that the concept of Policy Iteration can be integrated into numerical abstract domains in a generic way. This allows to widen considerably its applicability in static analysis. We then consider the verification of programs manipulating Boolean and numerical variables, and we provide an efficient method to integrate the concept of policy in a logico-numerical abstract domain that mixes Boolean and numerical properties. Our experiments show the benefit of our approach compared to a naive application of Policy Iteration to such programs. ? 2011 Springer-Verlag.},
   author = {Pascal Sotin and Bertrand Jeannet and Franck Védrine and Eric Goubault},
   doi = {10.1007/978-3-642-24372-1_21},
   isbn = {9783642243714},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {290-305},
   title = {Policy iteration within logico-numerical abstract domains},
   volume = {6996 LNCS},
   year = {2011},
}
@article{Godefroid2011,
   abstract = {Whitebox fuzzing extends dynamic test generation based on sym- bolic execution and constraint solving from unit testing to whole- application security testing. Unfortunately, input-dependent loops may cause an explosion in the number of constraints to be solved and in the number of execution paths to be explored. In practice, whitebox fuzzers arbitrarily bound the number of constraints and paths due to input-dependent loops, at the risk of missing code and bugs. In thiswork, we investigate the use of simple loop-guard pattern- matching rules to automatically guess an input constraint defining the number of iterations of input-dependent loops during dynamic symbolic execution. We discover the loop structure of the program on the fly, detect induction variables, which are variables modified by a constant value during loop iterations, and infer simple partial loop invariants relating the value of such variables. Whenever a guess is confirmed later during the current dynamic symbolic ex- ecution, we then inject new constraints representing pre and post loop conditions, effectively summarizing sets of executions of that loop. These pre and post conditions are derived from partial loop invariants synthesized dynamically using pattern-matching rules on the loop guards and induction variables,without requiring any static analysis, theoremproving, or input-format specification. This tech- nique has been implemented in the whitebox fuzzer SAGE, scales to large programs with many nested loops, and we present results of experiments with aWindows 7 image parser},
   author = {Patrice Godefroid and Daniel Luchaup},
   doi = {10.1145/2001420.2001424},
   isbn = {9781450305624},
   journal = {2011 International Symposium on Software Testing and Analysis, ISSTA 2011 - Proceedings},
   keywords = {loop invariant generation,program summarization,program testing and verification},
   pages = {23-33},
   title = {Automatic partial loop summarization in dynamic test generation},
   year = {2011},
}
@article{Goubault2011,
   abstract = {We define several abstract semantics for the static analysis of finite precision computations, that bound not only the ranges of values taken by numerical variables of a program, but also the difference with the result of the same sequence of operations in an idealized real number semantics. These domains point out with more or less detail (control point, block, function for instance) sources of numerical errors in the program and the way they were propagated by further computations, thus allowing to evaluate not only the rounding error, but also sensitivity to inputs or parameters of the program. We describe two classes of abstractions, a non relational one based on intervals, and a weakly relational one based on parametrized zonotopic abstract domains called affine sets, especially well suited for sensitivity analysis and test generation. These abstract domains are implemented in the Fluctuat static analyzer, and we finally present some experiments. ? 2011 Springer-Verlag.},
   author = {Eric Goubault and Sylvie Putot},
   doi = {10.1007/978-3-642-18275-4_17},
   isbn = {9783642182747},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {232-247},
   title = {Static analysis of finite precision computations},
   volume = {6538 LNCS},
   year = {2011},
}
@article{Sharma2011,
   abstract = {We present a novel static analysis technique that substantially improves the quality of invariants inferred by standard loop invariant generation techniques. Our technique decomposes multi-phase loops, which require disjunctive invariants, into a semantically equivalent sequence of single-phase loops, each of which requires simple, conjunctive invariants. We define splitter predicates which are used to identify phase transitions in loops, and we present an algorithm to find useful splitter predicates that enable the phase-reducing transformation. We show experimentally on a set of representative benchmarks from the literature and real code examples that our technique substantially increases the quality of invariants inferred by standard invariant generation techniques. Our technique is conceptually simple, easy to implement, and can be integrated into any automatic loop invariant generator.},
   author = {Rahul Sharma and Isil Dillig and Thomas Dillig and Alex Aiken},
   doi = {10.1007/978-3-642-22110-1_57},
   isbn = {9783642221095},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Static analysis,decomposition of multi-phase loops,invariant generation},
   pages = {703-719},
   title = {Simplifying loop invariant generation using splitter predicates},
   volume = {6806 LNCS},
   year = {2011},
}
@article{McCloskey2010,
   abstract = {We describe Deskcheck, a parametric static analyzer that is able to establish properties of programs that manipulate dynamically allocated memory, arrays, and integers. Deskcheck can verify quantified invariants over mixed abstract domains, e.g., heap and numeric domains. These domains need only minor extensions to work with our domain combination framework. The technique used for managing the communication between domains is reminiscent of the Nelson-Oppen technique for combining decision procedures, in that the two domains share a common predicate language to exchange shared facts. However, whereas the Nelson-Oppen technique is limited to a common predicate language of shared equalities, the technique described in this paper uses a common predicate language in which shared facts can be quantified predicates expressed in first-order logic with transitive closure. We explain how we used Deskcheck to establish memory safety of the thttpd web server’s cache data structure, which uses linked lists, a hash table, and reference counting in a single composite data structure. Our work addresses some of the most complex data-structure invariants considered in the shape-analysis literature.},
   author = {Bill McCloskey and Thomas Reps and Mooly Sagiv},
   doi = {10.1007/978-3-642-15769-1_6},
   isbn = {3642157688},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {71-99},
   title = {Statically inferring complex heap, array, and numeric invariants},
   volume = {6337 LNCS},
   year = {2010},
}
@article{Rival2010,
   abstract = {Interprocedural program analysis is often performed by computing procedure summaries. While possible, computing adequate summaries is difficult, particularly in the presence of recursive procedures. In this paper, we propose a complementary framework for interprocedural analysis based on a direct abstraction of the calling context. Specifically, our approach exploits the inductive structure of a calling context by treating it directly as a stack of activation records. We then build an abstraction based on separation logic with inductive definitions. A key element of this abstract domain is the use of parameters to refine the meaning of such call stack summaries and thus express relations across activation records and with the heap. In essence, we define an abstract interpretation-based analysis framework for recursive programs that permits a fluid per call site abstraction of the call stack-much like how shape analyzers enable a fluid per program point abstraction of the heap. Copyright © 2011 ACM.},
   author = {Xavier Rival and Bor Yuh Evan Chang},
   doi = {10.1145/1926385.1926406},
   isbn = {9781450304900},
   issn = {07308566},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   keywords = {Calling context,Context-sensitivity,Inductive definitions,Interprocedural analysis,Separation logic,Shape analysis,Symbolic abstract domain},
   note = {Problem: Interprocedural analysis<br/>The two mainstream approaches are based on the function summary and whole program analysis.<br/><br/>Limitations:<br/>(1) function summary: based on the assumption that the parameters are arbitrary, which is not the real case.<br/>(2) whole program analysis: information loss in the symobolic abstraction<br/><br/>The authors model the call stack by shape analysis based on Separation Logic. Because the call stack is inductive, the widening operation can be defined by the inductive rules of call stack.<br/><br/>The elements of this work and traditional shape analysis have the following responding relations:<br/>(1) The frame of the function &lt;-&gt; individual<br/>(2) local variable &lt;-&gt; field<br/>(3) caller-callee relation &lt;-&gt; point-to relation<br/>(4) recursive function &lt;-&gt; linked list<br/><br/>How to assure the converage:<br/>finding the difference between successive abstract program states<br/>(This is based on the inductive structure of call stack)<br/><br/>This paper might be extended to loop handling.},
   pages = {173-186},
   title = {Calling context abstraction with shapes},
   year = {2010},
}
@article{Dillig2010,
   abstract = {Many relational static analysis techniques for precise reasoning about heap contents perform an explicit case analysis of all possible heaps that can arise. We argue that such precise relational reasoning can be obtained in a more scalable and economical way by enforcing the memory invariant that every concrete memory location stores one unique value directly on the heap abstraction. Our technique combines the strengths of analyses for precise reasoning about heap contents with approaches that prioritize axiomatization of memory invariants, such as the theory of arrays. Furthermore, by avoiding an explicit case analysis, our technique is scalable and powerful enough to analyze real-world programs with intricate use of arrays and pointers; in particular, we verify the absence of buffer overruns, incorrect casts, and null pointer dereferences in OpenSSH (over 26,000 lines of code) after fixing 4 previously undiscovered bugs found by our system. Our experiments also show that the combination of reasoning about heap contents and enforcing existence and uniqueness invariants is crucial for this level of precision. },
   author = {Isil Dillig and Thomas Dillig and Alex Aiken},
   doi = {10.1145/1932682.1869493},
   isbn = {9781450302036},
   issn = {15232867},
   issue = {10},
   journal = {ACM SIGPLAN Notices},
   keywords = {Array analysis,Heap analysis,Memory invariants,Relational static analysis},
   pages = {397-410},
   title = {Symbolic heap abstraction with demand-driven axiomatization of memory invariants},
   volume = {45},
   year = {2010},
}
@article{Steenken2010,
   author = {Dominik Steenken and Heike Wehrheim and Dominik Steenken and Heike Wehrheim and Daniel Wonisch},
   issue = {v},
   journal = {Nordic Workshop on Programming Theory 2010},
   keywords = {Graph Transformation,Shape Analysis,Three-Vaued Logic},
   pages = {1-4},
   title = {Towards a Shape Analysis for Graph Transformation Systems},
   url = {http://arxiv.org/abs/1010.4423},
   year = {2010},
}
@article{Lvov2010,
   author = {M. S. Lvov},
   doi = {10.1007/s10559-010-9242-x},
   issn = {10600396},
   issue = {4},
   journal = {Cybernetics and Systems Analysis},
   keywords = {automatic generation problem,polynomial loop invariants,static program analysis},
   pages = {660-668},
   title = {Polynomial invariants for linear loops},
   volume = {46},
   year = {2010},
}
@article{Schrammel2010,
   abstract = {Acceleration methods are commonly used for computing precisely the effects of loops in the reachability analysis of counter machine models. Applying these methods on synchronous data-flow programs with Boolean and numerical variables, e.g. Lustre programs, firstly requires the enumeration of the Boolean states in order to obtain a control graph with numerical variables only. Secondly, acceleration methods have to deal with the non-determinism introduced by numerical input variables. In this article we address the latter problem by extending the concept of abstract acceleration of Gonnord et al. to numerical input variables. © 2010 Elsevier B.V. All rights reserved.},
   author = {Peter Schrammel and Bertrand Jeannet},
   doi = {10.1016/j.entcs.2010.09.009},
   issn = {15710661},
   issue = {1},
   journal = {Electronic Notes in Theoretical Computer Science},
   keywords = {Static analysis,abstract interpretation,acceleration,linear relation analysis},
   pages = {101-114},
   title = {Extending abstract acceleration methods to data-flow programs with numerical inputs},
   volume = {267},
   year = {2010},
}
@article{Ancourt2010,
   abstract = {Modular static analyzers use procedure abstractions, a.k.a. summarizations, to ensure that their execution time increases linearly with the size of analyzed programs. A similar abstraction mechanism is also used within a procedure to perform a bottom-up analysis. For instance, a sequence of instructions is abstracted by combining the abstractions of its components, or a loop is abstracted using the abstraction of its loop body: fixed point iterations for a loop can be replaced by a direct computation of the transitive closure of the loop body abstraction. More specifically, our abstraction mechanism uses affine constraints, i.e. polyhedra, to specify pre- and post-conditions as well as state transformers. We present an algorithm to compute the transitive closure of such a state transformer, and we illustrate its performance on various examples. Our algorithm is simple, based on discrete differentiation and integration: it is very different from the usual abstract interpretation fixed point computation based on widening. Experiments are carried out using previously published examples. We obtain the same results directly, without using any heuristic. © 2010 Elsevier B.V. All rights reserved.},
   author = {Corinne Ancourt and Fabien Coelho and François Irigoin},
   doi = {10.1016/j.entcs.2010.09.002},
   issn = {15710661},
   issue = {1},
   journal = {Electronic Notes in Theoretical Computer Science},
   keywords = {Abstract interpretation,fixed point computation,loop invariant},
   pages = {3-16},
   title = {A modular static analysis approach to affine loop invariants detection},
   volume = {267},
   year = {2010},
}
@article{Calcagno2009,
   author = {Cristiano Calcagno and Dino Distefano and Peter O Hearn},
   isbn = {9781605583792},
   keywords = {a program analysis is,compo-,languages,or program,parts,program,reliability,result of a composite,similarly,sitional if the analysis,the meanings of its,theory,verification},
   title = {Popl09},
   year = {2009},
}
@article{Albiz2009,
   author = {Syed S Albiz and Patrick Lam},
   issue = {September},
   title = {Implementation and Use of Data Structures in Java Programs},
   year = {2009},
}
@article{Balakrishnan2009,
   abstract = {We present a simple yet useful technique for refining the control structure of loops that occur in imperative programs. Loops containing complex control flow are common in synchronous embedded controllers derived from modeling languages such as Lustre, Esterel, and Simulink/Stateflow. Our approach uses a set of labels to distinguish different control paths inside a given loop. The iterations of the loop are abstracted as a finite state automaton over these labels. Subsequently, we use static analysis techniques to identify infeasible iteration sequences and subtract such forbidden sequences from the initial language to obtain a refinement. In practice, the refinement of control flow sequences often simplifies the control flow patterns in the loop. We have applied the refinement technique to improve the precision of abstract interpretation in the presence of widening. Our experiments on a set of complex reactive loop benchmarks clearly show the utility of our refinement techniques. Abstraction interpretation with our refinement technique was able to verify all the properties for 10 out of the 13 benchmarks, while abstraction interpretation without refinement was able to verify only four. Other potentially useful applications include termination analysis and reverse engineering models from source code.},
   author = {Gogul Balakrishnan and Sriram Sankaranarayanan and Franjo Ivančić and Aarti Gupta},
   doi = {10.1145/1629335.1629343},
   isbn = {9781605586274},
   journal = {Embedded Systems Week 2009 - Proceedings of the 7th ACM International Conference on Embedded Software, EMSOFT '09},
   keywords = {Abstract interpretation,Loop refinement,Model checking,Path-sensitive analysis,Program understanding,Program verification,Static analysis,Synchronous sytems},
   pages = {49-58},
   title = {Refining the control structure of loops using static analysis},
   year = {2009},
}
@article{Jeannet2009,
   abstract = {This article describes Apron, a freely available library dedi- cated to the static analysis of the numerical variables of programs by ab- stract interpretation. Its goal is threefold: provide analysis implementers with ready-to-use numerical abstractions under a unified API, encour- age the research in numerical abstract domains by providing a platform for integration and comparison, and provide teaching and demonstration tools to disseminate knowledge on abstract interpretation.},
   author = {Bertrand Jeannet and Antoine Miné},
   doi = {10.1007/978-3-642-02658-4_52},
   isbn = {3642026575},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {661-667},
   title = {Apron: A library of numerical abstract domains for static analysis},
   volume = {5643 LNCS},
   year = {2009},
}
@article{Marron2008,
   abstract = {The performance of heap analysis techniques has a significant impact on their utility in an optimizing compiler.Most shape analysis techniques perform interprocedural dataflow analysis in a context-sensitive manner, which can result in analyzing each procedure body many times (causing significant increases in runtime even if the analysis results are memoized). To improve the effectiveness of memoization (and thus speed up the analysis) project/extend operations are used to remove portions of the heap model that cannot be affected by the called procedure (effectively reducing the number of different contexts that a procedure needs to be analyzed with). This paper introduces project/extend operations that are capable of accurately modeling properties that are important when analyzing non-trivial programs (sharing, nullity information, destructive recursive functions, and composite data structures). The techniques we introduce are able to handle these features while significantly improving the effectiveness of memoizing analysis results (and thus improving analysis performance). Using a range of well known benchmarks (many of which have not been successfully analyzed using other existing shape analysis methods) we demonstrate that our approach results in significant improvements in both accuracy and efficiency over a baseline analysis.},
   author = {Mark Marron and Manuel Hermenegildo and Deepak Kapur and Darko Stefanovic},
   doi = {10.1007/978-3-540-78791-4_17},
   isbn = {3540787909},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {245-259},
   title = {Efficient context-sensitive shape analysis with graph based heap models},
   volume = {4959 LNCS},
   year = {2008},
}
@article{Fink2008,
   abstract = {This article addresses the challenge of sound typestate verification, with acceptable precision, for real-world Java programs. We present a novel framework for verification of typestate properties, including several new techniques to precisely treat aliases without undue performance costs. In particular, we present a flow-sensitive, context-sensitive, integrated verifier that utilizes a parametric abstract domain combining typestate and aliasing information. To scale to real programs without compromising precision, we present a staged verification system in which faster verifiers run as early stages which reduce the workload for later, more precise, stages.},
   author = {Stephen J. Fink and Eran Yahav and Nurit Dor and G. Ramalingam and Emmanuel Geay},
   doi = {10.1145/1348250.1348255},
   isbn = {1595932631},
   issn = {1049331X},
   issue = {2},
   journal = {ACM Transactions on Software Engineering and Methodology},
   keywords = {alias analysis,program verification,typestate},
   pages = {1-34},
   title = {Effective typestate verification in the presence of aliasing},
   volume = {17},
   url = {http://portal.acm.org/citation.cfm?doid=1348250.1348255},
   year = {2008},
}
@article{Manevich2008,
   abstract = {We demonstrate shape analyses that can achieve a state space reduction exponential in the number of threads compared to the state-of-the-art analyses, while retaining sufficient precision to verify sophisticated properties such as linearizability. The key idea is to abstract the global heap by decomposing it into (not necessarily disjoint) subheaps, abstracting away some correlations between them. These new shape analyses are instances of an analysis framework based on heap decomposition. This framework allows rapid prototyping of complex static analyses by providing efficient abstract transformers given user-specified decomposition schemes. Initial experiments confirm the value of heap decomposition in scaling concurrent shape analyses.},
   author = {Roman Manevich and Tal Lev-Ami and Mooly Sagiv and Ganesan Ramalingam and Josh Berdine},
   doi = {10.1007/978-3-540-69166-2_24},
   isbn = {3540691634},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {363-377},
   title = {Heap decomposition for concurrent shape analysis},
   volume = {5079 LNCS},
   year = {2008},
}
@article{Chang2008,
   abstract = {Shape analyses are concerned with precise abstractions of the heap to capture detailed structural properties. To do so, they need to build and decompose summaries of disjoint memory regions. Unfortunately, many data structure invariants require relations be tracked across disjoint regions, such as intricate numerical data invariants or structural invariants concerning back and cross pointers. In this paper, we identify issues inherent to analyzing relational structures and design an abstract domain that is parameterized both by an abstract domain for pure data properties and by user-supplied specifications of the data structure invariants to check. Particularly, it supports hybrid invariants about shape and data and features a generic mechanism for materializing summaries at the beginning, middle, or end of inductive structures. Around this domain, we build a shape analysis whose interesting components include a pre-analysis on the user-supplied specifications that guides the abstract interpretation and a widening operator over the combined shape and data domain. We then demonstrate our techniques on the proof of preservation of the red-black tree invariants during insertion. Copyright © 2008 ACM.},
   author = {Bor Yuh Evan Chang and Xavier Rival},
   doi = {10.1145/1328438.1328469},
   isbn = {9781595936899},
   issn = {07308566},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   keywords = {heap analysis,inductive definitions,materialization,separation logic,shape analysis,symbolic abstract domain},
   pages = {247-260},
   title = {Relational inductive shape analysis},
   year = {2008},
}
@article{Cortesi2008,
   abstract = {Interpretation, one of the most applied techniques for semantics based static analysis of software, is based on two main key-concepts: the correspondence between concrete and abstract semantics through Galois connections/insertions, and the feasibility of a fixed point computation of the abstract semantics, through the fast convergence of widening operators. The latter point is crucial to ensure the scalability of the analysis to large software systems. In this paper, we investigate which properties are necessary to support a systematic design of widening operators, by discussing and comparing different definitions in the literature, and by proposing various ways to combine them. In particular, we prove that, for Galois insertions, widening is preserved by abstraction, and we show how widening operators can be combined for the cartesian and reduced product of abstract domains.},
   author = {Agostino Cortesi},
   doi = {10.1109/SEFM.2008.20},
   isbn = {9780769534374},
   journal = {Proceedings - 6th IEEE International Conference on Software Engineering and Formal Methods, SEFM 2008},
   keywords = {Abstract domains,Abstract interpretation,Static analysis,Widening operators},
   pages = {31-40},
   publisher = {IEEE},
   title = {Widening operators for abstract interpretation},
   year = {2008},
}
@article{Pinzger2008,
   abstract = {Many program comprehension tools use graphs to visualize and analyze source code. The main issue is that existing approaches create graphs overloaded with too much information. Graphs contain hundreds of nodes and even more edges that cross each other. Understanding these graphs and using them for a given program comprehension task is tedious, and in the worst case developers stop using the tools. In this paper we present D A4 Java, a graphbased approach for visualizing and analyzing static dependencies between Java source code entities. The main contribution of DA4Java is a set of features to incrementully compose graphs and remove irrelevant nodes and edges from graphs. This leads to graphs that contain significantly fewer nodes and edges and need less effort to understand. © 2008 IEEE.},
   author = {Martin Pinzger and Katja Gräfenhain and Patrick Knab and Harald C. Gall},
   doi = {10.1109/ICPC.2008.23},
   isbn = {9780769531762},
   journal = {IEEE International Conference on Program Comprehension},
   pages = {254-259},
   publisher = {IEEE},
   title = {A tool for visual understanding of source code dependencies},
   year = {2008},
}
@article{Jarzabek2007,
   author = {Stanislaw Jarzabek},
   doi = {10.1201/9781420013115.ch2},
   journal = {Effective Software Maintenance and Evolution},
   pages = {15-46},
   title = {Static Program Analysis Methods},
   year = {2007},
}
@article{Bogudlov2007,
   abstract = {TVLA is a parametric framework for shape analysis that can be easily instantiated to create different kinds of analyzers for checking properties of programs that use linked data structures. We report on dramatic improvements in TVLA’s performance, which make the cost of parametric shape analysis comparable to that of the most efficient specialized shape-analysis tools (which restrict the class of data structures and programs analyzed) without sacrificing TVLA’s parametricity. The improvements were obtained by employing well-known techniques from the database community to reduce the cost of extracting information from shape descriptors and performing abstract interpretation of program statements and conditions. Compared to the prior version of TVLA, we obtained as much as 50-fold speedup.},
   author = {Igor Bogudlov and Tal Lev-Ami and Thomas Reps and Mooly Sagiv},
   isbn = {3540733671},
   issn = {03029743},
   issue = {ii},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {221-225},
   title = {Revamping TVLA: Making parametric shape analysis competitive},
   volume = {4590 LNCS},
   year = {2007},
}
@article{Gotsman2007,
   author = {Alexey Gotsman and Josh Berdine and Byron Cook},
   isbn = {9781595936332},
   issue = {Section 3},
   keywords = {abstract interpretation,concurrent programming,shape analysis,static analysis},
   pages = {266-277},
   title = {Thread-Modular Shape Analysis},
   year = {2007},
}
@article{Chang2007,
   abstract = {Developer-supplied data structure specifications are important to shape analyses, as they tell the analysis what information should be tracked in order to obtain the desired shape invariants. We observe that data structure checking code (e.g., used in testing or dynamic analysis) provides shape information that can also be used in static analysis. In this paper, we propose a lightweight, automatic shape analysis based on these developer-supplied structural invariant checkers. In particular, we set up a parametric abstract domain, which is instantiated with such checker specifications to summarize memory regions using both notions of complete and partial checker evaluations. The analysis then automatically derives a strategy for canonicalizing or weakening shape invariants. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Bor Yuh Evan Chang and Xavier Rival and George C. Necula},
   isbn = {9783540740605},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {384-401},
   title = {Shape analysis with structural invariant checkers},
   volume = {4634 LNCS},
   year = {2007},
}
@article{Gopan2007,
   abstract = {In static analysis, the semantics of the program is expressed as a set of equations. The equations are solved iteratively over some abstract domain. If the abstract domain is distributive and satisfies the ascending-chain condition, an iterative technique yields the most precise solution for the equations. However, if the above properties are not satisfied, the solution obtained is typically impre- cise. Moreover, due to the properties of widening operators, the precision loss is sensitive to the order in which the state-space is explored. In this paper, we introduce guided static analysis, a framework for controlling the exploration of the state-space of a program. The framework guides the state- space exploration by applying standard static-analysis techniques to a sequence of modified versions of the analyzed program. As such, the framework does not require any modifications to existing analysis techniques, and thus can be easily integrated into existing static-analysis tools. We present two instantiations of the framework, which improve the precision of widening in (i) loops with multiple phases and (ii) loops in which the transforma- tion performed on each iteration is chosen non-deterministically.},
   author = {Denis Gopan and Thomas Reps},
   isbn = {9783540740605},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {349-365},
   title = {Guided static analysis},
   volume = {4634 LNCS},
   year = {2007},
}
@article{Berdine2007,
   abstract = {An invariance assertion for a program location l is a statement that always holds at l during execution of the program. Program invariance analyses infer invariance assertions that can be useful when trying to prove safety properties. We use the term variance assertion to mean a statement that holds between any state at l and any previous state that was also at l. This paper is concerned with the development of analyses for variance assertions and their application to proving termination and liveness properties. We describe a method of constructing program variance analyses from invariance analyses. If we change the underlying invariance analysis, we get a different variance analysis. We describe several applications of the method, including variance analyses using linear arithmetic and shape analysis. Using experimental results we demonstrate that these variance analyses give rise to a new breed of termination provers which are competitive with and sometimes better than today's state-of-the-art termination provers.},
   author = {Josh Berdine and Aziem Chawdhary and Byron Cook and Dino Distefano and Peter O'Hearn},
   doi = {10.1145/1190216.1190249},
   isbn = {1595935754},
   issn = {07308566},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   keywords = {Formal verification,Liveness,Program analysis,Software model checking,Termination},
   pages = {211-224},
   title = {Variance analyses from invariance analyses},
   year = {2007},
}
@article{Janota2007,
   abstract = {Abstract. Many automated techniques for invariant generation are based on the idea that the invariant should show that something “bad” will not happen in the analyzed program. In this article we present an algorithm for loop invariant generation in programs with assertions using a weakest precondition calculus. We have realized the algorithm in the extended static checker ESC/Java2. Challenges stemming from our initial experience with the implementation are also discussed.},
   author = {Mikolas Janota},
   issue = {03},
   journal = {Jml},
   keywords = {Conference item,JML,algorithm,java modeling language},
   title = {Assertion-based loop invariant generation},
   year = {2007},
}
@article{Jeannet2007,
   author = {Bertrand Jeannet},
   issue = {October},
   journal = {October},
   title = {by Bertrand Jeannet and the APRON team},
   year = {2007},
}
@article{Ball2007,
   author = {Thomas Ball and Orna Kupferman and Mooly Sagiv},
   isbn = {3540733671},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {491-503},
   title = {Leaping loops in the presence of abstraction},
   volume = {4590 LNCS},
   year = {2007},
}
@article{Simon2006,
   abstract = {The abstract domain of polyhedra is sufficiently expressive to be deployed in verification. One consequence of the richness of this domain is that long, possibly infinite, sequences of polyhedra can arise in the analysis of loops. Widening and narrowing have been proposed to infer a single polyhedron that summarises such a sequence of polyhedra. Motivated by precision losses encountered in verification, we explain how the classic widening/narrowing approach can be refined by an improved extrapolation strategy. The insight is to record inequalities that are thus far found to be unsatisfiable in the analysis of a loop. These so-called landmarks hint at the amount of widening necessary to reach stability. This extrapolation strategy, which refines widening with thresholds, can infer post-fixpoints that are precise enough not to require narrowing. Un- like previous techniques, our approach interacts well with other domains, is fully automatic, conceptually simple and precise on complex loops.},
   author = {Axel Simon and Andy King},
   isbn = {3540489371},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {166-182},
   title = {Widening polyhedra with landmarks},
   volume = {4279 LNCS},
   year = {2006},
}
@article{Gopan2006,
   abstract = {We present lookahead widening, a novel technique for using existing widening and narrowing operators to improve the precision of static analysis. This technique is both self-contained and fully-automatic in the sense that it does not rely on separate analyzes or human involvement. We show how to integrate looka- head widening into existing analyzers with minimal effort. Experimental results indicate that the technique is able to achieve sizable precision improvements at reasonable costs.},
   author = {Denis Gopan and Thomas Reps},
   isbn = {354037406X},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {452-466},
   title = {Lookahead widening},
   volume = {4144 LNCS},
   year = {2006},
}
@article{Quinlan2006,
   abstract = {Testing forms a critical part of the development process for large-scale software, and there is growing need for automated tools that can read, represent, analyze, and transform the application's source code to help carry out testing tasks. However, the support required to compile applications written in common general purpose languages is generally inaccessible to the testing research community. In this paper, we report on an extensible, open-source compiler infrastructure called ROSE, which is currently in development at Lawrence Livermore National Laboratory. ROSE specifically targets developers who wish to build source-based tools that implement customized analyses and optimizations for large-scale C, C++, and Fortran90 scientific computing applications (on the order of a million lines of code or more). However, much of this infrastructure can also be used to address problems in testing, and ROSE is by design broadly accessible to those without a formal compiler background. This paper details the interactions between testing of applications and the ways in which compiler technology can aid in the understanding of those applications. We emphasize the particular aspects of ROSE, such as support for the general analysis of whole programs, that are particularly well-suited to the testing research community and the scale of the problems that community solves. © Springer-Verlag Berlin Heidelberg 2006.},
   author = {Dan Quinlan and Shmuel Ur and Richard Vuduc},
   doi = {10.1007/11678779_9},
   isbn = {3540326049},
   issn = {03029743},
   issue = {March 2006},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {116-133},
   title = {An extensible open-source compiler infrastructure for testing},
   volume = {3875 LNCS},
   year = {2006},
}
@article{Hackett2005,
   abstract = {This paper proposes a novel approach to shape analysis: using local reasoning about individual heap locations instead of global reasoning about entire heap abstractions. We present an inter-procedural shape analysis algorithm for languages with destructive updates. The key feature is a novel memory abstraction that differs from traditional abstractions in two ways. First, we build the shape abstraction and analysis on top of a pointer analysis. Second, we decompose the shape abstraction into a set of independent configurations, each of which characterizes one single heap location. Our approach: 1) leads to simpler algorithm specifications, because of local reasoning about the single location; 2) leads to efficient algorithms, because of the smaller granularity of the abstraction; and 3) makes it easier to develop context-sensitive, demand-driven, and incremental shape analyses.We also show that the analysis can be used to enable the static detection of memory errors in programs with explicit deallocation. We have built a prototype tool that detects memory leaks and accesses through dangling pointers in C programs. The experiments indicate that the analysis is sufficiently precise to detect errors with low false positive rates; and is sufficiently lightweight to scale to larger programs. For a set of three popular C programs, the tool has analyzed about 70K lines of code in less than 2 minutes and has produced 97 warnings, 38 of which were actual errors.},
   author = {Brian Hackett and Radu Rugina},
   isbn = {158113830X},
   issn = {07308566},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   keywords = {Memory leaks,Memory management,Shape analysis,Static error detection},
   pages = {310-323},
   title = {Region-based shape analysis with tracked locations},
   year = {2005},
}
@article{Rinetzky2005,
   abstract = {The goal of this work is to develop compile-time algorithms for automatically verifying properties of imperative programs that manipulate dynamically allocated storage. The paper presents an analysis method that uses a characterization of a procedure's behavior in which parts of the heap not relevant to the procedure are ignored. The paper has two main parts: The first part introduces a non-standard concrete semantics, LSL, in which called procedures are only passed parts of the heap. In this semantics, objects are treated specially when they separate the "local heap" that can be mutated by a procedure from the rest of the heap, which---from the viewpoint of that procedure---is non-accessible and immutable. The second part concerns abstract interpretation of LSL and develops a new static-analysis algorithm using canonical abstraction.},
   author = {Noam Rinetzky and Jörg Bauer and Thomas Reps and Mooly Sagiv and Reinhard Wilhelm},
   isbn = {158113830X},
   issn = {07308566},
   issue = {i},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   keywords = {3-valued logic,Abstract interpretation,Shape analysis,Static analysis},
   pages = {296-309},
   title = {A semantics for procedure local heaps and its abstractions},
   year = {2005},
}
@article{Yu2005,
   abstract = {Large-scale legacy programs take long time to compile, thereby hampering productivity. This paper presents algorithms that reduce compilation time by analyzing syntactic dependencies in fine-grain program units, and by removing redundancies as well as false dependencies. These algorithms are combined with parallel compilation techniques (compiler farms, compiler caches), to further reduce build time. We demonstrate through experiments their effectiveness in achieving significant speedup for both fresh and incremental builds.},
   author = {Yijun Yu and Homayoun Dayani-Fard and John Mylopoulos and Periklis Andritsos},
   doi = {10.1109/ICSM.2005.73},
   isbn = {0769523684},
   journal = {IEEE International Conference on Software Maintenance, ICSM},
   pages = {59-68},
   title = {Reducing build time through precompilations for evolving large software},
   volume = {2005},
   year = {2005},
}
@report{Mandelin2005,
   abstract = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object. In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jun-gloids using both API method signatures and jungloids mined from a corpus of sample client programs. We implemented a tool, PROSPECTOR, based on these techniques. PROSPECTOR is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested PROSPECTOR on a set of real programming problems involving APIs; PROSPECTOR found the desired solution for 18 of 20 problems. We also evaluated PROSPECTOR in a user study, finding that programmers solved programming problems more quickly and with more reuse when using PROSPECTOR than without PROSPECTOR.},
   author = {David Mandelin and Lin Xu and Rastislav Bodík and Doug Kimelman},
   keywords = {D213 [Software Engineering]: Reusable Software-Reuse Models,D26 [Software Engineer-ing]: Programming Environments-Integrated Environments,I22 [Artificial Intelligence]: Automatic Programming-Program synthesis General Terms Experimentation, Languages Keywords reuse, program synthesis, mining *},
   title = {Jungloid Mining: Helping to Navigate the API Jungle *},
   url = {www.cs.berkeley.edu/~mandelin/prospector},
   year = {2005},
}
@article{Sankaranarayanan2004,
   abstract = {We present a new technique for the generation of non-linear (algebraic) invariants of a program. Our technique uses the theory of ideals over polynomial rings to reduce the non-linear invariant generation problem to a numerical constraint solving problem. So far, the literature on invariant generation has been focussed on the construction of linear invariants for linear programs. Consequently, there has been little progress toward non-linear invariant generation. In this paper, we demonstrate a technique that encodes the conditions for a given template assertion being an invariant into a set of constraints, such that all the solutions to these constraints correspond to non-linear (algebraic) loop invariants of the program. We discuss some trade-offs between the completeness of the technique and the tractability of the constraint-solving problem generated. The application of the technique is demonstrated on a few examples.},
   author = {Sriram Sankaranarayanan and Henny B. Sipma and Zohar Manna},
   isbn = {158113729X},
   issn = {07308566},
   journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
   keywords = {Constraint Programming,Gröbner Bases,Ideals,Invariant Generation,Program Analysis,Symbolic Computation,Verification},
   pages = {318-329},
   title = {Non-linear loop invariant generation using gröbner bases},
   volume = {31},
   year = {2004},
}
@article{Simon2004,
   abstract = {Suppose $<A_i, \vec\{c\}_i>$ are planar (convex) H-polyhedra, that is, $A_i \in \mathbb\{R\}^\{n_i \times 2\}$ and $\vec\{c\}_i \in \mathbb\{R\}^\{n_i\}$. Let $P_i = \\{\vec\{x\} \in \mathbb\{R\}^2 \mid A_i\vec\{x\} \leq \vec\{c\}_i \\}$ and $n = n_1 + n_2$. We present an $O(n \log n)$ algorithm for calculating an H-polyhedron $<A, \vec\{c\}>$ with the smallest $P = \\{\vec\{x\} \in \mathbb\{R\}^2 \mid A\vec\{x\} \leq \vec\{c\} \\}$ such that $P_1 \cup P_2 \subseteq P$.},
   author = {Axel Simon and Andy King},
   doi = {10.1080/00207160310001650034},
   issn = {00207160},
   issue = {3},
   journal = {International Journal of Computer Mathematics},
   keywords = {Computational geometry,Convex hull},
   pages = {259-271},
   title = {Convex hull of planar H-polyhedra},
   volume = {81},
   year = {2004},
}
@article{Podelski2004,
   abstract = { Proof rules for program verification rely on auxiliary assertions. We propose a (sound and relatively complete) proof rule whose auxiliary assertions are transition invariants. A transition invariant of a program is a binary relation over program states that contains the transitive closure of the transition relation of the program. A relation is disjunctively well-founded if it is a finite union of well-founded relations. We characterize the validity of termination or another liveness property by the existence of a disjunctively well-founded transition invariant. The main contribution of our proof rule lies in its potential for automation via abstract interpretation.},
   author = {Andreas Podelski and Andrey Rybalchenko},
   issn = {10436871},
   journal = {Proceedings - Symposium on Logic in Computer Science},
   pages = {32-41},
   title = {Transition invariants},
   volume = {19},
   year = {2004},
}
@article{Casero2004,
   abstract = {Most modern object oriented programming languages do not offer constructs to specify dependencies among members of a class. Public interfaces are written using member types and method signatures only, which are not capable of expressing such kind of relationships. We show that stating which dependencies exist between class members, i.e. which methods could be affected by a change in the implementation of the others, constitutes a relevant information to be shipped to inheritors in order to help them in subclassing without inconsistencies. In this paper we present a tool that supports developers in this task by exploiting C# attributes, that are annotations accessible at runtime. The tool will be integrated in the popular developer environment Visual Studio .NET.},
   author = {Riccardo Casero and Mirko Cesarini and Mattia Monga},
   doi = {10.5381/jot.2004.3.2.a5},
   issn = {16601769},
   issue = {2},
   journal = {Journal of Object Technology},
   pages = {47-55},
   title = {Managing code dependencies in C#},
   volume = {3},
   year = {2004},
}
@article{Clarke2004,
   abstract = {We present a tool for the formal verification of ANSI-C programs using Bounded Model Checking (BMC). The emphasis is on usability: the tool supports almost all ANSI-C language features, including pointer constructs, dynamic memory allocation, recursion, and the float and double data types. From the perspective of the user, the verification is highly automated: the only input required is the BMC bound. The tool is integrated into a graphical user interface. This is essential for presenting long counterexample traces: the tool allows stepping through the trace in the same way a debugger allows stepping through a program. © Springer-Verlag 2004.},
   author = {Edmund Clarke and Daniel Kroening and Flavio Lerda},
   doi = {10.1007/978-3-540-24730-2_15},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {168-176},
   title = {A tool for checking ANSI-C programs},
   volume = {2988},
   year = {2004},
}
@article{Bagnara2003,
   abstract = {In the context of static analysis via abstract interpretation, convex polyhedra constitute the most used abstract domain among those capturing numerical relational information. Since the domain of convex polyhedra admits infinite ascending chains, it has to be used in conjunction with appropriate mechanisms for enforcing and accelerating the convergence of fixpoint computations. Widening operators provide a simple and general characterization for such mechanisms. For the domain of convex polyhedra, the original widening operator proposed by Cousot and Halbwachs amply deserves the name of standard widening since most analysis and verification tools that employ convex polyhedra also employ that operator. Nonetheless, there is an unfulfilled demand for more precise widening operators. In this paper, after a formal introduction to the standard widening where we clarify some aspects that are often overlooked, we embark on the challenging task of improving on it. We present a framework for the systematic definition of new widening operators that are never less precise than a given widening. The framework is then instantiated on the domain of convex polyhedra so as to obtain a new widening operator that improves on the standard widening by combining several heuristics. A preliminary experimental evaluation has yielded promising results. We also suggest an improvement to the well-known widening delay technique that allows one to gain precision while preserving its overall simplicity. © 2005 Elsevier B.V. All rights reserved.},
   author = {Roberto Bagnara and Patricia M. Hill and Elisa Ricci and Enea Zaffanella},
   doi = {10.1016/j.scico.2005.02.003},
   issn = {03029743},
   issue = {January 2003},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {337-354},
   title = {Precise widening operators for convex polyhedra},
   volume = {2694},
   year = {2003},
}
@article{Martel2003,
   abstract = {Many static analyses aim at assigning to each control point of a program an invariant property that characterizes any state of a trace corresponding to this point. The choice of the set of control points determines the states of an execution trace for which a common property must be found. We focus on sufficient conditions to substitute one control flow graph for another during an analysis. Next, we introduce a dynamic partitioning algorithm that improves the precision of the calculated invariants by deciding dynamically how to map the states of the traces to the control points, depending on the properties resulting from the first steps of the analysis. In particular, this algorithm enables the loops to be unfolded only if this improves the precision of the final invariants. Its correctness stems from the fact that it uses legal graph substitutions.},
   author = {M. Martel},
   doi = {10.1109/SCAM.2003.1238027},
   isbn = {0769520057},
   journal = {Proceedings - 3rd IEEE International Workshop on Source Code Analysis and Manipulation, SCAM 2003},
   keywords = {Algorithm design and analysis,Assembly,Conferences,Flow graphs,Heuristic algorithms,Law,Legal factors,Merging,Partitioning algorithms,Sufficient conditions},
   pages = {13-21},
   title = {Improving the static analysis of loops by dynamic partitioning techniques},
   year = {2003},
}
@article{Sharygina2003,
   author = {Natasha Sharygina and James C. Browne},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {325-340},
   title = {Model checking software via abstraction of loop transitions},
   volume = {2621},
   year = {2003},
}
@article{Yu2003,
   abstract = {The development of large software systems involves a continual lengthy build process that may include preprocessing, compilation and linking of tens of thousands of source code files. In many cases, much of this build time is wasted because of false dependencies between implementation files and their respective header files. We present a graph algorithm and a programming tool that discovers and removes false dependencies among files. We show experimentally that the resulting preprocessed code is more compact, thereby contributing to faster build processes. },
   author = {Yijun Yu and Homy Dayani-Fard and John Mylopoulos},
   journal = {Proceedings of the 2003 Conference of the Centre for Advanced Studies on Collaborative Research},
   pages = {343-352},
   title = {Removing False Code Dependencies to Speedup Software Build Processes},
   url = {http://dl.acm.org/citation.cfm?id=961322.961375},
   year = {2003},
}
@article{Reynolds2002,
   abstract = {In joint work with Peter O'Hearn and others, based on early ideas of Burstall, we have developed an extension of Hoare logic that permits reasoning about low-level imperative programs that use shared mutable data structure. The simple imperative programming language is extended with commands (not expressions) for accessing and modifying shared structures, and for explicit allocation and deallocation of storage. Assertions are extended by introducing a "separating conjunction" that asserts that its subformulas hold for disjoint parts of the heap, and a closely related "separating implication". Coupled with the inductive definition of predicates on abstract data structures, this extension permits the concise and flexible description of structures with controlled sharing. In this paper, we survey the current development of this program logic, including extensions that permit unrestricted address arithmetic, dynamically allocated arrays, and recursive procedures. We also discuss promising future directions.},
   author = {John C. Reynolds},
   issn = {10436871},
   issue = {1},
   journal = {Proceedings - Symposium on Logic in Computer Science},
   pages = {55-74},
   title = {Separation logic: A logic for shared mutable data structures},
   volume = {1},
   year = {2002},
}
@article{Sagiv2002,
   abstract = {Shape analysis concerns the problem of determining "shape invariants" for programs that perform destructive updating on dynamically allocated storage. This article presents a parametric framework for shape analysis that can be instantiated in different ways to create different shape-analysis algorithms that provide varying degrees of efficiency and precision. A key innovation of the work is that the stores that can possibly arise during execution are represented (conservatively) using 3-valued logical structures. The framework is instantiated in different ways by varying the predicates used in the 3-valued logic. The class of programs to which a given instantiation of the framework can be applied is not limited a priori (i.e., as in some work on shape analysis, to programs that manipulate only lists, trees, DAGS, etc.); each instantiation of the framework can be applied to any program, but may produce imprecise results (albeit conservative ones) due to the set of predicates employed.},
   author = {Mooly Sagiv and Thomas Reps and Reinhard Wilhelm},
   doi = {10.1145/514188.514190},
   issn = {01640925},
   issue = {3},
   journal = {ACM Transactions on Programming Languages and Systems},
   keywords = {3-valued logic,Abstract interpretation,Algorithms,Alias analysis,Constraint solving,Destructive updating,Languages,Pointer analysis,Shape analysis,Static analysis,Theory,Verification},
   pages = {217-298},
   title = {Parametric shape analysis via 3-valued logic},
   volume = {24},
   year = {2002},
}
@article{Ramalingam2002,
   abstract = {We are concerned with the problem of statically certifying (verifying) whether the client of a software component conforms to the component's constraints for correct usage. We show how conformance certification can be efficiently carried out in a staged fashion for certain classes of first-order safety (FOS) specifications, which can express relationship requirements among potentially unbounded collections of runtime objects. In the first stage of the certification process, we systematically derive an abstraction that is used to model the component state during analysis of arbitrary clients. In general, the derived abstraction will utilize first-order predicates, rather than the propositions often used by model checkers. In the second stage, the generated abstraction is incorporated into a static analysis engine to produce a certifier. In the final stage, the resulting certifier is applied to a client to conservatively determine whether the client violates the component's constraints. Unlike verification approaches that analyze a specification and client code together, our technique can take advantage of computationally-intensive symbolic techniques during the abstraction generation phase, without affecting the performance of Client analysis. Using as a running example the Concurrent Modification Problem (CMP), which arises when certain classes defined by the Java Collections Framework are misused, we describe several different classes of certifiers with varying time/space/precision tradeoffs. Of particular note are precise, polynomial-time, flow- and context-sensitive certifiers for certain classes of FOS specifications and client programs. Finally, we evaluate a prototype implementation of a certifier for CMP on a variety of test programs. The results of the evaluation show that our approach, though conservative, yields very few " false alarms," with acceptable performance.},
   author = {G. Ramalingam and Alex Warshavsky and John Field and Deepak Goyal and Mooly Sagiv},
   isbn = {1581134630},
   journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
   keywords = {Abstract interpretation,Model checking,Predicate abstraction,Software components,Static analysis},
   pages = {83-94},
   title = {Deriving specialized program analyses for certifying component-client conformance},
   year = {2002},
}
@article{Rinetzky2001,
   author = {Noam Rinetzky and Mooly Sagiv},
   doi = {10.1007/3-540-45306-7_10},
   isbn = {354041861X},
   issn = {16113349},
   issue = {January 2001},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {133-149},
   title = {Interprocedural shape analysis for recursive programs},
   volume = {2027},
   year = {2001},
}
@article{Gannod2001,
   abstract = {A modularization is a partitioning of a software system into components based on a variety of criteria, each depending on the clustering approach and desired level of abstraction. Source-header dependency graphs are bipartite graphs that are formed by flattening include file dependencies and enumerating source file to header file dependencies. In this paper, we describe an approach for identifying candidate modularizations of software systems by analyzing connectivity properties of source-header dependency graphs. In addition, we apply the approach to a large software system to demonstrate its applicability.},
   author = {G. C. Gannod and B. D. Gannod},
   journal = {Reverse Engineering - Working Conference Proceedings},
   pages = {115-124},
   title = {An investigation into the connectivity properties of source-header dependency graphs},
   year = {2001},
}
@article{Krishnaswamy2000,
   abstract = {This paper describes the crucial design and implementation issues that arise in building a fully automatic precompiled header mechanism for compiling industrial-strength C and C++ applications. The key challenges include designing the Makefile-transparent automation, determining the precompile-able region, capturing the compile environment and verifying it and addressing the correctness issues involved in using precompiled headers. The ensuing discussion treats the internals of the actual dumping and loading of precompiled headers as a black-box beyond a brief high-level description. An automatic precompiled header mechanism has been implemented in aCC, the HP ANSI C++ compiler, and the results of compiling real applications show that it achieves significant speedup in compile-times of real applications.},
   author = {Tara Krishnaswamy},
   isbn = {1880446154},
   journal = {Proceedings of the 1st Conference on Industrial Experiences with Systems Software, WIESS 2000},
   title = {Automatic precompiled headers: Speeding up C++ application build times},
   year = {2000},
}
@article{Guyer1999,
   abstract = {This paper introduces an annotation language and a compiler that together\ncan customize a library implementation for specific application needs.\nOur approach is distinguished by its ability to exploit high level,\ndomain-specific information in the customization process. In particular,\nthe annotations provide semantic information that enables our compiler\nto analyze and optimize library operations as if they were primitives\nof a domain-specific language. Thus, our approach yields many of\nthe performance benefits of domain-specific languages, without the\neffort of developing a new compiler for each domain.\n\nThis paper presents the annotation language, describes its role in\noptimization, and illustrates the benefits of the overall approach.\nUsing a partially implemented compiler, we show how our system can\nsignificantly improve the performance of two applications written\nusing the PLAPACK parallel linear algebra library.},
   author = {Samuel Z. Guyer and Calvin Lin},
   doi = {10.1145/331960.331970},
   isbn = {1581132557},
   journal = {Proceedings of the 2nd Conference on Domain-Specific Languages, DSL 1999},
   pages = {39-52},
   title = {An annotation language for optimizing software libraries},
   year = {1999},
}
@article{Dean1995,
   abstract = {Optimizing compilers for object-oriented languages apply static class analysis and other techniques to try to deduce precise information about the possible classes of the receivers of messages; if successful, dynamically-dispatched messages can be replaced with direct procedure calls and potentially further optimized through inline-expansion. By examining the complete inheritance graph of a program, which we call class hierarchy analysis, the compiler can improve the quality of static class information and thereby improve run-time performance. In this paper we present class hierarchy analysis and describe techniques for implementing this analysis effectively in both statically- and dynamically-typed languages and also in the presence of multi-methods. We also discuss how class hierarchy analysis can be supported in an interactive programming environment and, to some extent, in the presence of separate compilation. Finally, we assess the bottom-line performance improvement due to class hierarchy analysis alone and in combination with two other “competing” optimizations, profile-guided receiver class prediction and method specialization.},
   author = {Jeffrey Dean and David Grove and Craig Chambers},
   isbn = {3540601600},
   issn = {16113349},
   issue = {August},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {77-101},
   title = {Optimization of object-oriented programs using static class hierarchy analysis},
   volume = {952},
   year = {1995},
}
@article{Bourdoncle1993,
   abstract = {Abstract interpretation is a formal method that enables the static and automatic determination of run-time properties of programs. This method uses a characterization of program invariants as least and greatest fixed points of continuous functions over complete lattices of program properties. In this paper, we study precise and efficient chaotic iteration strategies for computing such fixed points when lattices are of infinite height and speedup techniques, known as widening and narrowing, have to be used. These strategies are based on a weak topological ordering of the dependency graph of the system of semantic equations associated with the program and minimize the loss in precision due to the use of widening operators. We discuss complexity and implementation issues and give precise upper bounds on the complexity of the intraprocedural and interprocedural abstract interpretation of higher-order programs based on the structure of their control flow graph.},
   author = {François Bourdoncle},
   isbn = {9783540573166},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {128-141},
   title = {Efficient chaotic iteration strategies with widenings},
   volume = {735 LNCS},
   year = {1993},
}