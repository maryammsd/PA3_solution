@article{Yan2021,
author = {Yan, Jiwei and Zhou, Hao and Deng, Xi and Wang, Ping and Yan, Rongjie and Yan, Jun and Zhang, Jian},
doi = {10.1016/j.scico.2020.102522},
issn = {0167-6423},
journal = {Science of Computer Programming},
pages = {102522},
publisher = {Elsevier B.V.},
title = {Science of Computer Programming Efficient testing of GUI applications by event sequence reduction},
url = {https://doi.org/10.1016/j.scico.2020.102522},
volume = {201},
year = {2021}
}
@article{Jiang2017,
author = {Jiang, Bo and Wu, Yuxuan and Li, Teng},
isbn = {9781538626849},
pages = {297--307},
title = {SimplyDroid : Efficient Event Sequence Simplification for Android Application },
year = {2017}
}
@article{Liang2018,
author = {Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei},
doi = {10.1109/TR.2018.2834476},
file = {:home/maryam/Documents/PhD/Android Testing/Fuzzing/2018 - Fuzzing\: State of the Art.pdf:pdf},
journal = {IEEE Transactions on Reliability},
number = {3},
pages = {1199--1218},
publisher = {IEEE},
title = {Fuzzing : State of the Art},
volume = {67},
year = {2018}
}
@article{Lin,
author = {Lin, Jun-wei and Salehnamadi, Navid and Malek, Sam},
isbn = {9781450367684},
keywords = {2020,acm reference format,and sam malek,android,automated testing,empirical study,jun-wei lin,mobile apps,navid salehnamadi,test automation},
title = {Test Automation in Open-Source Android Apps : A Large-Scale Empirical Study}
}
@article{Alian2013,
abstract = {Software testing is most expensive phase of development. It becomes unfeasible to execute all the test cases. Test case minimization techniques are used to minimize the testing cost in terms of execution time, resources etc. The purpose of test case minimization is to generate representative set from test suite that satisfy all the requirements as original test suite with minimum number of test cease. Main purpose of test case minimization techniques is to remove test cases that become redundant and obsolete over time. Several techniques have been purposed in literature. These techniques can be categorized as Heuristics, Genetic Algorithm, Integer Linear Programming based techniques. This paper presents a survey on the work that has been done in test case minimization.},
author = {Alian, Marwah and For, Princess Sumaya University and Technology and Amman, Jordan and Suleiman, Dima and For, Princess Sumaya University and Technology and Amman, Jordan and Shaout, Adnan and -, The University of Michigan and Dearborn and Dearbor, Michigan -},
file = {:home/maryam/Documents/PhD/Android Testing/Crash Replay/Test Case Minimization Techniques \: A Review.pdf:pdf},
journal = {International Journal of Engineering Research & Technology},
keywords = {literature review,reduction techniques,software,survey,test case minimization,test suite,testing},
number = {12},
pages = {1048--1056},
title = {Test Case Minimization Techniques : A Review 1,2},
volume = {2},
year = {2013}
}
@article{Godefroid2020,
author = {Godefroid, Patrice},
doi = {10.1145/3363824},
file = {:home/maryam/Documents/PhD/Android Testing/Fuzzing/fuzzing4.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {2},
pages = {70--76},
title = {Fuzzing: Hack, art, and science},
volume = {63},
year = {2020}
}
@article{Sprundel2005,
abstract = {Fuzzing is the art of automatic bug finding. This is done by providing an application with semi-valid input. The input should in most cases be good enough so applications will assume it's valid input, but at the same time be broken enough so that parsing done on this input will fail. Such failing can lead to unexpected results such as crashes, information leaks, delays, etc.},
author = {Sprundel, Ilja Van},
file = {:home/maryam/Documents/PhD/Android Testing/Fuzzing/fuzzing5.pdf:pdf},
journal = {22nd Chaos Communication Congress: Private Investigations},
keywords = {automated,breaking software in an,zing},
pages = {1--5},
title = {Fuzzing: Breaking software in an automated fashion},
year = {2005}
}
@article{Zhu2022,
abstract = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this article, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
author = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
doi = {10.1145/3512345},
file = {:home/maryam/Documents/PhD/Android Testing/Fuzzing/fuzzing3.pdf:pdf},
issn = {0360-0300},
journal = {ACM Computing Surveys},
number = {11s},
pages = {1--36},
title ={Fuzzing: A Survey for Roadmap},
volume = {54},
year = {2022}
}
@book{Author2022,
author = {Author, Anonymous},
booktitle = {Proceedings of The 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022)},
doi = {10.1145/3540250.3549170},
isbn = {9781450394130},
keywords = {Software testing,mobile apps,non-crashing functional bugs},
number = {1},
publisher = {Association for Computing Machinery},
title = {Detecting Non-crashing Functional Bugs in Android Apps via Deep-State Differential Analysis},
volume = {1},
year = {2022}
}
@book{Fazzini2022,
abstract = {Android apps interact with their environment extensively, which can result in flaky, slow, or hard-to-debug tests. Developers often address these problems using test doubles - developer-defined objects that replace app or library classes during test execution. Although test doubles are widely used, there is limited understanding of how they are used in practice. To bridge this gap, we present an in-depth empirical study that aims to shed light on how developers create and use test doubles in Android apps. In our study, we first analyze a dataset of 1,006 apps with publicly available test suites to identify which frameworks and approaches developers most commonly use to create test doubles. We then investigate several research questions by studying how test doubles defined using these popular frameworks are created and used in the ten apps in the dataset that define the highest number of test doubles using these frameworks. Our results, based on the analysis of 2,365 test doubles that replace a total of 784 classes, provide insight into the types of test doubles used within Android apps and how they are utilized. Our results also show that test doubles used in Android apps and traditional Java test doubles differ in at least some respect. Finally, our results show that test doubles can introduce test smells and even mistakes in the test code. In the paper, we also discuss some implications of our findings that can help researchers and practitioners working in this area and guide future research.},
author = {Fazzini, Mattia and Choi, Chase and Copia, Juan Manuel and Lee, Gabriel and Kakehi, Yoshiki and Gorla, Alessandra and Orso, Alessandro},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/3510003.3510175},
file = {:home/maryam/Documents/PhD/Android Testing/Crash Detection/ICSE 22 - Use of Test Doubles in Android Testing, An In-Depth Investigation.pdf:pdf},
isbn = {9781450392211},
issn = {02705257},
keywords = {Test mocking,mobile apps,software environment},
number = {1},
pages = {2266--2278},
publisher = {Association for Computing Machinery},
title = {Use of Test Doubles in Android Testing: An In-Depth Investigation},
volume = {2022-May},
year = {2022}
}
@article{Lin2020,
abstract = {Automated testing of mobile apps has received significant attention in recent years from researchers and practitioners alike. In this paper, we report on the largest empirical study to date, aimed at understanding the test automation culture prevalent among mobile app developers. We systematically examined more than 3.5 million repositories on GitHub and identified more than 12, 000 non-trivial and real-world Android apps. We then analyzed these non-trivial apps to investigate (1) the prevalence of adoption of test automation; (2) working habits of mobile app developers in regards to automated testing; and (3) the correlation between the adoption of test automation and the popularity of projects. Among others, we found that (1) only 8% of the mobile app development projects leverage automated testing practices; (2) developers tend to follow the same test automation practices across projects; and (3) popular projects, measured in terms of the number of contributors, stars, and forks on GitHub, are more likely to adopt test automation practices. To understand the rationale behind our observations, we further conducted a survey with 148 professional and experienced developers contributing to the subject apps. Our findings shed light on the current practices and future research directions pertaining to test automation for mobile app development.},
author = {Lin, Jun-Wei and Salehnamadi, Navid and Malek, Sam},
doi = {10.1145/3324884.3416623},
file = {:home/maryam/Downloads/ASE 20 - Test Automation in Open-Source Android Apps\: A Large-Scale Empirical Study..pdf:pdf},
isbn = {9781450367684},
keywords = {2020,acm reference format,and sam malek,android,automated testing,empirical study,jun-wei lin,mobile apps,navid salehnamadi,test automation},
pages = {1078--1089},
title = {Test automation in open-source Android apps},
year = {2020}
}
@article{Wang2021,
abstract = {Due to the importance of Android app quality assurance, many Android UI testing tools have been developed by researchers over the years. However, recent studies show that these tools typically achieve low code coverage on popular industrial apps. In fact, given a reasonable amount of run time, most state-of-the-art tools cannot even outperform a simple tool, Monkey, on popular industrial apps with large codebases and sophisticated functionalities. Our motivating study finds that these tools perform two types of operations, UI Hierarchy Capturing (capturing information about the contents on the screen) and UI Event Execution (executing UI events, such as clicks), often inefficiently using UIAutomator, a component of the Android framework. In total, these two types of operations use on average 70% of the given test time. Based on this finding, to improve the effectiveness of Android testing tools, we propose TOLLER, a tool consisting of infrastructure enhancements to the Android operating system. TOLLER injects itself into the same virtual machine as the app under test, giving TOLLER direct access to the app's runtime memory. TOLLER is thus able to directly (1) access UI data structures, and thus capture contents on the screen without the overhead of invoking the Android framework services or remote procedure calls (RPCs), and (2) invoke UI event handlers without needing to execute the UI events. Compared with the often-used UIAutomator, TOLLER reduces average time usage of UI Hierarchy Capturing and UI Event Execution operations by up to 97% and 95%, respectively. We integrate TOLLER with existing state-of-the-art/practice Android UI testing tools and achieve the range of 11.8% to 70.1% relative code coverage improvement on average. We also find that TOLLER-enhanced tools are able to trigger 1.4x to 3.6x distinct crashes compared with their original versions without TOLLER enhancement. These improvements are so substantial that they also change the relative competitiveness of the tools under empirical comparison. Our findings highlight the practicality of TOLLER as well as raising the community awareness of infrastructure support's significance beyond the community's existing heavy focus on algorithms.},
author = {Wang, Wenyu and Lam, Wing and Xie, Tao},
doi = {10.1145/3460319.3464828},
file = {:home/maryam/Downloads/ISSTA 21 - An infrastructure approach to improving effectiveness of Android UI testing tools.pdf:pdf},
isbn = {9781450384599},
journal = {ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
keywords = {Android framework,Test generation,UI testing},
pages = {165--176},
title = {An infrastructure approach to improving effectiveness of Android UI testing tools},
year = {2021}
}
@article{Baek2016,
author = {Baek, Young-min and Bae, Doo-hwan},
isbn = {9781450338455},
journal = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
keywords = {android application testing,eration,gui comparison criteria,gui model gen-,gui testing,model-based test input},
pages = {238--249},
title = {Automated Model-Based Android GUI Testing using Multi-level GUI Comparison Criteria Young-Min},
year = {2016}
}
@misc{Li2019,
abstract = {Automated input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most black-box input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. We propose Humanoid, an automated black-box Android app testing tool based on deep learning. The key technique behind Humanoid is a deep neural network model that can learn how human users choose actions based on an app's GUI from human interaction traces. The learned model can then be used to guide test input generation to achieve higher coverage. Experiments on both open-source apps and market apps demonstrate that Humanoid is able to reach higher coverage, and faster as well, than the state-of-the-art test input generators. Humanoid is open-sourced at https://github.com/yzygitzh/Humanoid and a demo video can be found at https://youtu.be/PDRxDrkyORs.},
archivePrefix = {arXiv},
arxivId = {1901.02633},
author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
booktitle = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
doi = {10.1109/ASE.2019.00104},
eprint = {1901.02633},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2019 - Humanoid A deep learning-based approach to automated black-box android app testing.pdf:pdf},
isbn = {9781728125084},
keywords = {Android,Automated test input generation,Deep learning,Graphical user interface,Mobile application,Software testing},
pages = {1070--1073},
title = {Humanoid: A deep learning-based approach to automated black-box android app testing},
year = {2019}
}
@article{Wang,
author = {Wang, Jue},
isbn = {9781450385626},
keywords = {2021,GUI testing, Android apps, Crash bugs, Benchmarkin,acm reference format,and zhendong su,android apps,benchmarking,benchmarking automated gui,crash bugs,gui testing,jue wang,ting su},
title = {Benchmarking Automated GUI Testing for Android against Real-World Bugs}
}
@article{Pan2020,
abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
doi = {10.1145/3395363.3397354},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan et al. - 2020 - Reinforcement learning based curiosity-driven testing of Android applications.pdf:pdf},
isbn = {9781450380089},
journal = {ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
keywords = {Android app testing,functional scenario division,reinforcement learning},
pages = {153--164},
title = {Reinforcement learning based curiosity-driven testing of Android applications},
year = {2020}
}
@article{Machiry2013,
abstract = {We present a system Dynodroid for generating relevant inputs to unmodified Android apps. Dynodroid views an app as an event-driven program that interacts with its environment by means of a sequence of events through the Android framework. By instrumenting the framework once and for all, Dynodroid monitors the reaction of an app upon each event in a lightweight manner, using it to guide the generation of the next event to the app. Dynodroid also allows interleaving events from machines, which are better at generating a large number of simple inputs, with events from humans, who are better at providing intelligent inputs. We evaluated Dynodroid on 50 open-source Android apps, and compared it with two prevalent approaches: users manually exercising apps, and Monkey, a popular fuzzing tool. Dynodroid, humans, and Monkey covered 55%, 60%, and 53%, respectively, of each app's Java source code on average. Monkey took 20X more events on average than Dynodroid. Dynodroid also found 9 bugs in 7 of the 50 apps, and 6 bugs in 5 of the top 1, 000 free apps on Google Play. Copyright 2013 ACM.},
annote = {Notes available at Notion :)},
author = {Machiry, Aravind and Tahiliani, Rohan and Naik, Mayur},
doi = {10.1145/2491411.2491450},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Machiry, Tahiliani, Naik - 2013 - Dynodroid An input generation system for android apps.pdf:pdf},
isbn = {9781450322379},
journal = {2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings},
keywords = {Android,GUI testing,Testing event-driven programs},
pages = {224--234},
title = {Dynodroid: An input generation system for android apps},
year = {2013}
}
@article{Kong2019,
abstract = {Android apps are prone to crash. This often arises from the misuse of Android framework APIs, making it harder to debug since official Android documentation does not discuss thoroughly potential exceptions.Recently, the program repair community has also started to investigate the possibility to fix crashes automatically. Current results, however, apply to limited example cases. In both scenarios of repair, the main issue is the need for more example data to drive the fix processes due to the high cost in time and effort needed to collect and identify fix examples. We propose in this work a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We developed a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches.},
author = {Kong, Pingfan and Li, L. and Gao, Jun and Bissyand}, 
doi = {10.1145/3293882.3330572},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kong et al. - 2019 - Mining android crash fixes in the absence of issue- And change-tracking systems.pdf:pdf},
isbn = {9781450362245},
journal = {ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
keywords = {Android,Crash,Debugging,Mining software repository,Testing},
pages = {123--133},
title = {Mining android crash fixes in the absence of issue- And change-tracking systems},
year = {2019}
}
@article{Su2017,
abstract = {Mobile apps are ubiquitous, operate in complex environments and are developed under the time-to-market pressure. Ensuring their correctness and reliability thus becomes an important challenge. This paper introduces Stoat, a novel guided approach to perform stochastic model-based testing on Android apps. Stoat operates in two phases: (1) Given an app as input, it uses dynamic analysis enhanced by a weighted UI exploration strategy and static analysis to reverse engineer a stochastic model of the app's GUI interactions; and (2) it adapts Gibbs sampling to iteratively mutate/refine the stochastic model and guides test generation from the mutated models toward achieving high code and model coverage and exhibiting diverse sequences. During testing, system-level events are randomly injected to further enhance the testing effectiveness. Stoat was evaluated on 93 open-source apps. The results show (1) the models produced by Stoat cover 17$\sim$31% more code than those by existing modeling tools; (2) Stoat detects 3X more unique crashes than two state-of-the-art testing tools, Monkey and Sapienz. Furthermore, Stoat tested 1661 most popular Google Play apps, and detected 2110 previously unknown and unique crashes. So far, 43 developers have responded that they are investigating our reports. 20 of reported crashes have been confirmed, and 8 already fixed.},
author = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong},
doi = {10.1145/3106237.3106298},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Su et al. - 2017 - Guided, stochastic model-based GUI testing of android apps(2).pdf:pdf},
isbn = {9781450351058},
journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
keywords = {GUI testing,Mobile apps,Model-based testing},
pages = {245--256},
title = {Guided, stochastic model-based GUI testing of android apps},
volume = {Part F1301},
year = {2017}
}
@article{Fan2018,
abstract = {Mobile apps have become ubiquitous. For app developers, it is a key priority to ensure their apps' correctness and reliability. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to guide developers, or help improve testing and analysis tools. However, such studies do not exist -- this paper fills this gap. Over a four-month long effort, we have collected 16,245 unique exception traces from 2,486 open-source Android apps, and observed that framework-specific exceptions account for the majority of these crashes. We then extensively investigated the 8,243 framework-specific exceptions (which took six person-months): (1) identifying their characteristics (e.g., manifestation locations, common fault categories), (2) evaluating their manifestation via state-of-the-art bug detection techniques, and (3) reviewing their fixes. Besides the insights they provide, these findings motivate and enable follow-up research on mobile apps, such as bug detection, fault localization and patch generation. In addition, to demonstrate the utility of our findings, we have optimized Stoat, a dynamic testing tool, and implemented ExLocator, an exception localization tool, for Android apps. Stoat is able to quickly uncover three previously-unknown, confirmed/fixed crashes in Gmail and Google+; ExLocator is capable of precisely locating the root causes of identified exceptions in real-world apps. Our substantial dataset is made publicly available to share with and benefit the community.},
archivePrefix = {arXiv},
arxivId = {1801.07009},
author = {Fan, Lingling and Su, Ting and Chen, Sen and Meng, Guozhu and Liu, Yang and Xu, Lihua and Pu, Geguang and Su, Zhendong},
doi = {10.1145/3180155.3180222},
eprint = {1801.07009},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan et al. - 2018 - Large-scale analysis of framework-specific exceptions in Android apps.pdf:pdf},
isbn = {9781450356381},
issn = {1558-1225},
keywords = {acm reference format,empirical study,geguang,guozhu meng,lihua xu,lingling fan,mobile app bugs,sen chen,static analysis,testing,ting su,yang liu},
pages = {408--419},
publisher = {ACM},
title = {Large-scale analysis of framework-specific exceptions in Android apps},
year = {2018}
}
@article{Borges2017,
abstract = {Testing user interfaces (UIs) is a challenging task. Ideally, every sequence of UI elements should be tested to guarantee that the application works correctly. This is, however, unfeasible due to the number of UI elements in an application. A better approach is to limit the evaluation to UI elements that affect a specific functionality. In this paper I present a novel technique to identify the relation between UI elements using the statically extracted data flows. I also present a method to refine these relations using dynamic analysis, in order to ensure that relations extracted from unreachable data flows are removed. Using these relations it is possible to more efficiently test a functionality. Finally, I present an approach to evaluate how these UI-aware data flows can be used as an heuristic to measure test coverage.},
author = {Borges, Nataniel P.},
doi = {10.1145/3092703.3098234},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Borges - 2017 - Data flow oriented UI testing Exploiting data flows and UI elements to test android applications.pdf:pdf},
isbn = {9781450350761},
journal = {ISSTA 2017 - Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
keywords = {Data flow analysis,Software testing,Test generation,User interface testing},
pages = {432--435},
title = {Data flow oriented UI testing: Exploiting data flows and UI elements to test android applications},
volume = {3098234},
year = {2017}
}
@article{Sadeghi2017,
abstract = {Recent introduction of a dynamic permission system in Android, allowing the users to grant and revoke permissions after the installation of an app, has made it harder to properly test apps. Since an app's behavior may change depending on the granted permissions, it needs to be tested under a wide range of permission combinations. At the state-of-the-art, in the absence of any automated tool support, a developer needs to either manually determine the interaction of tests and app permissions, or exhaustively re-execute tests for all possible permission combinations, thereby increasing the time and resources required to test apps. This paper presents an automated approach, called PATDroid, for efficiently testing an Android app while taking the impact of permissions on its behavior into account. PATDroid performs a hybrid program analysis on both an app under test and its test suite to determine which tests should be executed on what permission combinations. Our experimental results show that PATDroid significantly reduces the testing effort, yet achieves comparable code coverage and fault detection capability as exhaustively testing an app under all permission combinations.},
author = {Sadeghi, Alireza and Jabbarvand, Reyhaneh and Malek, Sam},
doi = {10.1145/3106237.3106250},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sadeghi, Jabbarvand, Malek - 2017 - PATDroid permission-aware GUI testing of Android.pdf:pdf},
isbn = {9781450351058},
keywords = {2017,access control,acm reference format,alireza sadeghi,and sam malek,android,patdroid,permission,reyhaneh jabbarvand,software testing},
pages = {220--232},
title = {PATDroid: permission-aware GUI testing of Android},
year = {2017}
}
@article{Kowalczyk2018,
abstract = {Android has rocketed to the top of the mobile market thanks in large part to its open source model. Vendors use Android for their devices for free, and companies make customizations to suit their needs. This has resulted in a myriad of configurations that are extant in the user space today. In this paper, we show that differences in configurations, if ignored, can lead to differences in test outputs and code coverage. Consequently, researchers who develop new testing techniques and evaluate them on only one or two configurations are missing a necessary dimension in their experiments and developers who ignore this may release buggy software. In a large study on 18 apps across 88 configurations, we show that only one of the 18 apps studied showed no variation at all. The rest showed variation in either, or both, code coverage and test results. 15% of the 2,000 plus test cases across all of the apps vary, and some of the variation is subtle, i.e. not just a test crash. Our results suggest that configurations in Android testing do matter and that developers need to test using configuration-aware techniques.},
author = {Kowalczyk, Emily and Cohen, Myra B. and Memon, Atif M.},
doi = {10.1145/3243218.3243219},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kowalczyk, Cohen, Memon - 2018 - Configurations in android testing They matter.pdf:pdf},
isbn = {9781450359733},
journal = {A-Mobile 2018 - Proceedings of the 1st International Workshop on Advances in Mobile App Analysis, co-located with ASE 2018},
keywords = {Android,Mobile testing},
pages = {1--6},
title = {Configurations in android testing: They matter},
year = {2018}
}
@article{Manes2019,
abstract = {Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
archivePrefix = {arXiv},
arxivId = {1812.00140},
author = {Manes, Valentin Jean Marie and Han, Hyung Seok and Han, Choongwoo and sang kil Cha and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
doi = {10.1109/TSE.2019.2946563},
eprint = {1812.00140},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manes et al. - 2019 - The Art, Science, and Engineering of Fuzzing A Survey.pdf:pdf},
issn = {19393520},
journal = {IEEE Transactions on Software Engineering},
keywords = {Computer bugs,Fuzzing,Security,Terminology,automated software testing,fuzz testing,fuzzing,software security},
pages = {1--21},
title = {The Art, Science, and Engineering of Fuzzing: A Survey},
year = {2019}
}
@article{Li2018,
abstract = {Security vulnerability is one of the root causes of cyber-security threats. To discover vulnerabilities and fix them in advance, researchers have proposed several techniques, among which fuzzing is the most widely used one. In recent years, fuzzing solutions, like AFL, have made great improvements in vulnerability discovery. This paper presents a summary of the recent advances, analyzes how they improve the fuzzing process, and sheds light on future work in fuzzing. Firstly, we discuss the reason why fuzzing is popular, by comparing different commonly used vulnerability discovery techniques. Then we present an overview of fuzzing solutions, and discuss in detail one of the most popular type of fuzzing, i.e., coverage-based fuzzing. Then we present other techniques that could make fuzzing process smarter and more efficient. Finally, we show some applications of fuzzing, and discuss new trends of fuzzing and potential future directions.},
author = {Li, Jun and Zhao, Bodong and Zhang, Chao},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhao, Zhang - 2018 - Cybersecurity Fuzzing a survey.pdf:pdf},
journal = {CyberSecurity},
keywords = {Coverage-based fuzzing,Fuzzing,Software security,Vulnerability discovery},
pages = {1--13},
title = {Cybersecurity Fuzzing: a survey},
url = {https://doi.org/10.1186/s42400-018-0002-y},
year = {2018}
}
@article{Jabbarvand2019,
abstract = {The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efficiently use the device's battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app's energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present COBWEB, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app's energy behavior, COBWEB generates a test suite that can effectively find energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efficiently test energy behavior of apps, but also its superiority over prior techniques by finding a wider and more diverse set of energy defects.},
author = {Jabbarvand, Reyhaneh and Lin, Jun Wei and Malek, Sam},
doi = {10.1109/ICSE.2019.00115},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jabbarvand, Lin, Malek - 2019 - Search-Based Energy Testing of Android.pdf:pdf},
isbn = {9781728108698},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Android,Energy Testing,Software Testing},
pages = {1119--1130},
title = {Search-Based Energy Testing of Android},
volume = {2019-May},
year = {2019}
}
@article{Mao2016,
abstract = {We introduce Sapienz, an approach to Android testing that uses multi-objective search-based testing to automatically explore and optimise test sequences, minimising length, while simultaneously maximising coverage and fault revelation. Sapienz combines random fuzzing, systematic and search-based exploration, exploiting seeding and multi-level instrumentation. Sapienz significantly outperforms (with large effect size) both the state-of-the-art technique Dynodroid and the widely-used tool, Android Monkey, in 7/10 experiments for coverage, 7/10 for fault detection and 10/10 for fault-revealing sequence length. When applied to the top 1, 000 Google Play apps, Sapienz found 558 unique, previously unknown crashes. So far we have managed to make contact with the developers of 27 crashing apps. Of these, 14 have confirmed that the crashes are caused by real faults. Of those 14, six already have developer-confirmed fixes.},
author = {Mao, Ke and Harman, Mark and Jia, Yue},
doi = {10.1145/2931037.2931054},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao, Harman, Jia - 2016 - Sapienz Multi-objective automated testing for android applications.pdf:pdf},
isbn = {9781450343909},
journal = {ISSTA 2016 - Proceedings of the 25th International Symposium on Software Testing and Analysis},
keywords = {Android,Search-based software testing,Test generation},
pages = {94--105},
title = {Sapienz: Multi-objective automated testing for android applications},
year = {2016}
}
@article{Song2017,
abstract = {With the prevalence of Android-based mobile devices, automated testing for Android apps has received increasing attention. However, owing to the large variety of events that Android supports, test input generation is a challenging task. In this paper, we present a novel approach and an open source tool called EHBDroid for testing Android apps. In contrast to conventional GUI testing approaches, a key novelty of EHBDroid is that it does not generate events from the GUI, but directly invokes callbacks of event handlers. By doing so, EHBDroid can efficiently simulate a large number of events that are difficult to generate by traditional UI-based approaches. We have evaluated EHBDroid on a collection of 35 real-world large-scale Android apps and compared its performance with two state-of-the-art UI-based approaches, Monkey and Dynodroid. Our experimental results show that EHBDroid is significantly more effective and efficient than Monkey and Dynodroid: in a much shorter time, EHBDroid achieves as much as 22.3% higher statement coverage (11.1% on average) than the other two approaches, and found 12 bugs in these benchmarks, including 5 new bugs that the other two failed to find.},
author = {Song, Wei and Qian, Xiangxing and Huang, Jeff},
doi = {10.1109/ASE.2017.8115615},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Qian, Huang - 2017 - EHBDroid Beyond GUI testing for Android applications.pdf:pdf},
isbn = {9781538626849},
journal = {ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
keywords = {Android,automated testing,event generation,event handlers},
pages = {27--37},
title = {EHBDroid: Beyond GUI testing for Android applications},
year = {2017}
}
@article{Lu2019,
abstract = {Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore, need to be thoroughly tested. Unfortunately, the specific preferences used in test cases are typically not explicitly specified, forcing testers to manually set options or blindly try different option combinations. To effectively test the impacts of different preference options, this paper presents PREFEST, as a preference-wise enhanced automatic testing approach, for Android apps. Given a set of test cases, PREFEST can locate the preferences that may affect the test cases with a static and dynamic combined analysis on the app under test, and execute these test cases only under necessary option combinations. The evaluation shows that PREFEST can improve 6.8% code coverage and 12.3% branch coverage and find five more real bugs compared to testing with the original test cases. The test cost is reduced by 99% for both the number of test cases and the testing time, compared to testing under pairwise combination of options.},
author = {Lu, Yifei and Pan, Minxue and Zhai, Juan and Zhang, Tian and Li, Xuandong},
doi = {10.1145/3338906.3338980},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2019 - Preference-wise testing for Android applications.pdf:pdf},
isbn = {9781450355728},
journal = {ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Android apps,Android testing,Preference-wise testing},
pages = {268--278},
title = {Preference-wise testing for Android applications},
year = {2019}
}
@article{Gu2019,
abstract = {This paper introduces a new, fully automated modelbased approach for effective testing of Android apps. Different from existing model-based approaches that guide testing with a static GUI model (i.e., the model does not evolve its abstraction during testing, and is thus often imprecise), our approach dynamically optimizes the model by leveraging the runtime information during testing. This capability of model evolution significantly improves model precision, and thus dramatically enhances the testing effectiveness compared to existing approaches, which our evaluation confirms.We have realized our technique in a practical tool, APE. On 15 large, widely-used apps from the Google Play Store, APE outperforms the state-of-the-art Android GUI testing tools in terms of both testing coverage and the number of detected unique crashes. To further demonstrate APE's effectiveness and usability, we conduct another evaluation of APE on 1,316 popular apps, where it found 537 unique crashes. Out of the 38 reported crashes, 13 have been fixed and 5 have been confirmed.},
author = {Gu, Tianxiao and Sun, Chengnian and Ma, Xiaoxing and Cao, Chun and Xu, Chang and Yao, Yuan and Zhang, Qirun and Lu, Jian and Su, Zhendong},
doi = {10.1109/ICSE.2019.00042},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2019 - Practical GUI Testing of Android Applications Via Model Abstraction and Refinement.pdf:pdf},
isbn = {9781728108698},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {CEGAR,GUI testing,mobile app testing},
pages = {269--280},
title = {Practical GUI Testing of Android Applications Via Model Abstraction and Refinement},
volume = {2019-May},
year = {2019}
}
@article{Yan2020,
abstract = {Existing GUI testing approaches of Android apps usually test apps from a single entry. In this way, the marginal activities far away from the default entry are difficult to be covered. The marginal activities may fail to be launched due to requiring a great number of activity transitions or involving complex user operations, leading to uneven coverage on activity components. Besides, since the test space of GUI programs is infinite, it is difficult to test activities under complete launching contexts using single-entry testing approaches. In this paper, we address these issues by constructing activity launching contexts and proposing a multiple-entry testing framework. We perform an inter-procedural, flow-, context- and pathsensitive analysis to build activity launching models and generate complete launching contexts. By activity exposing and static analysis, we could launch activities directly under various contexts without performing long event sequence on GUI. Besides, to achieve an in-depth exploration, we design an adaptive exploration framework which supports the multiple-entry exploration and dynamically assigns weights to entries in each turn. Our approach is implemented in a tool called Fax, with an activity launching strategy Faxla and an exploration strategy Faxex . The experiments on 20 real-world apps show that Faxla can cover 96.4% and successfully launch 60.6% activities, based on which Faxex further achieves a relatively 19.7% improvement on method coverage compared with the most popular tool Monkey. Our tool also behaves well in revealing hidden bugs. Fax can trigger over seven hundred unique crashes, including 180 Errors and 539 Warnings, which is significantly higher than those of other tools. Among the 46 bugs reported to developers on Github, 33 have been fixed up to now.},
author = {Yan, Jiwei and Liu, Hao and Pan, Linjie and Yan, Jun and Zhang, Jian and Liang, Bin},
doi = {10.1145/3377811.3380347},
isbn = {9781450371216},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Android app,Icc,Multiple-entry testing,Static analysis},
pages = {457--468},
title = {{Multiple-entry testing of android applications by constructing activity launching contexts}},
year = {2020}
}
@article{Riccio2018,
abstract = {Android is today the world's most popular mobile operating system and the demand for quality to Android mobile apps has grown together with their spread. Testing is a well-known approach for assuring the quality of software applications but Android apps have several peculiarities compared to traditional software applications that have to be taken into account by testers. Several studies have pointed out that mobile apps suffer from issues that can be attributed to Activity lifecycle mishandling, e.g. crashes, hangs, waste of system resources. Therefore the lifecycle of the Activities composing an app should be properly considered by testing approaches. In this paper we propose ALARic, a fully automated Black-Box Event-based testing technique that explores an application under test for detecting issues tied to the Android Activity lifecycle. ALARic has been implemented in a tool. We conducted an experiment involving 15 real Android apps that showed the effectiveness of ALARic in finding GUI failures and crashes tied to the Activity lifecycle. In the study, ALARic proved to be more effective in detecting crashes than Monkey, the state-of-The practice automated Android testing tool.},
author = {Riccio, Vincenzo and Amalfitano, Domenico and Fasolino, Anna Rita},
doi = {10.1145/3236454.3236490},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riccio, Amalfitano, Fasolino - 2018 - Is this the lifecycle we really want An automated black-box testing approach for Android activitie.pdf:pdf},
isbn = {9781450359399},
journal = {Companion Proceedings for the ISSTA/ECOOP 2018 Workshops},
pages = {68--77},
title = {{Is this the lifecycle we really want?: An automated black-box testing approach for Android activities}},
year = {2018}
}
@article{Bonett2018,
abstract = {Mobile application security has been one of the major areas of security research in the last decade. Numerous application analysis tools have been proposed in response to malicious, curious, or vulnerable apps. However, existing tools, and specifically, static analysis tools, trade soundness of the analysis for precision and performance, and are hence soundy. Unfortunately, the specific unsound choices or flaws in the design of these tools are often not known or well-documented, leading to a misplaced confidence among researchers, developers, and users. This paper proposes the Mutation-based soundness evaluation ($\mu$SE) framework, which systematically evaluates Android static analysis tools to discover, document, and fix, flaws, by leveraging the well-founded practice of mutation analysis. We implement $\mu$SE as a semi-automated framework, and apply it to a set of prominent Android static analysis tools that detect private data leaks in apps. As the result of an in-depth analysis of one of the major tools, we discover 13 undocumented flaws. More importantly, we discover that all 13 flaws propagate to tools that inherit the flawed tool. We successfully fix one of the flaws in cooperation with the tool developers. Our results motivate the urgent need for systematic discovery and documentation of unsound choices in soundy tools, and demonstrate the opportunities in leveraging mutation testing in achieving this goal.},
author = {Bonett, Richard and Kafle, Kaushal and Moran, Kevin and Nadkarni, Adwait and Poshyvanyk, Denys},
eprint = {1806.09761},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonett et al. - 2018 - Discovering flaws in security-focused static analysis tools for android using systematic mutation.pdf:pdf},
isbn = {9781939133045},
journal = {Proceedings of the 27th USENIX Security Symposium},
pages = {1263--1280},
title = {Discovering flaws in security-focused static analysis tools for android using systematic mutation},
year = {2018}
}
@article{Bohme2021,
author = {Bhme, Marcel and Cojocaru, Lucia},
isbn = {9781450371216},
keywords = {#AppCollection},
mendeley-tags = {#AppCollection},
title = {Time-travel Testing of Android Apps},
year = {2021}
}
@article{Tud-cs--2013,
author = {Tud-cs--, Nr and Fritz, Christian and Spride, E C and Arzt, Steven and Spride, E C and Rasthofer, Siegfried and Spride, E C and Bodden, Eric and Spride, E C and Klein, Jacques and Traon, Yves and Octeau, Damien and Mcdaniel, Patrick},
keywords = {10,5,75,according to a recent,android,android now has about,market share in the,mobile-phone market,static analysis,study,taint analysis,with a 91},
title = {Highly Precise Taint Analysis for Android Application},
year = {2013}
}
@article{Calzavara,
author = {Calzavara, Stefano and Grishchenko, Ilya and Koutsos, Adrien and Maffei, Matteo},
eprint = {arXiv:1705.10482v2},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calzavara et al. - Unknown - A Sound Flow-Sensitive Heap Abstraction for the Static Analysis of Android Applications.pdf:pdf},
pages = {1--66},
title = {A Sound Flow-Sensitive Heap Abstraction for the Static Analysis of Android Applications}
}
@article{Octeau2013,
abstract = {Name: Epicc},
author = {Octeau, Damien and McDaniel, Patrick and Jha, Somesh and Bartel, Alexandre and Bodden, Eric and Klein, Jacques and {Le Traon}, 
doi = {10.1016/j.ejps.2018.07.016},
isbn = {978-1-931971-03-4},
journal = {USENIX Security 2013},
pages = {543--558},
title = {Effective inter-component communication mapping in android with epicc: An essential step towards holistic security analysis},
year = {2013}
}
@article{Calzavara2016,
abstract = {We present HornDroid, a new tool for the static analysis of information flow properties in Android applications. The core idea underlying HornDroid is to use Horn clauses for soundly abstracting the semantics of Android applications and to express security properties as a set of proof obligations that are automatically discharged by an off-the-shelf SMT solver. This approach makes it possible to fine-tune the analysis in order to achieve a high degree of precision while still using off-the-shelf verification tools, thereby leveraging the recent advances in this field. As a matter of fact, HornDroid outperforms state-of-the-art Android static analysis tools on benchmarks proposed by the community. Moreover, HornDroid is the first static analysis tool for Android to come with a formal proof of soundness, which covers the core of the analysis technique: besides yielding correctness assurances, this proof allowed us to identify some critical corner-cases that affect the soundness guarantees provided by some of the previous static analysis tools for Android.},
author = {Calzavara, Stefano and Grishchenko, Ilya and Maffei, Matteo},
doi = {10.1109/EuroSP.2016.16},
eprint = {1707.07866},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calzavara, Grishchenko, Maffei - 2016 - HornDroid Practical and sound static analysis of android applications by SMT solving.pdf:pdf},
isbn = {9781509017515},
journal = {Proceedings - 2016 IEEE European Symposium on Security and Privacy, EURO S and P 2016},
pages = {47--62},
title = {HornDroid: Practical and sound static analysis of android applications by SMT solving},
year = {2016}
}
@article{Razeen2018,
abstract = {The most promising way to improve the performance of dynamic information-flow tracking (DIFT) for machine code is to only track instructions when they process tainted data. Unfortunately, prior approaches to on-demand DIFT are a poor match for modern mobile platforms that rely heavily on parallelism to provide good inter-activity in the face of computationally intensive tasks like image processing. The main shortcoming of these prior efforts is that they cannot support an arbitrary mix of parallel threads due to the limitations of page protections. In this paper, we identify parallel permissions as a key require-ment for multithreaded, on-demand native DIFT, and we describe the design and implementation of a system called SandTrap that embodies this approach. Using our prototype implementation, we demonstrate that SandTrap's native DIFT overhead is proportional to the amount of tainted data that native code processes. For exam-ple, in the photo-sharing app Instagram, SandTrap's performance is close to baseline (1x) when the app does not access tainted data. When it does, SandTrap imposes a slowdown comparable to prior DIFT systems (8x).},
author = {Razeen, Ali and Lebeck, Alvin R and Liu, David H and Meijer, Alexander and Cox, Landon P},
doi = {10.1145/3210240.3210321},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Razeen et al. - 2018 - SandTrap Tracking Information Flows On Demand with Parallel Permissions.pdf:pdf},
isbn = {9781450357203},
journal = {ACM International Conference on Mobile Systems, Applications, and Services (MobiSys)},
keywords = {KEYWORDS parallel memory permissions,Multithreading,dynamic information-flow tracking,native code},
title = {SandTrap: Tracking Information Flows On Demand with Parallel Permissions},
url = {https:},
year = {2018}
}
@article{Costanzo,
author = {Costanzo, David},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Costanzo - Unknown - End-to-End Verification of Information-Flow Security for C and Assembly Programs.pdf:pdf},
isbn = {9781450342612},
title = {End-to-End Verification of Information-Flow Security for C and Assembly Programs}
}
@article{Tsutano,
author = {Tsutano, Yutaka and Bachala, Shakthi and Srisa-an, Witawas and Rothermel, Gregg and Dinh, Jackson},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsutano et al. - Unknown - An Efficient , Robust , and Scalable Approach for Analyzing Interacting Android Apps.pdf:pdf},
title = {An Efficient , Robust , and Scalable Approach for Analyzing Interacting Android Apps}
}
@article{Hedin,
author = {Hedin, Daniel and Sabelfeld, Andrei},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hedin, Sabelfeld - Unknown - A Perspective on Information-Flow Control.pdf:pdf},
title = {A Perspective on Information-Flow Control}
}
@article{Paper2017,
author = {Paper, Conference and Jaipur, Technology and Inp, Herbreteau Bordeaux},
doi = {10.1109/Trustcom/BigDataSE/ICESS.2017.249},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paper, Jaipur, Inp - 2017 - SneakLeak Detecting Multipartite Leakage Paths in Android Apps.pdf:pdf},
number = {August},
title = {SneakLeak : Detecting Multipartite Leakage Paths in Android Apps},
year = {2017}
}
@article{Wei2017,
author = {Wei, Fengguo and Roy, Sankardas and Ou, Xinming},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei, Roy, Ou - 2017 - Amandroid A Precise and General Inter-component Data Flow Analysis Framework for Security Vetting of Android.pdf:pdf},
title = {Amandroid : A Precise and General Inter-component Data Flow Analysis Framework for Security Vetting of Android},
year = {2017}
}
@article{B,
author = {B, Elias Athanasopoulos and Kemerlis, Vasileios P and Portokalidis, Georgios},
doi = {10.1007/978-3-319-45744-4},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/B, Kemerlis, Portokalidis - Unknown - NaClDroid Native Code Isolation.pdf:pdf},
isbn = {9783319457444},
keywords = {sfi},
pages = {422--439},
title = {NaClDroid : Native Code Isolation},
volume = {1}
}
@article{Sun2014,
abstract = {Native code is external, difficult to analyze, mostly computation. They isolate it by putting it in a different app with zero permissions. They do not require OS mods or source code, but they recompile the app in two separate apps (Java and native). They need to proxy all the JNI stuff. Good performance MiBench (17% - 0.5%). 28/30 real-wold apps work. App to 183% with high context switches.},
author = {Sun, Mengtao and Tan, Gang},
doi = {10.1145/2627393.2627396},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Tan - 2014 - NativeGuard Protecting Android Applications from Third-Party Native Libraries.pdf:pdf},
isbn = {9781450329729},
journal = {Proceedings of the 2014 ACM Conference on Security and Privacy in Wireless & Mobile Networks (WiSec)},
keywords = {android,java native interface,privilege isolation},
pages = {165--176},
title = {NativeGuard: Protecting Android Applications from Third-Party Native Libraries},
year = {2014}
}
@article{Alam2017,
abstract = {According to the Symantec and F-Secure threat reports, mobile malware development in 2013 and 2014 has continued to focus almost exclusively ($\sim$99%) on the Android platform. Malware writers are applying stealthy mutations (obfuscations) to create malware variants, thwarting detection by signature-based detectors. In addition, the plethora of more sophisticated detectors making use of static analysis techniques to detect such variants operate only at the bytecode level, meaning that malware embedded in native code goes undetected. A recent study shows that 86% of the most popular Android applications contain native code, making native code malware a plausible threat vector. This paper proposes DroidNative, an Android malware detector that uses specific control flow patterns to reduce the effect of obfuscations and provides automation. As far as we know, DroidNative is the first system that builds cross-platform (x86 and ARM) semantic-based signatures at the Android native code level, allowing the system to detect malware embedded in either bytecode or native code. When tested with a dataset of 5490 samples, DroidNative achieves a detection rate (DR) of 93.57% and a false positive rate of 2.7%. When tested with traditional malware variants, it achieves a DR of 99.48%, compared to the DRs of academic and commercial tools that range from 8.33% to 93.22%.},
author = {Alam, Shahid and Qu, Zhengyang and Riley, Ryan and Chen, Yan and Rastogi, Vaibhav},
doi = {10.1016/j.cose.2016.11.011},
eprint = {1602.04693},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alam et al. - 2017 - DroidNative Automating and optimizing detection of Android native code malware variants.pdf:pdf},
issn = {01674048},
journal = {Computers and Security},
keywords = {Android native code,Control flow analysis,Data mining,Malware analysis,Malware variant detection},
pages = {230--246},
publisher = {Elsevier Ltd},
title = {DroidNative: Automating and optimizing detection of Android native code malware variants},
url = {http://dx.doi.org/10.1016/j.cose.2016.11.011},
volume = {65},
year = {2017}
}
@article{Qian2014,
abstract = {Android provides native development kit through JNI for developing high-performance applications (or simply apps). Although recent years have witnessed a considerable increase in the number of apps employing native libraries, only a few systems can examine them. However, none of them scrutinizes the interactions through JNI in them. In this paper, we conduct a systematic study on tracking information flows through JNI in apps. More precisely, we first perform a large-scale examination on apps using JNI and report interesting observations. Then, we identify scenarios where information flows uncaught by existing systems can result in information leakage. Based on these insights, we propose and implement NDroid, an efficient dynamic taint analysis system for check-ing information flows through JNI. The evaluation through real apps shows NDroid can effectively identify information leaks through JNI with low performance overheads.},
author = {Qian, Chenxiong and Luo, Xiapu and Shao, Yuru and Chan, Alvin T.S.},
doi = {10.1109/DSN.2014.30},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2014 - On Tracking Information Flows through JNI in Android Applications.pdf:pdf},
isbn = {9781479922338},
journal = {Proceedings - 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2014},
pages = {180--191},
title = {On Tracking Information Flows through JNI in Android Applications},
year = {2014}
}
@article{Afonso2016,
abstract = {Current static analysis techniques for Android ap-plications operate at the Java levelthat is, they analyze either the Java source code or the Dalvik bytecode. However, Android allows developers to write code in C or C++ that is cross-compiled to multiple binary architectures. Furthermore, the Java-written components and the native code components (C or C++) can interact. Native code can access all of the Android APIs that the Java code can access, as well as alter the Dalvik Virtual Machine, thus rendering static analysis techniques for Java unsound or misleading. In addition, malicious apps frequently hide their malicious functionality in native code or use native code to launch kernel exploits. It is because of these security concerns that previous research has proposed native code sandboxing, as well as mechanisms to enforce security policies in the sandbox. However, it is not clear whether the large-scale adoption of these mechanisms is practical: is it possible to define a meaningful security policy that can be imposed by a native code sandbox without breaking app functionality? In this paper, we perform an extensive analysis of the native code usage in 1.2 million Android apps. We first used static analysis to identify a set of 446k apps potentially using native code, and we then analyzed this set using dynamic analysis. This analysis demonstrates that sandboxing native code with no permissions is not ideal, as apps' native code components perform activities that require Android permissions. However, our analysis provided very encouraging insights that make us believe that sandboxing native code can be feasible and useful in practice. In fact, it was possible to automatically generate a native code sandboxing policy, which is derived from our analysis, that limits many malicious behaviors while still allowing the correct execution of the behavior witnessed during dynamic analysis for 99.77% of the benign apps in our dataset. The usage of our system to generate policies would reduce the attack surface available to native code and, as a further benefit, it would also enable more reliable static analysis of Java code.},
author = {Afonso, Vitor and Bianchi, Antonio and Fratantonio, Yanick and Doupe, Adam and Polino, Mario and de Geus, Paulo and Kruegel, Christopher and Vigna, Giovanni},
doi = {10.14722/ndss.2016.23384},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Afonso et al. - 2016 - Going Native Using a Large-Scale Analysis of Android Apps to Create a Practical Native-Code Sandboxing Policy.pdf:pdf},
isbn = {1-891562-41-X},
issn = {189156241X},
journal = {Proceedings 2016 Network and Distributed System Security Symposium},
title = {Going Native: Using a Large-Scale Analysis of Android Apps to Create a Practical Native-Code Sandboxing Policy},
url = {https://www.internetsociety.org/sites/default/files/blogs-media/going-native-large-scale-analysis-android-apps-practical-native-code-sandboxing-policy.pdf},
year = {2016}
}
@article{Gharat2016,
abstract = {The bottom up interprocedural methods construct summary flow functions for procedures to represent their calls. These methods have been effectively used for many analyses except for flow and context sensitive points-to analysis which require representing indirect accesses of pointees defined in the callers. This is conventionally handled by using placeholders which explicate the unknown locations resulting in either a large number of placeholders or multiple call-specific summary flow functions for a procedure. We propose a bounded representation of summary flow functions for may points-to analysis called the higher order reachability graph (HRG). The conventional graph reachability based program analyses relate variables but not their pointees whereas HRGs relate the (transitively indirect) pointees of a variable with those of another variable in terms of indirection levels. A simple arithmetic on indirection levels allows unknown locations to be left implicit and is sufficient to relate the indirect pointees defined in the callers obviating the need of placeholders. Thus we construct a single summary flow function (HRG) per procedure which is bounded by the number of variables, is flow sensitive, and performs strong updates in the calling contexts. Our empirical measurements on SPEC benchmarks show that most summary flow functions are compact and are used multiple times. We have been able to scale flow and context sensitive points-to analysis to 158 kLoC using HRGs. Thus, this is a promising direction for further investigations in efficiency and scalability of points-to analysis without compromising on precision.},
author = {Gharat, Pritam M. and Khedker, Uday P.},
eprint = {1603.09597},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gharat, Khedker - 2016 - Flow and Context Sensitive Points-to Analysis using Higher Order Reachability.pdf:pdf},
number = {April},
pages = {1--43},
title = {Flow and Context Sensitive Points-to Analysis using Higher Order Reachability},
url = {http://arxiv.org/abs/1603.09597},
year = {2016}
}
@inproceedings{Sadeghi2015,
abstract = {The state-of-the-art in securing mobile software systems are substantially intended to detect and mitigate vulnerabilities in a single app, but fail to identify vulnerabilities that arise due to the interaction of multiple apps, such as collusion attacks and privilege escalation chaining, shown to be quite common in the apps on the market. This paper demonstrates COVERT, a novel approach and accompanying tool-suite that relies on a hybrid static analysis and lightweight formal analysis technique to enable compositional security assessment of complex software. Through static analysis of Android application packages, it extracts relevant security specifications in an analyzable formal specification language, and checks them as a whole for inter-app vulnerabilities. To our knowledge, COVERT is the first formally-precise analysis tool for automated compositional analysis of Android apps. Our study of hundreds of Android apps revealed dozens of inter-app vulnerabilities, many of which were previously unknown.},
author = {Sadeghi, Alireza and Bagheri, Hamid and Malek, Sam},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.233},
isbn = {9781479919345},
issn = {02705257},
title = {Analysis of Android Inter-App Security Vulnerabilities Using COVERT},
volume = {2},
year = {2015}
}
@inproceedings{Li2015,
abstract = {Shake Them All is a popular "Wallpaper" application exceeding millions of downloads on Google Play. At installation, this application is given permission to (1) access the Internet (for updating wallpapers) and (2) use the device microphone (to change background following noise changes). With these permissions, the application could silently record user conversations and upload them remotely. To give more confidence about how Shake Them All actually processes what it records, it is necessary to build a precise analysis tool that tracks the flow of any sensitive data from its source point to any sink, especially if those are in different components. Since Android applications may leak private data carelessly or maliciously, we propose IccTA, a static taint analyzer to detect privacy leaks among components in Android applications. IccTA goes beyond state-of-the-art approaches by supporting inter- component detection. By propagating context information among components, IccTA improves the precision of the analysis. IccTA outperforms existing tools on two benchmarks for ICC-leak detectors: DroidBench and ICC-Bench. Moreover, our approach detects 534 ICC leaks in 108 apps from MalGenome and 2,395 ICC leaks in 337 apps in a set of 15,000 Google Play apps.},
author = {Li, Li and Bartel, Alexandre and Bissyand}, 
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.48},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - IccTA Detecting inter-component privacy leaks in android apps.pdf:pdf},
isbn = {9781479919345},
issn = {02705257},
pages = {280--291},
title = {IccTA: Detecting inter-component privacy leaks in android apps},
volume = {1},
year = {2015}
}
@article{Oluwatimi2017,
abstract = {2016 IEEE.When a mobile device is used for both personal and business purposes, securing enterprise content and preserving employees' privacy are vital. Containerization is a promising approach to address such requirements.},
author = {Oluwatimi, Oyindamola and Midi, Daniele and Bertino, Elisa},
doi = {10.1109/MSP.2017.12},
issn = {15584046},
journal = {IEEE Security and Privacy},
keywords = {access controls,containerization,privacy,security,virtualization},
number = {1},
pages = {22--31},
title = {Overview of Mobile Containerization Approaches and Open Research Directions},
volume = {15},
year = {2017}
}
@article{ARM2009,
abstract = {This document provides an overview of the ARM TrustZone technology and how this can provide a practical level of security through careful System-on-a-Chip (SoC) configuration and software design. ARM TrustZone technology includes the ARM Security Extensions to the processor, the security signals added to the AMBA 3 bus infrastructure, and a number of pieces of peripheral Intellectual Property (IP) which can be used to build security on top of the processor architecture and system architecture.},
author = {ARM},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ARM - 2009 - ARM Security Technology. Building a Secure System using TrustZone Technology ARM.pdf:pdf},
journal = {ARM white paper},
pages = {108},
title = {ARM Security Technology. Building a Secure System using TrustZone Technology ARM},
url = {http://infocenter.arm.com/help/topic/com.arm.doc.prd29-genc-009492c/PRD29-GENC-009492C_trustzone_security_whitepaper.pdf},
year = {2009}
}
@article{Girard2013,
author = {Girard, John and Cosgrove, Terrence and Basso, Monica and Redman, Phillip},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girard et al. - 2013 - Magic Quadrant for Mobile Device Management Software.pdf:pdf},
number = {May},
title = {Magic Quadrant for Mobile Device Management Software},
year = {2013}
}
@article{Gordon2015,
abstract = {We present DroidSafe, a static information flow analysis tool that reports potential leaks of sensitive information in Android applications. DroidSafe includes a comprehensive model of the Android API and runtime, built on top of the Android Open Source Project implementation of the Android API. This model accurately captures the data-flow and aliasing semantics of API calls, life-cycle event handlers, callback handlers, and native methods. DroidSafe includes an analysis to statically resolve dynamic inter-component communication linkage mechanisms, enabling DroidSafe to precisely track intent- and message- and RPC-mediated information flows that traverse multiple Android components. The DroidSafe information flow analysis has high-depth heap and method object-sensitivity, and the analysis considers all possible interleavings of life-cycle events and callback handlers. We also present several domain-specific analyses that significantly enhance DroidSafe's ability to successfully analyze Android applications. We evaluate DroidSafe on a suite of 24 real-world Android applications that contain malicious information leaks. These applications were developed by independent, hostile Red Team organizations. The malicious flows in these applications were designed specifically to evade or overwhelm information flow analysis tools. DroidSafe detects all of the malicious flows in all 24 applications. We compare DroidSafe to a current state-of-the-art analysis, which detects malicious flows in only 3 of these applications. We also evaluate DroidSafe on DroidBench version 1.2, a suite of 65 independently-developed Android micro-applications designed to evaluate the capabilities of information flow analysis systems. We report the highest information flow precision and recall to date for DroidBench 1.2. Michael I. Gordon, Deokhwan Kim, and Jeff Perkins (MIT CSAIL), Limei Gilham (Kestrel Institute), Nguyen Nguyen (unaffiliated), and Martin Rinard (MIT CSAIL)},
author = {Gordon, Michael I and Kim, Deokhwan and Perkins, Jeff and Gilham, Limei and Nguyen, Nguyen and Rinard, Martin},
doi = {10.14722/ndss.2015.23089},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordon et al. - 2015 - Information-Flow Analysis of Android Applications in DroidSafe.pdf:pdf},
isbn = {189156238X},
journal = {2015 Network and Distributed System Security (NDSS) Symposium},
number = {February},
pages = {8--11},
title = {Information-Flow Analysis of Android Applications in DroidSafe},
year = {2015}
}
@article{Chang2013,
author = {Chang, J Morris},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang - 2013 - Securing BYOD.pdf:pdf},
journal = {BYOD-Reference-Guide},
number = {March},
pages = {9--11},
title = {{Securing BYOD}},
url = {http://resources.idgenterprise.com/original/AST-0089445_BYOD-Security.pdf},
year = {2013}
}
@article{Mavroudis,
abstract = {The ultrasound tracking ecosystem is a relatively new and emerging set of technologies that use in- audible beacons to track users and devices. Our work is the first comprehensive security analysis of this little known ecosystem.},
author = {Mavroudis, V and Hao, S and Fratantonio, Y and Maggi, F and Kruegel, C and Vigna, G},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mavroudis et al. - Unknown - The Ultrasound Tracking Ecosystem.pdf:pdf},
title = {The Ultrasound Tracking Ecosystem}
}
@article{Rosenberg2014,
author = {Rosenberg, Dan},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenberg - 2014 - Reflections on Trusting TrustZone What is TrustZone(2).pdf:pdf},
journal = {Black Hat USA},
title = {Reflections on Trusting TrustZone What is TrustZone ?},
year = {2014}
}
@article{Fratantonio2016,
abstract = {Android is the most popular mobile platform today, and it is also the mobile operating system that is most heavily targeted by malware. Existing static analyses are effective in detecting the presence of most malicious code and unwanted information flows. However, certain types of malice are very dif-ficult to capture explicitly by modeling permission sets, suspicious API calls, or unwanted information flows. One important type of such malice is malicious application logic, where a program (often subtly) modifies its outputs or per-forms actions that violate the expectations of the user. Malicious application logic is very hard to identify without a specification of the " normal, " expected functionality of the application. We refer to malicious application logic that is executed, or triggered, only under certain (often narrow) circumstances as a logic bomb. This is a powerful mechanism that is commonly employed by targeted malware, often used as part of APTs and state-sponsored attacks: in fact, in this scenario, the malware is designed to target specific victims and to only activate under certain circumstances. In this paper, we make a first step towards detecting logic bombs. In particular, we propose trigger analysis, a new static analysis technique that seeks to automatically identify triggers in Android applications. Our analysis combines symbolic execu-tion, path predicate reconstruction and minimization, and inter-procedural control-dependency analysis to enable the precise detection and characterization of triggers, and it overcomes several limitations of existing approaches. We implemented a prototype of our analysis, called TRIG-GERSCOPE, and we evaluated it over a large corpus of 9,582 benign apps from the Google Play Store and a set of trigger-based malware, including the recently-discovered HackingTeam's RCSAndroid advanced malware. Our system is capable of automatically identify several interesting time-, location-, and SMS-related triggers, is affected by a low false positive rate (0.38%), and it achieves 100% detection rate on the malware set. We also show how existing approaches, specifically when tasked to detect logic bombs, are affected by either a very high false positive rate or false negative rate. Finally, we discuss the logic bombs identified by our analysis, including two previously-unknown backdoors in benign apps.},
author = {Fratantonio, Yanick and Bianchi, Antonio and Robertson, William and Kirda, Engin and Kruegel, Christopher and Vigna, Giovanni},
doi = {10.1109/SP.2016.30},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fratantonio et al. - 2016 - TriggerScope Towards Detecting Logic Bombs in Android Applications.pdf:pdf},
isbn = {9781509008247},
journal = {Proceedings - 2016 IEEE Symposium on Security and Privacy, SP 2016},
keywords = {Mobile Security,Static Analysis},
pages = {377--396},
title = {TriggerScope: Towards Detecting Logic Bombs in Android Applications},
year = {2016}
}
@article{Kanonov2016,
abstract = {Bring Your Own Device (BYOD) is a growing trend among enterprises, aiming to improve workers' mobility and productivity via their smartphones. The threats and dangers posed by the smartphones to the enterprise are also ever-growing. Such dangers can be mitigated by running the enterprise software inside a "secure container" on the smartphone. In our work we present a systematic assessment of security critical areas in design and implementation of a secure container for Android using reverse engineering and attacker-inspired methods. We do this through a case-study of Samsung KNOX, a real-world product deployed on millions of devices. Our research shows how KNOX security features work behind the scenes and lets us compare the vendor's public security claims against reality. Along the way we identified several design weaknesses and a few vulnerabilities that were disclosed to Samsung.},
author = {Kanonov, Uri and Wool, Avishai},
doi = {10.1145/2994459.2994470},
eprint = {1605.08567},
isbn = {9781450345644},
pages = {1--37},
title = {Secure Containers in Android: the Samsung KNOX Case Study},
url = {http://arxiv.org/abs/1605.08567},
year = {2016}
}
@article{Arzt2014,
abstract = {Today's smartphones are a ubiquitous source of private and confi- dential data. At the same time, smartphone users are plagued by carelessly programmed apps that leak important data by accident, and by malicious apps that exploit their given privileges to copy such data intentionally. While existing static taint-analysis approaches have the potential of detecting such data leaks ahead of time, all ap- proaches for Android use a number of coarse-grain approximations that can yield high numbers of missed leaks and false alarms. In this work we thus present FLOWDROID, a novel and highly precise static taint analysis for Android applications. A precise model of Android's lifecycle allows the analysis to properly handle callbacks invoked by the Android framework, while context, flow, field and object-sensitivity allows the analysis to reduce the number of false alarms. Novel on-demand algorithms help FLOWDROID maintain high efficiency and precision at the same time. We also propose DROIDBENCH, an open test suite for evaluating the effectiveness and accuracy of taint-analysis tools specifically for Android apps. As we show through a set of experiments using SecuriBench Micro, DROIDBENCH, and a set of well-known An- droid test applications, FLOWDROID finds a very high fraction of data leaks while keeping the rate of false positives low. On DROIDBENCH, FLOWDROID achieves 93% recall and 86% pre- cision, greatly outperforming the commercial tools IBM AppScan Source and Fortify SCA. FLOWDROID successfully finds leaks in a subset of 500 apps from Google Play and about 1,000 malware apps from},
author = {Arzt, Steven and Rasthofer, Siegfried and Fritz, Christian and Bodden, Eric and Bartel, Alexandre and Klein, Jacques and Traon, Yves Le and Octeau, Damien and Mcdaniel, Patrick},
doi = {10.1145/2594291.2594299},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arzt et al. - 2014 - FlowDroid Precise Context , Flow , Field , Object-sensitive and Lifecycle-aware Taint Analysis for Android Apps.pdf:pdf},
isbn = {9781450327848},
issn = {15232867},
journal = {PLDI '14 Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {259--269},
title = {FlowDroid : Precise Context , Flow , Field , Object-sensitive and Lifecycle-aware Taint Analysis for Android Apps},
year = {2014}
}
@article{Sadeghi2016,
abstract = {In parallel with the meteoric rise of mobile software, we are witnessing an alarming escalation in the number and sophistication of the security threats targeted at mobile platforms, particularly Android, as the dominant platform. While existing research has made significant progress towards detection and mitigation of Android security, gaps and challenges remain. This paper contributes a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area. We have carefully followed the systematic literature review process, and analyzed the results of more than 300 research papers, resulting in the most comprehensive and elaborate investigation of the literature in this area of research. The systematic analysis of the research literature has revealed patterns, trends, and gaps in the existing literature, and underlined key challenges and opportunities that will shape the focus of future research efforts.},
author = {Sadeghi, Alireza and Bagheri, Hamid and Garcia, Joshua and Malek, Sam},
doi = {10.1109/TSE.2016.2615307},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sadeghi et al. - 2016 - A Taxonomy and Qualitative Comparison of Program Analysis Techniques for Security Assessment of Android Software.pdf:pdf},
isbn = {0098-5589 VO  - 43},
journal = {IEEE Transactions on Software Engineering},
keywords = {Android Platform,Program Analysis,Security Assessment,Taxonomy and Survey},
title = {A Taxonomy and Qualitative Comparison of Program Analysis Techniques for Security Assessment of Android Software},
year = {2016}
}
@article{Lia,
abstract = {Context: Static analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analysers face when dealing with Android apps. Objective: We aim to provide a clear view of the state-of-the-art works that statically analyse Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed. Method: We have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011 -December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed. Results: Our in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artefacts publicly available. Conclusion: The research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers.},
author = {Li, Li and Bissyand}, 
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Static Analysis of Android Apps A Systematic Literature Review.pdf:pdf},
title = {Static Analysis of Android Apps: A Systematic Literature Review}
}
@article{Field2015,
abstract = {We've been hearing for years now that mobile security threats are coming into their own, both in terms of volume and capacity to inflict harm. And certainly we see plenty of evidence of evolving mobile malware strains and new exploits that focus on compromising mobile applications and data. So, is it time for enterprise mobility to come of age? Is 2015 the year when organizations will move past their fundamental BYOD debates and start discussing more progressive mobile security topics? That is the premise behind this Mobile Security Maturity Survey report. Inspired and sponsored by IBM, which has crafted a new Mobile Security Framework, this study focuses on the four key security pillars of IBM's structure: The device, content, applications and transactions. With an eye toward the shape of mobile security in 2015, this study focuses on:  Mobile Security Landscape  where are enterprises most vulnerable today?  Maturity Level  at which stage of development are these organizations?  2015 Agenda  how will budgets grow, and where will priority investments be made? It's a fascinating topic. Everyone is talking about mobile security, but not enough organizations are assessing and growing their mobile security maturity. Here's an opportunity to start that conversation and spark that growth.},
author = {Field, Tom},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Field - 2015 - The State of Mobile Security Maturity 2015 When Mobility Comes of Age.pdf:pdf},
title = {The State of Mobile Security Maturity 2015: When Mobility Comes of Age},
year = {2015}
}
@inproceedings{Asokan2012,
abstract = {We make the case for usable mobile security by outlining why usable security in mobile devices is important and why it is hard to achieve. We describe a number of current problems in mobile devices that need usable and secure solutions. Finally, we discuss the characteristics of mobile devices that can actually help in designing usable solutions to mobile security problems.},
author = {Asokan, N. and Kuo, Cynthia},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-28073-3_1},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Asokan, Kuo - 2012 - Usable mobile security.pdf:pdf},
isbn = {9783642280726},
issn = {03029743},
title = {Usable mobile security},
year = {2012}
}
@inproceedings{Junokas2017,
abstract = {Fault-tolerant distributed algorithms play an important role in many critical/high-availability applications. These algorithms are notori-ously difficult to implement correctly, due to asynchronous com-munication and the occurrence of faults, such as the network drop-ping messages or computers crashing. We introduce PSYNC, a domain specific language based on the Heard-Of model, which views asynchronous faulty systems as syn-chronous ones with an adversarial environment that simulates asyn-chrony and faults by dropping messages. We define a runtime sys-tem for PSYNC that efficiently executes on asynchronous networks. We formalize the relation between the runtime system and PSYNC in terms of observational refinement. The high-level lockstep ab-straction introduced by PSYNC simplifies the design and imple-mentation of fault-tolerant distributed algorithms and enables auto-mated formal verification. We have implemented an embedding of PSYNC in the SCALA programming language with a runtime system for asynchronous networks. We show the applicability of PSYNC by implementing several important fault-tolerant distributed algorithms and we com-pare the implementation of consensus algorithms in PSYNC against implementations in other languages in terms of code size, runtime efficiency, and verification.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.09436},
author = {Junokas, Michael J. and Kohlburn, Greg and Kumar, Sahil and Lane, Benjamin and Fu, Wai Tat and Lindgren, Robb},
booktitle = {CEUR Workshop Proceedings},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {arXiv:1603.09436},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Junokas et al. - 2017 - Using one-shot machine learning to implement real-time multimodal learning analytics.pdf:pdf},
isbn = {9781450335492},
issn = {16130073},
keywords = {Cognitive embodiment,Educational technology,Gesture recognition,One-shot machine learning},
title = {Using one-shot machine learning to implement real-time multimodal learning analytics},
year = {2017}
}
@article{BOWen,
abstract = {The safety of software is becoming increasingly important as computers pervade control systems on which h uman life depends. Whilst hardware has become signiicantly more reliable over the years, the same cannot be said of software. This has become more complex and methods to ensure its correctness have been slow in development. One approach i s t o mathematically verify software in such systems. This paper investigates the industrial use of these techniques, their advantages and disadvantages, and the introduction of standards and their recommendations concerning formal methods in this area. The cost of safety i s also considered.},
author = {{B O Wen}, 
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/B O Wen, Stavridou - Unknown - FORMAL METHODS AND SOFTWARE SAFETY.pdf:pdf},
title = {FORMAL METHODS AND SOFTWARE SAFETY}
}
@article{Moran2016,
abstract = {Mobile developers face unique challenges when detecting and reporting crashes in apps due to their prevailing GUI event-driven nature and additional sources of inputs (e.g., sensor readings). To support developers in these tasks, we introduce a novel, automated approach called CRASHSCOPE. This tool explores a given Android app using systematic input generation, according to several strategies informed by static and dynamic analyses, with the intrinsic goal of triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented crash report containing screenshots, detailed crash reproduction steps, the captured exception stack trace, and a fully replayable script that automatically reproduces the crash on a target device(s). We evaluated CRASHSCOPE's effectiveness in discovering crashes as compared to five state-of-the-art Android input generation tools on 61 applications. The results demonstrate that CRASHSCOPE performs about as well as current tools for detecting crashes and provides more detailed fault information. Additionally, in a study analyzing eight real-world Android app crashes, we found that CRASHSCOPE's reports are easily readable and allow for reliable reproduction of crashes by presenting more explicit information than human written reports.},
author = {Moran, Kevin and Linares-Vasquez, Mario and Bernal-Cardenas, Carlos and Vendome, Christopher and Poshyvanyk, Denys},
doi = {10.1109/ICST.2016.34},
eprint = {1706.01130},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moran et al. - 2016 - Automatically Discovering, Reporting and Reproducing Android Application Crashes.pdf:pdf},
isbn = {9781509018260},
journal = {Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016},
keywords = {GUI-testing,android,crash reports},
pages = {33--44},
title = {Automatically Discovering, Reporting and Reproducing Android Application Crashes},
year = {2016}
}
@article{Clapp2016,
author = {Clapp, Lazaro and Bastani, Osbert and Aiken, Alex},
doi = {10.1145/1235},
file = {:home/maryam/Documents/PhD/Android Testing/Crash Replay/FSE16 - Minimizing GUI Event Traces.pdf:pdf},
isbn = {9781450321389},
keywords = {android,delta debugging,testing,trace minimization},
title = {Minimizing GUI Event Traces},
year = {2016}
}
@article{Lib,
author = {Li, Cong and Jiang, Yanyan and Xu, Chang},
file = {:home/maryam/Documents/PhD/Android Testing/Crash Replay/FSE22-Cross-Device Record and Replay for Android Apps.pdf:pdf},
isbn = {9781450394130},
keywords = {2022,Android app testing, record and replay,acm reference format,and chang xu,android app testing,cong li,cross-device record and replay,record and replay,yanyan jiang},
title = {Cross-Device Record and Replay for Android Apps}
}
@article{Zhao2019,
abstract = {The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers heavily rely on bug reports in issue tracking systems to reproduce failures (e.g., crashes). However, the process of crash reproduction is often manually done by developers, making the resolution of bugs inefficient, especially that bug reports are often written in natural language. To improve the productivity of developers in resolving bug reports, in this paper, we introduce a novel approach, called ReCDroid, that can automatically reproduce crashes from bug reports for Android apps. ReCDroid uses a combination of natural language processing (NLP) and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash. We have evaluated ReCDroid on 51 original bug reports from 33 Android apps. The results show that ReCDroid successfully reproduced 33 crashes (63.5% success rate) directly from the textual description of bug reports. A user study involving 12 participants demonstrates that ReCDroid can improve the productivity of developers when resolving crash bug reports.},
author = {Zhao, Yu and Yu, Tingting and Su, Ting and Liu, Yang and Zheng, Wei and Zhang, Jingzhi and Halfond, William},
doi = {10.1109/ICSE.2019.00030},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2019 - ReCDroid Automatically Reproducing Android Application Crashes from Bug Reports.pdf:pdf},
isbn = {9781728108698},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Android,Bug reproduction,Natural language processing},
pages = {128--139},
title = {ReCDroid: Automatically Reproducing Android Application Crashes from Bug Reports},
volume = {2019-May},
year = {2019}
}
@article{Chaparro2019,
abstract = {A major problem with user-written bug reports, indicated by developers and documented by researchers, is the (lack of high) quality of the reported steps to reproduce the bugs. Low-quality steps to reproduce lead to excessive manual effort spent on bug triage and resolution. This paper proposes Euler, an approach that automatically identifies and assesses the quality of the steps to reproduce in a bug report, providing feedback to the reporters, which they can use to improve the bug report. The feedback provided by Euler was assessed by external evaluators and the results indicate that Euler correctly identified 98% of the existing steps to reproduce and 58% of the missing ones, while 73% of its quality annotations are correct.},
archivePrefix = {arXiv},
arxivId = {1906.07107},
author = {Chaparro, Oscar and Bernal},
doi = {10.1145/3338906.3338947},
eprint = {1906.07107},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaparro et al. - 2019 - Assessing the quality of the steps to reproduce in bug reports.pdf:pdf},
isbn = {9781450355728},
journal = {ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Bug Report Quality,Dynamic Software Analysis,Textual Analysis},
pages = {86--96},
title = {Assessing the quality of the steps to reproduce in bug reports},
year = {2019}
}
@article{Kong2020,
author = {Kong, Pingfan and Klein, Jacques},
eprint = {arXiv:2008.01676v1},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kong, Klein - 2020 - Anchor Locating Android Framework-specific Crashing Faults.pdf:pdf},
keywords = {android crash,crashing fault,fault localization},
number = {July 2017},
title = {Anchor: Locating Android Framework-specific Crashing Faults},
year = {2020}
}
@article{Fazzini2018,
abstract = {When users experience a software failure, they have the option of submitting a bug report and provide information about the failure and howit happened. If the bug report contains enough information, developers can then try to recreate the issue and investigate it, so as to eliminate its causes. Unfortunately, the number of bug reports filed by users is typically large, and the tasks of analyzing bug reports and reproducing the issues described therein can be extremely time consuming. To help make this process more efficient, in this paper we propose Yakusu, a technique that uses a combination of program analysis and natural language processing techniques to generate executable test cases from bug reports. We implemented Yakusu for Android apps and performed an empirical evaluation on a set of over 60 real bug reports for different real-world apps. Overall, our technique was successful in 59.7% of the cases; that is, for a majority of the bug reports, developers would not have to study the report to reproduce the issue described and could simply use the test cases automatically generated by Yakusu. Furthermore, in many of the remaining cases, Yakusu was unsuccessful due to limitations that can be addressed in future work.},
author = {Fazzini, Mattia and Prammer, Martin and D'Amorim, Marcelo and Orso, Alessandro},
doi = {10.1145/3213846.3213869},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fazzini et al. - 2018 - Automatically translating bug reports into test cases for mobile apps.pdf:pdf},
isbn = {9781450356992},
journal = {ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
keywords = {Mobile testing and debugging,Natural language processing},
pages = {141--152},
title = {Automatically translating bug reports into test cases for mobile apps},
year = {2018}
}
@article{Sui2019,
abstract = {Existing Android testing tools, such as Monkey, generate a large quantity and a wide variety of user events to expose latent GUI bugs in Android apps. However, even if a bug is found, a majority of the events thus generated are often redundant and bug-irrelevant. In addition, it is also time-consuming for developers to localize and replay the bug given a long and tedious event sequence (trace). This paper presents ECHO, an event trace reduction tool for effective bug replay by using a new differential GUI state analysis. Given a sequence of events (trace), ECHO aims at removing bug-irrelevant events by exploiting the differential behavior between the GUI states collected when their corresponding events are triggered. During dynamic testing, ECHO injects at most one lightweight inspection event after every event to collect its corresponding GUI state. A new adaptive model is proposed to selectively inject inspection events based on sliding windows to differentiate the GUI states on-the-fly in a single testing process. The experimental results show that ECHO improves the effectiveness of bug replay by removing 85.11% redundant events on average while also revealing the same bugs as those detected when full event sequences are used.},
author = {Sui, Yulei and Zhang, Yifei and Zheng, Wei and Zhang, Manqing and Xue, Jingling},
doi = {10.1145/3338906.3341183},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sui et al. - 2019 - Event trace reduction for effective bug replay of Android apps via differential GUI state analysis.pdf:pdf},
isbn = {9781450355728},
journal = {ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Android testing,Bug replay,Program analysis},
pages = {1095--1099},
title = {Event trace reduction for effective bug replay of Android apps via differential GUI state analysis},
year = {2019}
}
@article{Rountev,
author = {Rountev, Atanas},
isbn = {9781450326704},
keywords = {android,gui analysis,reference analysis},
title = {Static Reference Analysis for GUI Objects in Android Software Categories and Subject Descriptors}
}
@article{Wanga,
author = {Wang, Yan},
file = {:home/maryam/Documents/PhD/Android Static Analysis/SOAP - 16 - On the Unsoundness of Static Analysis for Android GUIs.pdf:pdf},
isbn = {9781450343855},
keywords = {android,soundness,static analysis},
pages = {18--23},
title = {On the Unsoundness of Static Analysis for Android GUIs}
}
@article{Liu2022,
abstract = {Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.},
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Che, Xing and Huang, Yuekai and Hu, Jun and Wang, Qing},
eprint = {2212.04732},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2022 - Fill in the Blank Context-aware Automated Text Input Generation for Mobile GUI Testing.pdf:pdf},
month = {dec},
title = {Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing},
url = {http://arxiv.org/abs/2212.04732},
year = {2022}
}
@article{Yazdanibanafshedaragh,
author = {Yazdanibanafshedaragh, Faraz and Malek, Sam},
title = {Deep GUI : Black-box GUI Input Generation with Deep Learning}
}
@article{Author2022a,
author = {Author, Anonymous},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Author - 2022 - MUBot Learning to Test Large-Scale Commercial Android Apps like a Human ( Experience Paper ).pdf:pdf},
journal = {Proceedings of ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2022)},
keywords = {android testing, graphical user interface, deep le},
number = {1},
pages = {18--22},
title = {MUBot : Learning to Test Large-Scale Commercial Android Apps like a Human ( Experience Paper )},
volume = {1},
year = {2022}
}
@article{Choi2018,
abstract = {In recent years, several automated GUI testing techniques for Android apps have been proposed. These tools have been shown to be effective in achieving good test coverage and in finding bugs without human intervention. Being automated, these tools typically run for a long time (say, for several hours), either until they saturate test coverage or until a testing time budget expires. Thus, these automated tools are not good at generating concise regression test suites that could be used for testing in incremental development of the apps and in regression testing. We propose a heuristic technique that helps create a small regression test suite for an Android app from a large test suite generated by an automated Android GUI testing tool. The key insight behind our technique is that if we can identify and remove some common forms of redundancies introduced by existing automated GUI testing tools, then we can drastically lower the time required to minimize a GUI test suite. We have implemented our algorithm in a prototype tool called DetReduce. We applied DetReduce to several Android apps and found that DetReduce reduces a test-suite by an average factor of 16.9 in size and 14.7 in running time. We also found that for a test suite generated by running SwiftHand and a randomized test generation algorithm for 8 hours, DetReduce minimizes the test suite in an average of 14.6 hours.},
author = {Choi, Wontae and Sen, Koushik and Necula, George and Wang, Wenyu},
doi = {10.1145/3180155.3180173},
isbn = {9781450356381},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Android,GUI,Test minimization},
title = {DetReduce: Minimizing Android GUI test suites for regression testing},
volume = {2018-Janua},
year = {2018}
}
@article{Sui2019a,
abstract = {Existing Android testing tools, such as Monkey, generate a large quantity and a wide variety of user events to expose latent GUI bugs in Android apps. However, even if a bug is found, a majority of the events thus generated are often redundant and bug-irrelevant. In addition, it is also time-consuming for developers to localize and replay the bug given a long and tedious event sequence (trace). This paper presents ECHO, an event trace reduction tool for effective bug replay by using a new differential GUI state analysis. Given a sequence of events (trace), ECHO aims at removing bug-irrelevant events by exploiting the differential behavior between the GUI states collected when their corresponding events are triggered. During dynamic testing, ECHO injects at most one lightweight inspection event after every event to collect its corresponding GUI state. A new adaptive model is proposed to selectively inject inspection events based on sliding windows to differentiate the GUI states on-the-fly in a single testing process. The experimental results show that ECHO improves the effectiveness of bug replay by removing 85.11% redundant events on average while also revealing the same bugs as those detected when full event sequences are used.},
author = {Sui, Yulei and Zhang, Yifei and Zheng, Wei and Zhang, Manqing and Xue, Jingling},
doi = {10.1145/3338906.3341183},
isbn = {9781450355728},
journal = {ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Android testing,Bug replay,Program analysis},
pages = {1095--1099},
title = {Event trace reduction for effective bug replay of Android apps via differential GUI state analysis},
year = {2019}
}
@article{Choudhary2016,
abstract = {Like all software, mobile applications ("apps") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.},
author = {Choudhary, Shauvik Roy and Gorla, Alessandra and Orso, Alessandro},
doi = {10.1109/ASE.2015.89},
eprint = {1503.07217},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choudhary, Gorla, Orso - 2016 - Automated test input generation for android Are we there yet.pdf:pdf},
isbn = {9781509000241},
journal = {Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
keywords = {Android apps,Automated testing,Test input generation},
pages = {429--440},
title = {Automated test input generation for android: Are we there yet?},
year = {2016}
}
@article{Lv,
author = {Lv, Zhengwei and Liu, Kai},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lv, Liu - Unknown - Fastbot2 Reusable Automated Model-based GUI Testing for Android Enhanced by Reinforcement Learning.pdf:pdf},
isbn = {9781450394758},
title = {Fastbot2: Reusable Automated Model-based GUI Testing for Android Enhanced by Reinforcement Learning}
}
@article{Cai2020,
abstract = {Model-based test (MBT) generation techniques for automated GUI testing are of great value for app testing. Existing GUI model-based testing tools may fall into cyclic operations and run out of resources, when applied to apps with industrial complexity and scalability. In this work, we present a multi-agent GUI MBT system named Fastbot. Fastbot performs model construction on the server end. It applies multi-agent collaboration mechanism to speed up the model construction procedure. The proposed approach was applied on more than 20 applications from Bytedance with more than 1500 million monthly active users. Higher code coverage in less testing time is achieved with comparison of three other automated testing tools including Droidbot, Humanoid and Android Monkey.},
author = {Cai, Tianqin and Zhang, Zhao and Yang, Ping},
doi = {10.1145/3387903.3389308},
isbn = {9781450379571},
journal = {Proceedings - 2020 IEEE/ACM 1st International Conference on Automation of Software Test, AST 2020},
keywords = {Automatic testing,Dynamic DAG exploration,Model-based GUI testing,Multi-agent collaboration,Traversal algorithm},
pages = {93--96},
title = {Fastbot: A multi-agent model-based test generation system},
year = {2020}
}
@article{Lic,
author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
eprint = {arXiv:1901.02633v2},
file = {:home/maryam/Documents/PhD/Android Testing/Crash Detection/Evolutionary Fuzzing/1901.02633.pdf:pdf},
pages = {1--12},
title = {Humanoid : A Deep Learning-based Approach to Automated Black-box Android App Testing}
}
@article{Romdhana2022,
author = {Romdhana, Andrea and Universit},
number = {4},
title = {Deep Reinforcement Learning for Black-box Testing of Android Apps},
volume = {31},
year = {2022}
}
@article{Feng2022,
abstract = {Due to the importance of Android app quality assurance, many automated GUI testing tools have been developed. Although the test algorithms have been improved, the impact of GUI rendering has been overlooked. On the one hand, setting a long waiting time to execute events on fully rendered GUIs slows down the testing process. On the other hand, setting a short waiting time will cause the events to execute on partially rendered GUIs, which negatively affects the testing effectiveness. An optimal waiting time should strike a balance between effectiveness and efficiency. We propose AdaT, a lightweight image-based approach to dynamically adjust the inter-event time based on GUI rendering state. Given the real-time streaming on the GUI, AdaT presents a deep learning model to infer the rendering state, and synchronizes with the testing tool to schedule the next event when the GUI is fully rendered. The evaluations demonstrate the accuracy, efficiency, and effectiveness of our approach. We also integrate our approach with the existing automated testing tool to demonstrate the usefulness of AdaT in covering more activities and executing more events on fully rendered GUIs.},
archivePrefix = {arXiv},
arxivId = {2212.05203},
author = {Feng, Sidong and Xie, Mulong and Chen, Chunyang},
eprint = {2212.05203},
file = {:home/maryam/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng, Xie, Chen - 2022 - Efficiency Matters Speeding Up Automated Testing with GUI Rendering Inference(2).pdf:pdf},
month = {dec},
title = {Efficiency Matters: Speeding Up Automated Testing with GUI Rendering Inference},
url = {http://arxiv.org/abs/2212.05203},
year = {2022}
}
